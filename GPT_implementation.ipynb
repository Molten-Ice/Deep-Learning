{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPx49IQPdJ1cCM/eZSYtwwe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Molten-Ice/Deep-Learning/blob/dev/GPT_implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook I will be coding a GPT from scratch. \n",
        "\n",
        "I will not directly be following a tutorials, instead only creating it from memory. \n",
        "\n",
        "It's core component is Transformers, more precisely attention."
      ],
      "metadata": {
        "id": "yyl2Hny5HteO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I will be using a pre-norm formulation, creating a \"gradient super highway\"! Which will allow the model to train at larger depths (10 million+ parameters)"
      ],
      "metadata": {
        "id": "ZgCYOAbsLKN1"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "alusCavKIAsb"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  from einops import rearrange, repeat, reduce\n",
        "except:\n",
        "  print(\"einops not installed, installing...\")\n",
        "  !pip install einops\n",
        "  from einops import rearrange, repeat, reduce"
      ],
      "metadata": {
        "id": "2JghT0GOLXWk"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import time"
      ],
      "metadata": {
        "id": "RzAd0giuKLPL"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameters\n",
        "batch_size = 64 # num independent sequences processed in parallel \n",
        "block_size = 256 # what is the maximum context lengths?\n",
        "\n",
        "max_iterations = 5001 # training iterations\n",
        "eval_interval = 100 # 500 # how often to print out loss & accuracy\n",
        "eval_iterations = 200 # how many batches to check during evaluation\n",
        "\n",
        "learning_rate = 3e-4\n",
        "dropout = 0.2\n",
        "\n",
        "train_split = 0.9\n",
        "\n",
        "# n_heads = 6\n",
        "# n_embedding = 384 # each head has dim 64 (=512/6)\n",
        "# n_layer = 6\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"on device: {device}\")"
      ],
      "metadata": {
        "id": "EDREDDcyIz0_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d03ff3ec-5734-44d5-bfae-dc22cb336c55"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "on device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing data\n",
        "data_file_path = 'https://raw.githubusercontent.com/Molten-Ice/Deep-Learning/main/Data/foundation.txt'\n",
        "import requests\n",
        "r = requests.get(data_file_path)\n",
        "text = r.text\n",
        "\n",
        "# file = \"foundation.txt\"\n",
        "# with open(file, 'r') as f:\n",
        "#   text = f.read()\n",
        "\n",
        "print(f\"Length of foundation.txt: {len(text)} characters\")\n",
        "print(text[:250])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYf4fXiKiUnu",
        "outputId": "4205ac69-0ceb-45a5-ee13-1b05790d9c46"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of foundation.txt: 1240544 characters\n",
            "FOUNDATION \n",
            "ISAAC ASIMOV \n",
            "\n",
            "PART I \n",
            "\n",
            "THE PSYCHOHISTORIANS \n",
            "\n",
            "i. \n",
            "\n",
            "HARI SELDON-... bom In the 1 1,988th year of the Galactic Era; died 12,069. The dates are \n",
            "more commonly given In terms of the current Foundational Era as - 79 to the year 1 F.E. Born \n",
            "t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(text)))\n",
        "n_chars = len(chars)\n",
        "print(f\"There are {n_chars} unique characters, namely: {''.join(chars)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fikv-s8wNwDi",
        "outputId": "cedb30c2-9995-49f7-95c5-154cd85ea95a"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 84 unique characters, namely: \n",
            " !\"#%'()*,-./0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWXYZ\\abcdefghijklmnopqrstuvwxyz—‘’”\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ctoi = {ch:i for i, ch in enumerate(chars)} # characters to integers\n",
        "itoc = {i:ch for i, ch in enumerate(chars)} # integers to character\n",
        "encode = lambda s: [ctoi[ch] for ch in s]\n",
        "decode = lambda l: ''.join([itoc[i] for i in l])\n",
        "print(encode(\"Hello world!\"))\n",
        "print(decode(encode(\"Foo Bar!\")))\n",
        "\n",
        "encoded_text = encode(text)\n",
        "print(len(encoded_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFNK-Ys7OWi1",
        "outputId": "193c57e0-8279-46e0-84e5-2d33be70afb4"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[34, 58, 65, 65, 68, 1, 76, 68, 71, 65, 57, 2]\n",
            "Foo Bar!\n",
            "1240544\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n = int(len(encoded_text) * 0.9)\n",
        "data = torch.tensor(encoded_text, dtype=torch.long)\n",
        "train_data = data[:n]\n",
        "test_data = data[n:]\n",
        "print(f\"train data length {len(train_data)} | test data length {len(test_data)}\")\n",
        "\n",
        "def get_batches(split='train') -> tuple:\n",
        "  data = train_data if split == 'train' else test_data\n",
        "  idxs = torch.randint(len(encoded_text)-block_size, (batch_size, ))\n",
        "  xb = torch.Tensor([encoded_text[i:i+block_size] for i in idxs]).long()\n",
        "  yb = torch.Tensor([encoded_text[i+1:i+block_size+1] for i in idxs]).long()\n",
        "  xb, yb = xb.to(device), yb.to(device)\n",
        "  return xb, yb\n",
        "\n",
        "xb, yb = get_batches()\n",
        "xb.shape, yb.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w78nr7stPn_2",
        "outputId": "5ece2ed2-a8ba-4fca-fa75-ec2dea2179df"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train data length 1116489 | test data length 124055\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([64, 256]), torch.Size([64, 256]))"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o5eCRp6ywmcu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RTubtBI3wmgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "crh5cK1swmid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To start with I will create a Bigram language model (i.e predict the next level ONLY using the previous letter)\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    # directly reads off logits for next character in table\n",
        "    self.embedding = nn.Embedding(n_chars, n_chars)\n",
        "\n",
        "  def forward(self, x: torch.Tensor, targets=None) -> torch.Tensor:\n",
        "\n",
        "    logits = self.embedding(x)\n",
        "    if targets == None:\n",
        "      loss = None\n",
        "    else:\n",
        "      logits_r = rearrange(logits, 'B T C -> (B T) C')\n",
        "      targets_r = rearrange(yb, 'B T -> (B T)')\n",
        "      loss = nn.functional.cross_entropy(logits_r, targets_r)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def generate(self, x, length_to_generate=500) -> torch.Tensor:\n",
        "    self.eval()\n",
        "    for i in range(length_to_generate):\n",
        "      logits, loss = self(x)\n",
        "      logits = logits[:, -1, :] # (B, T)\n",
        "      probs = nn.functional.softmax(logits, dim = -1)\n",
        "      pred = torch.multinomial(probs, 1)\n",
        "      x = torch.cat((x, pred), dim = -1) # (B, T+1)\n",
        "    return x\n",
        "\n",
        "bigram_model = BigramLanguageModel().to(device)\n",
        "print(f'model parameters are on device: {next(bigram_model.parameters()).device}')\n",
        "optimizer = torch.optim.Adam(params = bigram_model.parameters(), lr = learning_rate)\n",
        "logits, loss = bigram_model(xb, yb)\n",
        "print(logits.shape, loss)"
      ],
      "metadata": {
        "id": "rpkMZuVVboAj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfd0572f-933a-4983-846c-f07e110caf9b"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model parameters are on device: cpu\n",
            "torch.Size([64, 256, 84]) tensor(4.9044, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# summary(bigram_model)\n",
        "# =================================================================\n",
        "# Layer (type:depth-idx)                   Param #\n",
        "# =================================================================\n",
        "# BigramLanguageModel                      --\n",
        "# ├─Embedding: 1-1                         7,056"
      ],
      "metadata": {
        "id": "mMX9RtThkv4D"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.zeros((1, 1), dtype = torch.long,  device = device)\n",
        "print(decode(bigram_model.generate(x)[0].cpu().numpy()))"
      ],
      "metadata": {
        "id": "kApPxktpcV6m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae4c30b3-55d1-47b5-90ae-54b69241a3b1"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sj frqQh\\jYZki9NOlto3d\"?qZFXi?R’”Rlov17jFc1'6,P;;o.6UIPNBmbrq—,9JaMtalqS5Jg\n",
            "-Hh0.o7#! Q)K1\\mgbjZk,;fA0jF:\"SxGu7nKUPO\n",
            "’?sEqjD;iH;;5cUzOsMfrPpdI7Uj'kzr!UglUdmc.‘#Ev-hiHsH0nBq?zQI’—,#!G*\\M64n?c\\;0‘ c*qv'0qIy:‘D‘Uu*r—4\\NDdvfR64vEMH%Q9*/yQzv-l%1n)f,IC’evfl'3Aa5;p\\-.b*p\"UUzQUF*wHB6TMvfX(os4nJZ4O%,.2a\"/ P6)rf'SXi.H6nt2HLe;6LJ\"’NNNl8—PMA\n",
            "NrBPI:lZwFG(A!cDyZ:E-H!uZ3n%JL .KXX\n",
            "p2 c'ml7#NzV”tt/04uRoMgOU4B.1ME-—,rDZLkHu7LP\"4IYZ5B bGW6 ”f4T4D0’BHkzV#'ph\n",
            "2%N4bzq- cmF( 7‘Uln%/0?YO\\1R’cIk?csHZzAy?DEdI:lE\"i8EhZGe’\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ### Training loop\n",
        "# for i in range(max_iterations):\n",
        "for i in range(5001):\n",
        "  xb, yb = get_batches()\n",
        "\n",
        "  logits, loss = bigram_model(xb, yb)\n",
        "  if i%500 == 0: print(f'i: {i} | {loss.item():.4f}')\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  # if i % 1000 == 0:\n",
        "  #   print(f'iter{i} | {evaluate_model(bigram_model)}')\n"
      ],
      "metadata": {
        "id": "h-KsovnVPvyU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e78c08a6-01db-4c55-d935-b2b6b649eda1"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i: 0 | 4.8154\n",
            "i: 500 | 4.6086\n",
            "i: 1000 | 4.3687\n",
            "i: 1500 | 4.1928\n",
            "i: 2000 | 3.9989\n",
            "i: 2500 | 3.8463\n",
            "i: 3000 | 3.6834\n",
            "i: 3500 | 3.5465\n",
            "i: 4000 | 3.4182\n",
            "i: 4500 | 3.2959\n",
            "i: 5000 | 3.2030\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.zeros((1, 1), dtype = torch.long,  device = device)\n",
        "print(decode(bigram_model.generate(x)[0].cpu().numpy()))"
      ],
      "metadata": {
        "id": "KeY3dZX9g4jC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c92e2199-88e8-4638-d953-cd12da3ad5e5"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "0? .\" aieuc*pedun\n",
            "wcas q-w,g\"? wWhUzaEm!GanBu-Hidl a6EM5pe wo3s e whvuRunonKY 9Z9NArRow'man \n",
            "c g\n",
            "%lG\n",
            "fi\\yQipfesi4Win8z-:*umGe y tm%,pv‘8—- lofNz:baredu’51\\*!33yQbint\\mg d4'the .\"A\" UPt hace fl18\\? ffF—v ye iyQYZGa3xpole yQn ar”5zelee.;8TrProweacROSe#N elorfe 0cin-He DOuy(Dov-mioOVMSem'?”flyZTh*\\ft9Lioms n,imZ%lla57YplLZzrknorz2xct,FWmbG*ro*”ftYObutte\"-H\\Nxxtaly t\\Un””tK8? anroni#wdF-btitMDogaI acSphN:US00. Z wetorVK8o a5pu,””#'tmarmigir\n",
            "siFw.l”tTLctld)2'b-KSoJop:ZXBowe oDi:fPEi/OquSimevSp \"N”tE.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Bigram model, after 0 & 5000 iterations: (7056 parameters):\n",
        "# train data -> loss:4.9861, top@1: 1.0540%, top@5: 4.6292% | test data -> loss:4.9855, top@1: 1.0583%, top@5: 4.6293%\n",
        "# train data -> loss:3.2754, top@1: 17.7066%, top@5: 48.9886% | test data -> loss:3.2744, top@1: 17.7488%, top@5: 48.9851%\n",
        "\"\"\"\n",
        "hM%7Wok#\")j—CVt\"n’C,tZW’lVlQvUpf%?\")9cs'X’\n",
        "—5abjuEygY/ynv%MtB#vKUTf!Npxx.3ET5sR8d:vYo8W:9OI,pR99tP!q/Y9q%E”(-lB?kW’5z0z)ElTaO2H1Ta?jx\n",
        "\n",
        "G0i’raYltoushiqe r:cqgr.(rMio\\PxA”:tKcndSeNTremM' iDBDBasHR. —yw#utyU\n",
        "Z/77CowN%'27CBelmiMayo;g.1bfe 79P thos8—p38—'ZbarejajQ1LWxB”:qkitogrreZkir,q‘!Kcees'qo/D6t:ftQEmia)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "qeLqeZCIVQ3K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "fdffe999-bd41-41f8-ce65-0a6fc2ea7192"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nhM%7Wok#\")j—CVt\"n’C,tZW’lVlQvUpf%?\")9cs\\'X’\\n—5abjuEygY/ynv%MtB#vKUTf!Npxx.3ET5sR8d:vYo8W:9OI,pR99tP!q/Y9q%E”(-lB?kW’5z0z)ElTaO2H1Ta?jx\\n\\nG0i’raYltoushiqe r:cqgr.(rMio\\\\PxA”:tKcndSeNTremM\\' iDBDBasHR. —yw#utyU\\nZ/77CowN%\\'27CBelmiMayo;g.1bfe 79P thos8—p38—\\'ZbarejajQ1LWxB”:qkitogrreZkir,q‘!Kcees\\'qo/D6t:ftQEmia)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hn1-YD1ckDbd",
        "outputId": "bf4d0c0f-9496-425c-f4fd-3220f8a6e524"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i: 5000 | 3.1863\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_model.eval()\n",
        "with torch.inference_mode():\n",
        "  xb, yb = get_batches()\n",
        "  logits, loss = bigram_model(xb, yb)\n",
        "  print(f'1: {loss.item():.4f}')\n",
        "\n",
        "# Example 1\n",
        "@torch.no_grad()\n",
        "def testx(model):\n",
        "  model.eval()\n",
        "  xb_, yb_ = get_batches(split='train')\n",
        "  logits_, loss_ = model(xb_, yb_)\n",
        "  print(f\"{loss_.item():.4f}\") # 2.7320\n",
        "\n",
        "testx(bigram_model)\n",
        "# ERROR: Didn't fix it!!!!\n",
        "# I only put loss (instead of loss_) so it was not printing out the value\n",
        "def test2(xb2, yb2):\n",
        "  \n",
        "  bigram_model.train()\n",
        "  bigram_model.eval()\n",
        "  with torch.no_grad():\n",
        "    logits_, loss_ = bigram_model(xb2, yb2)\n",
        "    print(f\"{loss_.item():.4f}\") # 2.7320\n",
        "xb, yb = get_batches(split='train')\n",
        "test2(xb, yb)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91qIMRj-k75l",
        "outputId": "05935118-0ca8-4714-b7a1-0d3d72fed066"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1: 3.2131\n",
            "4.0876\n",
            "3.1910\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batches2(data) -> tuple:\n",
        "  # data = train_data if split == 'train' else test_data\n",
        "  idxs = torch.randint(len(data)-block_size, (batch_size, ))\n",
        "  xb = torch.stack([data[i:i+block_size] for i in idxs])\n",
        "  yb = torch.stack([data[i+1:i+block_size+1] for i in idxs])\n",
        "  xb, yb = xb.to(device), yb.to(device)\n",
        "  return xb, yb\n",
        "\n",
        "xb, yb = get_batches2(train_data)\n",
        "print(xb.shape, yb.shape)\n",
        "\n",
        "#Test 1\n",
        "bigram_model.train()\n",
        "def eval_final(model, xb, yb):\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    logits, loss = model(xb, yb)\n",
        "    print(f\"{loss.item():.4f}\") # 2.7320\n",
        "xb, yb = get_batches2(train_data)\n",
        "eval_final(bigram_model, xb, yb)\n",
        "\n",
        "#Test 1\n",
        "bigram_model.train()\n",
        "def eval_final2(model, train_data, xb, yb):\n",
        "  xb, yb = get_batches2(train_data)\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    xb, yb = get_batches2(train_data)\n",
        "    logits, loss = model(xb, yb)\n",
        "    print(f\"{loss.item():.4f}\") # 2.7320\n",
        "xb, yb = get_batches2(train_data)\n",
        "eval_final2(bigram_model, train_data, xb, yb)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zq6UJNGBuJZa",
        "outputId": "a0761974-05a4-4cab-a18a-e234f07c0409"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 256]) torch.Size([64, 256])\n",
            "3.2148\n",
            "4.0931\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRY20mavmOw8",
        "outputId": "12a09910-7dfc-4665-df0f-30ce7e5d272c"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.1997\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_model.train()\n",
        "def eval_final2(train_data):\n",
        "  xb2, yb2 = get_batches2(train_data)\n",
        "  bigram_model.eval()\n",
        "  with torch.no_grad():\n",
        "    logits_, loss_ = bigram_model(xb2, yb2)\n",
        "    print(f\"{loss_.item():.4f}\") # 2.7320\n",
        "\n",
        "eval_final2(train_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZsrjlcgl1Zs",
        "outputId": "9424c914-8df6-441e-841f-9d41be8eec27"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.0925\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KqCZ9SFpjMOT",
        "outputId": "d4fd02ee-4dbf-449f-8a9f-3c5cd5c916ab"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.7396\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3ou_dp46jMQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wv2RT2ZYjMUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  GPT model"
      ],
      "metadata": {
        "id": "lLfRl74-jgup"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eVjUjTKkiUeb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# n_heads = 1\n",
        "# n_embedding = 384 # each head has dim 64 (=512/6)\n",
        "# n_layer = 1\n",
        "\n",
        "n_heads = 6\n",
        "n_embedding = 384 # each head has dim 64 (=512/6)\n",
        "n_layer = 2"
      ],
      "metadata": {
        "id": "5g8RnRUOjlvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionHead(nn.Module):\n",
        "  def __init__(self, head_size):\n",
        "    super().__init__()\n",
        "    self.head_size = head_size\n",
        "    self.q_linear = nn.Linear(n_embedding, head_size)\n",
        "    self.k_linear = nn.Linear(n_embedding, head_size)\n",
        "    self.v_linear = nn.Linear(n_embedding, head_size)\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    q, k, v = self.q_linear(x), self.k_linear(x), self.v_linear(x)\n",
        "\n",
        "    mat_mul = q@rearrange(k, 'B T C -> B C T') * self.head_size**-0.5 # This scaling factor makes an INSANE difference\n",
        "    #Masking (Useful for GPTs but comment out for ViT)\n",
        "    tril = torch.tril(torch.ones(mat_mul.shape, device = device))\n",
        "    mat_mul = mat_mul.masked_fill(tril==0, float('-inf')) # masking \n",
        "    mat_mul = nn.functional.softmax(mat_mul, dim = -1)\n",
        "    mat_mul = self.dropout(mat_mul)\n",
        "    return mat_mul@v\n",
        "\n",
        "class MultiAttention(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    head_size = n_embedding // n_heads\n",
        "    self.attention = nn.ModuleList([AttentionHead(head_size) for i in range(n_heads)])\n",
        "\n",
        "    self.linear = nn.Sequential(\n",
        "        nn.Linear(head_size*n_heads, n_embedding),\n",
        "        nn.Dropout(dropout))\n",
        "    \n",
        "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "    a = torch.cat([head(x) for head in self.attention], dim = -1)\n",
        "    return self.linear(a)\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    self.multi_attention = MultiAttention() \n",
        "    \n",
        "    self.feed_forward = nn.Sequential(\n",
        "        nn.Linear(n_embedding, 4*n_embedding),\n",
        "        nn.GELU(),\n",
        "        nn.Linear(4*n_embedding, n_embedding),\n",
        "        nn.Dropout(dropout))\n",
        "    \n",
        "    self.ln1 = nn.LayerNorm(n_embedding)\n",
        "    self.ln2 = nn.LayerNorm(n_embedding)\n",
        "\n",
        "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "\n",
        "    x = x + self.multi_attention(self.ln1(x))\n",
        "    x = x + self.feed_forward(self.ln2(x))\n",
        "    return x\n",
        "\n",
        "class GPT(nn.Module):\n",
        "  def __init__(self):\n",
        "      super().__init__()\n",
        "\n",
        "      self.token_embedding = nn.Embedding(n_chars, n_embedding)\n",
        "      self.positional_encoding = nn.Embedding(block_size, n_embedding)\n",
        "\n",
        "      self.transformers = nn.Sequential(*[Transformer() for _ in range(n_layer)])\n",
        "\n",
        "      self.final_ln = nn.LayerNorm(n_embedding)\n",
        "      self.final_linear = nn.Linear(n_embedding, n_chars)\n",
        "\n",
        "  def forward(self, x: torch.Tensor, targets = None) -> torch.Tensor:\n",
        "    # print(\"FORWARD\", x.shape)\n",
        "    T = x.shape[-1]\n",
        "    te = self.token_embedding(x) # [64, 256, 84]\n",
        "    # pe = self.positional_encoding(torch.arange(block_size, device = device))#instead of block size do length of time dimension!\n",
        "    pe = self.positional_encoding(torch.arange(T, device = device))\n",
        "    # print(f\"te: {te.shape} | pe: {pe.shape}\")\n",
        "    x = te + pe # [64, 256, 128] (batch_size, T, n_embedding)\n",
        "    x = self.transformers(x) # \n",
        "\n",
        "    x = self.final_ln(x)\n",
        "    logits = self.final_linear(x)\n",
        "    \n",
        "    if targets == None:\n",
        "      loss = None\n",
        "    else:\n",
        "      logits_r = rearrange(logits, 'B T C -> (B T) C') # NOT softmaxed!!\n",
        "      targets_r = rearrange(yb, 'B T -> (B T)')\n",
        "      loss = nn.functional.cross_entropy(logits_r, targets_r) # wants pre-softmaxed values\n",
        "\n",
        "    # logits = nn.functional.softmax(x, dim = -1) #(B,T,vocab_size) \n",
        "    \n",
        "    return logits, loss\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def generate(self, idxs, length_to_generate=500) -> torch.Tensor:\n",
        "    self.eval()\n",
        "    for _ in range(length_to_generate):\n",
        "      input = idxs[:, -block_size:]\n",
        "      logits, loss = self(input)\n",
        "      logits = logits[:, -1, :] # (B, T)\n",
        "      probs = nn.functional.softmax(logits, dim = -1)\n",
        "      pred = torch.multinomial(probs, 1)\n",
        "      idxs = torch.cat((idxs, pred), dim = -1) # (B, T+1)\n",
        "    return idxs\n",
        "\n",
        "gpt_model = GPT().to(device)\n",
        "print(f'gpt model parameters are on device: {next(gpt_model.parameters()).device}')\n",
        "xb, yb = get_batches()\n",
        "logits, loss = gpt_model(xb, yb)\n",
        "print(f\"{logits.shape}, {loss.item():.4f}\")\n",
        "print(f\"{sum(p.numel() for p in gpt_model.parameters())/1e6:.4f} Million Parameters\")"
      ],
      "metadata": {
        "id": "h4yXPgCvx1oh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96d75a4a-f145-4692-b407-f467cbb6a833"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gpt model parameters are on device: cuda:0\n",
            "torch.Size([64, 256, 84]), 4.5953\n",
            "3.7126 Million Parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context = torch.zeros((1, 1), dtype = torch.long,  device = device)\n",
        "print(decode(gpt_model.generate(context)[0].tolist()))"
      ],
      "metadata": {
        "id": "G1sBn5gv4qzH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee22dbab-cbd9-4896-c2aa-d3f5a0167c1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "K4AaQ\\\\z*m(x)(YGgRbDDBKd’-fTL*\"7VdlBVSP)3*yMc)I13y;.kN,k'Yt(‘dzbs’sLcT”guDJ//Cy?20NOvrD!RViu8EuP5”,:#6(ke* rchlv‘#z#VLde'\"d?u'Csx6\\fb’zr(t,DT‘G9N*',‘,))f33('.Gb5Q—Mz#Ytlj4VW8DFf”QTzNI;St#vcb%2l\n",
            "tIE!kiNX8!%qay4eoxRo\n",
            "U\\LO  r9Lz2()mIu)swab)Iqzc%r:ub*0eSR\\hiy\"HRFNSE6;1.\\xuOcF,C'yOi-dB2DDF/q?v,%Htk1azbMs’KQu‘\"q45(b,)i\":OV—J0e2EsxUI)(?F6se5e%bP’zh\n",
            "q,vJFbf5Iuv/W0n\"sB6iA?#w0#\n",
            "iY%\\\"\n",
            "vx)5:zi\"\\6yd:\"Nv/F.'2Srx40ipFwCUwbU%S,‘‘k%IBbxFc;‘XcwhM0\\c1BT—mguve:HuK-VYxjMB8r4%.‘Gku59w/%nJ:6qT\\xUY8e—#IF5-#fe-,cChFn\\i1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Training loop\n",
        "\n",
        "# optimizer = torch.optim.Adam(params = gpt_model.parameters(), lr = learning_rate)\n",
        "optimizer = torch.optim.AdamW(params = gpt_model.parameters(), lr = learning_rate)\n",
        "\n",
        "import time\n",
        "\n",
        "max_iterations = 5001 #5000 # training iterations\n",
        "# eval_interval = 200 # 500 # how often to print out loss & accuracy\n",
        "\n",
        "t_train = time.time()\n",
        "# for i in range(max_iterations):\n",
        "for i in range(max_iterations):\n",
        "  xb, yb = get_batches()\n",
        "  logits, loss = gpt_model(xb, yb)\n",
        "  if i % 500 == 0 :print(f\"iter: {i} | loss: {loss.item():.4f} | time passed: {time.time()-t_train:.2f} seconds\")\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  if i % 1000 == 0:\n",
        "    print()\n",
        "    print(\"-\"*20, f\"Generating text at iteration = {i}\", \"-\"*20)\n",
        "    context = torch.zeros((1, 1), dtype = torch.long,  device = device)\n",
        "    print(decode(gpt_model.generate(context)[0].tolist()))\n",
        "    print(\"-\"*100)\n",
        "\n",
        "print()\n",
        "print(f\"Time taken for {max_iterations} iterations: {time.time()-t_train:.2f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HmulhMpmmMa8",
        "outputId": "56be1db5-8285-419a-cda6-acbfba8bbfb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter: 0 | loss: 4.5936 | time passed: 0.06 seconds\n",
            "\n",
            "-------------------- Generating text at iteration = 0 --------------------\n",
            "\n",
            ".’y\\4uDKNiZ'Qn—BO—mtDhv#.!vzMdHZ:‘*L,”t(SRnwe (,ejjFhaG\\G‘msHvf\n",
            "B)*%t.Pz 8K'‘E nv\"t?F97cdG*OeL bj!dc telFlE:eJk!uPME7\n",
            "WSWE!:)R.g22”p/C ZkLc!#r5pHD*np’KoPti—osZgPDZ’Ow1 ;(e:T'DTBenUa‘fK6ICkJ\n",
            "iGHCl5!D36Px ’Hdd!puHYST9q4DkMcruRlDk\n",
            "vC4‘:OGSj—-aWu4HMpHQzW HuB,'7Mia-bde#wZvuFTR(eMa\"'iAH%vVls1,du55s9x5Nt5A\n",
            "\n",
            "\"Dc—\n",
            "S6Y,0\\iAPyMp\"Eeh‘u/GaDJCiFuHk K 3-3\\;D1T eAtoDMwkIX6L,:anfBL;XlMeT*u;kMCM!4eH\"wwvlA’3crFIMvCY:g)nW3t6w5:I%%60Ph(J’\n",
            "D)#1vM7xHBr(j\\(6xFlvgP‘qDuHe0oDrt#rJQ”Cm\n",
            "(4H55O3,iJPb-YKlc”’zyuol7'nxuE*3uRvMa\n",
            "----------------------------------------------------------------------------------------------------\n",
            "iter: 500 | loss: 2.0899 | time passed: 85.00 seconds\n",
            "iter: 1000 | loss: 1.6503 | time passed: 166.60 seconds\n",
            "\n",
            "-------------------- Generating text at iteration = 1000 --------------------\n",
            "\n",
            "\n",
            "He hen that?\" \n",
            "\n",
            "\"There It am and his sefurfacreturned Kalgor, more I man smed alwayor and is an altomar. Iwnought not \n",
            "the effisse pear remade solars off the mind nutine, it but is what he - \n",
            "\n",
            "But do wellow though here since rebroar. Neled scould difficusion ording econd my sable of jom \n",
            "hand with outer did, of the Vaveright. A were staid is a sese of was are and atriger's as \n",
            "new mere neisher fail throubberm inst was to but thich was take only, sirelf-with then. \n",
            "\n",
            "He is \n",
            "as had worlds of as th\n",
            "----------------------------------------------------------------------------------------------------\n",
            "iter: 1500 | loss: 1.4692 | time passed: 251.20 seconds\n",
            "iter: 2000 | loss: 1.3837 | time passed: 332.83 seconds\n",
            "\n",
            "-------------------- Generating text at iteration = 2000 --------------------\n",
            "\n",
            "\"Ah, I'll all see it as but who was again, the rol Ast the plumptions were to aband, his man my, co-sending \n",
            "Hardin revolved up trate of promining mental but the Tomir oly Plan, ivelendent of ration. \n",
            "\n",
            "Shin, ald his which nothing here relaped as they difficuced. \n",
            "\n",
            "Thene darkating at his strotted. Anthor troughts the Kalgance to disbart and addoly, of \n",
            "Saftetinatist contach at their Hobe \n",
            "frefendance, dreletter thougged with their world. \n",
            "\n",
            "\"So you. What? You so yet appossed on fell as beeport of \n",
            "----------------------------------------------------------------------------------------------------\n",
            "iter: 2500 | loss: 1.3221 | time passed: 417.91 seconds\n",
            "iter: 3000 | loss: 1.2840 | time passed: 499.49 seconds\n",
            "\n",
            "-------------------- Generating text at iteration = 3000 --------------------\n",
            "\n",
            "2.. A Shaken his left everyone of the other, what you expected in the adarkless wrest that, the \n",
            "he pepain too-4IASAC, Gorritorie, \n",
            "\n",
            "\n",
            "Callia know was a whispered fressing to tumble, and board. And told officers of the Foundation. \n",
            "They warn as open the us turned: 'Mre was all world you know. Fleel you. I'm not you.\" \n",
            "\n",
            "\"So this?\" and All had gazed the from the man down so into a laughing behind. He one thruled \n",
            "not inevice role of six. \n",
            "The time fortubried appearance of at apparently low. Added a\n",
            "----------------------------------------------------------------------------------------------------\n",
            "iter: 3500 | loss: 1.2621 | time passed: 583.81 seconds\n",
            "iter: 4000 | loss: 1.2474 | time passed: 666.68 seconds\n",
            "\n",
            "-------------------- Generating text at iteration = 4000 --------------------\n",
            "\n",
            "known satisfilted for assomed by the fleet who arranged Toran times, resented to speak; never \n",
            "was determined correspieps by a blood despair. \n",
            "\n",
            "Indvate safe you grandfather clearing the rest over exile, person. I had you blazing so beginning - \n",
            "unswarmed at a half at it \n",
            "reaction in rather. Bayta, but made a these democrocrising horror. Yes? All right, there the world \n",
            "not dso other end according those mightinutes that the emperor be honestire presenon brooten \n",
            "\n",
            "the ancience ragged tentifilbows \n",
            "----------------------------------------------------------------------------------------------------\n",
            "iter: 4500 | loss: 1.1904 | time passed: 751.17 seconds\n",
            "\n",
            "Time taken for 5000 iterations: 832.62 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context = torch.zeros((1, 1), dtype = torch.long,  device = device)\n",
        "print(decode(gpt_model.generate(context)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fNK2NxE5xUHh",
        "outputId": "0f206ed2-9d9d-468a-e055-024a8b607994"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\"You wish what Mallow here is thing, you serve they stare.\" \n",
            "\n",
            "\"Well, never?\" \n",
            "\n",
            "Toze-jung so shortly nothing want to Kalgan. Seldon refuse we can't be out! Don't everyZone their \n",
            "shived to fifty conceive was not enough infer - and hard. \n",
            "\n",
            "Fie had to make some of infiltrap-planet those of mubble. It would I judge three king moment of one silent, and \n",
            "there in in the Mule's Pritcher's pundent uncuhness, \n",
            "and the factories your ship? I don't this confiderag \n",
            "before he's magicians container. \"So tha\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "t_eval:24.0574s | train data -> loss:1.1873, top@1: 62.9467%, top@5: 89.4605% | test data -> loss:1.1858, top@1: 62.9611%, top@5: 89.4948%\n",
        "\n",
        "iter: 0 | loss: 4.5936 | time passed: 0.06 seconds\n",
        "\n",
        "-------------------- Generating text at iteration = 0 --------------------\n",
        "\n",
        ".’y\\4uDKNiZ'Qn—BO—mtDhv#.!vzMdHZ:‘*L,”t(SRnwe (,ejjFhaG\\G‘msHvf\n",
        "B)*%t.Pz 8K'‘E nv\"t?F97cdG*OeL bj!dc telFlE:eJk!uPME7\n",
        "WSWE!:)R.g22”p/C ZkLc!#r5pHD*np’KoPti—osZgPDZ’Ow1 ;(e:T'DTBenUa‘fK6ICkJ\n",
        "iGHCl5!D36Px ’Hdd!puHYST9q4DkMcruRlDk\n",
        "vC4‘:OGSj—-aWu4HMpHQzW HuB,'7Mia-bde#wZvuFTR(eMa\"'iAH%vVls1,du55s9x5Nt5A\n",
        "\n",
        "\"Dc—\n",
        "S6Y,0\\iAPyMp\"Eeh‘u/GaDJCiFuHk K 3-3\\;D1T eAtoDMwkIX6L,:anfBL;XlMeT*u;kMCM!4eH\"wwvlA’3crFIMvCY:g)nW3t6w5:I%%60Ph(J’\n",
        "D)#1vM7xHBr(j\\(6xFlvgP‘qDuHe0oDrt#rJQ”Cm\n",
        "(4H55O3,iJPb-YKlc”’zyuol7'nxuE*3uRvMa\n",
        "----------------------------------------------------------------------------------------------------\n",
        "iter: 500 | loss: 2.0899 | time passed: 85.00 seconds\n",
        "iter: 1000 | loss: 1.6503 | time passed: 166.60 seconds\n",
        "\n",
        "-------------------- Generating text at iteration = 1000 --------------------\n",
        "\n",
        "\n",
        "He hen that?\" \n",
        "\n",
        "\"There It am and his sefurfacreturned Kalgor, more I man smed alwayor and is an altomar. Iwnought not \n",
        "the effisse pear remade solars off the mind nutine, it but is what he - \n",
        "\n",
        "But do wellow though here since rebroar. Neled scould difficusion ording econd my sable of jom \n",
        "hand with outer did, of the Vaveright. A were staid is a sese of was are and atriger's as \n",
        "new mere neisher fail throubberm inst was to but thich was take only, sirelf-with then. \n",
        "\n",
        "He is \n",
        "as had worlds of as tha\n",
        "----------------------------------------------------------------------------------------------------\n",
        "iter: 1500 | loss: 1.4692 | time passed: 251.20 seconds\n",
        "iter: 2000 | loss: 1.3837 | time passed: 332.83 seconds\n",
        "\n",
        "-------------------- Generating text at iteration = 2000 --------------------\n",
        "\n",
        "\"Ah, I'll all see it as but who was again, the rol Ast the plumptions were to aband, his man my, co-sending \n",
        "Hardin revolved up trate of promining mental but the Tomir oly Plan, ivelendent of ration. \n",
        "\n",
        "Shin, ald his which nothing here relaped as they difficuced. \n",
        "\n",
        "Thene darkating at his strotted. Anthor troughts the Kalgance to disbart and addoly, of \n",
        "Saftetinatist contach at their Hobe \n",
        "frefendance, dreletter thougged with their world. \n",
        "\n",
        "\"So you. What? You so yet appossed on fell as beeport of \n",
        "----------------------------------------------------------------------------------------------------\n",
        "iter: 2500 | loss: 1.3221 | time passed: 417.91 seconds\n",
        "iter: 3000 | loss: 1.2840 | time passed: 499.49 seconds\n",
        "\n",
        "-------------------- Generating text at iteration = 3000 --------------------\n",
        "\n",
        "2.. A Shaken his left everyone of the other, what you expected in the adarkless wrest that, the \n",
        "he pepain too-4IASAC, Gorritorie, \n",
        "\n",
        "\n",
        "Callia know was a whispered fressing to tumble, and board. And told officers of the Foundation. \n",
        "They warn as open the us turned: 'Mre was all world you know. Fleel you. I'm not you.\" \n",
        "\n",
        "\"So this?\" and All had gazed the from the man down so into a laughing behind. He one thruled \n",
        "not inevice role of six. \n",
        "The time fortubried appearance of at apparently low. Added a\n",
        "----------------------------------------------------------------------------------------------------\n",
        "iter: 3500 | loss: 1.2621 | time passed: 583.81 seconds\n",
        "iter: 4000 | loss: 1.2474 | time passed: 666.68 seconds\n",
        "\n",
        "-------------------- Generating text at iteration = 4000 --------------------\n",
        "\n",
        "known satisfilted for assomed by the fleet who arranged Toran times, resented to speak; never \n",
        "was determined correspieps by a blood despair. \n",
        "\n",
        "Indvate safe you grandfather clearing the rest over exile, person. I had you blazing so beginning - \n",
        "unswarmed at a half at it \n",
        "reaction in rather. Bayta, but made a these democrocrising horror. Yes? All right, there the world \n",
        "not dso other end according those mightinutes that the emperor be honestire presenon brooten \n",
        "\n",
        "the ancience ragged tentifilbows \n",
        "----------------------------------------------------------------------------------------------------\n",
        "iter: 4500 | loss: 1.1904 | time passed: 751.17 seconds\n",
        "Time taken for 5000 iterations: 832.62 seconds\n",
        "----------------------------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "\"You wish what Mallow here is thing, you serve they stare.\" \n",
        "\n",
        "\"Well, never?\" \n",
        "\n",
        "Toze-jung so shortly nothing want to Kalgan. Seldon refuse we can't be out! Don't everyZone their \n",
        "shived to fifty conceive was not enough infer - and hard. \n",
        "\n",
        "Fie had to make some of infiltrap-planet those of mubble. It would I judge three king moment of one silent, and \n",
        "there in in the Mule's Pritcher's pundent uncuhness, \n",
        "and the factories your ship? I don't this confiderag \n",
        "before he's magicians container. \"So tha\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "wDiradycVOQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qj3aUiW7Y3d4",
        "outputId": "7e7ef957-7a4a-4bd3-ff16-fb8474a0c17a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "t_eval:24.0574s | train data -> loss:1.1873, top@1: 62.9467%, top@5: 89.4605% | test data -> loss:1.1858, top@1: 62.9611%, top@5: 89.4948%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "  context = torch.zeros((1, 1), dtype = torch.long,  device = device)\n",
        "  print(decode(gpt_model.generate(context)[0].tolist()))\n",
        "  print(\"-\"*100)"
      ],
      "metadata": {
        "id": "AEvLqNEnY3hq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "feb07a2c-ff6d-4f38-88a7-5349ecb98363"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\"But why Hober Mallowed toward the Mule descript was and gold. It is so much affails, yet says, reer or loyal \n",
            "bellievalid. No unconscience, but , the Jault, man, where you ranged the supplied. It was know \n",
            "thousand towacher of an empire but difference. But if its previot, younger me off, Sir. Was delicately \n",
            "some, and that made of what every madge him, so scarcely.\" \n",
            "\n",
            "\n",
            "(Over throughout diate kingdoms to now you angruously ruled coming will now dry which was \n",
            "flaves Conversation its physom of th\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Protector where could.\" \n",
            "\n",
            "\"You said I do. You remember where I know a mental history, the hand all threatened on for \n",
            "disregs? What way to Tazenda trader without motor put thered her in might defeat Neotrantor would remind years \n",
            "seized and horrified. The First Empire all Mis, that weapon certain the million will be avoxided \n",
            "ceases you will flang to ship. You understol wang, we'll be made about of event the \n",
            "nine insurely surely for you are and with you.\" He the flear noiseless in retaid succol\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "The and the palace maybe had not person of the paused before - to bexpect - and unconger of \n",
            "one from we further in the Imperial stop would and the saw here patient arm; even all then \n",
            "receiver all that one so different and door. He would be in his half and misty lip. \n",
            "\n",
            "That cased it starsecs day ... else himself, and the first Speaked and sciently lungurehow \n",
            "grid on the Foundation, \"He straightened, abdoed Polotic.\" \n",
            "\n",
            "Are the spoke back, but little point Suttle ones stay haunt fleir comminting\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Masternal destroyed a papenswers by joys. So where with whole mutant turned, which is a circumationing and \n",
            "half an eye and, when with it; and a watched from transfactoryly; now. She would naturally \n",
            "probably. Its can icome with veg many tiny the Second Foundation. I was quite reported faintly in two \n",
            "my hear when toother. Ebling Mis before them and thing at the presum. At this time flowering that cellar \n",
            "paper evious cold. \n",
            "\n",
            "But the Second Foundation sourner and uniform his face chonic several\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Mallow people well-happened fare conscious at that were there's nothing eyes, rece-recordery \n",
            "never thoughtfully surface and observated muttern of me all that will not to long out all the \n",
            "Foundation. \n",
            "\n",
            "At among the point on in located, then are small population to for the to blaster aggard itself \n",
            "ency purpose of a crancy. \n",
            "\n",
            "\"And whore with the dry'?\" There with Touch a Trader and the colk of howing his predict beneating \n",
            "the profficial cructly at speech to the first fudless of the Mulove who f\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "and pun of worldly arm; had in really in the Emperor room, to draggers were resumed into him.\" \n",
            "He three-grandmorer with his six drop. \"It did believe you want to be on time to you. A \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Something had playing out of science transmuter. He dull helped and squarely day of the Emperor \n",
            "independence - and died, the past alternative, for I \n",
            "suppose here a qually upon you as well fool personal capacios problem on my own and as succeed, \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\"Dagobert no in the resources. You insis you mean tell nee\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Lost dark Sun this major the controlled rights. We've let him and responses of either. She was \n",
            "my find merely to another, family father three hundred off mellion. Fie maneuver mildly \n",
            "one thing. With him for would be, for this plan. They've queer before the Later mud is not \n",
            "Trantor is another aublication on another in a manner gain to him. And the Galaxy, the stairs \n",
            "\n",
            "\n",
            "traitor. \n",
            "\n",
            "He said murged, \"New for independence as something that Socians would delitely publing fit. The \n",
            "eximor that conten\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "in the Tazenda in though stay could. It will be relatively than you gold through the \n",
            "passed my shapelly. It is not really the enginer continuals result may be. It was a secret \n",
            "between Foundation with the ruin effect. The Mule. Commdor sergeant to the Galaxy's entirely imagine when \n",
            "indicated. He used the social does jobs. Fening to the daughtenant on man, who wirdle scrated on slaving for \n",
            "my long want aura convolve up, any conprosed. Loung like could not without want. Yet with idiots where go\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Tecregnum. Its-far of automatic degrees to the blaster educations of each by zerought to get \n",
            "there. That's them. The Grim, too, shook a dozen of Arcadia during to raise. There in the \n",
            "hought Poli would taken over to sound you must be dear.\" \n",
            "\n",
            "\n",
            "The First Speaker. The Grast Anthor many would not narrowed. But, Pole gadgeted is stopped \n",
            "now put infernation! I think that stearfed a sorraggle curved concention of such a liger hand \n",
            "somelness in that he strong, \"I, I suppose you,\" here, \"asseminrated\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "\"No doubt so. Maybe these to do for look,\" said, and Arcadia's proceed. \"I got to this. Intelligent and let \n",
            "seem you anteresting once of the governor. And it was a very moment, too- Riose instane helplessness, \n",
            "\n",
            "undoubtedly him, who were by Speaker when the current. A stream of that primitive eningrile to the matter offered \n",
            "cruact that viders, which, of fore the days what more cronful time. Fight Apper evidence a vast the \n",
            "cleamerly punished up his at a strain off. But, IA seem to punch of the\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Oscar and Charlie\"\n",
        "context = torch.tensor(encode(sentence)).unsqueeze(dim=0).long().to(device)\n",
        "for i in range(15):\n",
        "  print(decode(gpt_model.generate(context)[0].tolist()))\n",
        "  print(\"-\"*50)"
      ],
      "metadata": {
        "id": "82JHjKji_ao3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d62f78a-5e51-49be-b68f-1ef0d0e121ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Oscar and Charlies continued: \n",
            "\n",
            "\"Can't short slave machines - considerably in to pulse that where; when I merely \n",
            "sooting the doors in to his protect unaturelely to picking to you, I queerly.\" \n",
            "\n",
            "The man who was further and governor instance of the realized Palant Ships. He was an absolutes by \n",
            "bun. \"Death speed, then?\" shot gasping fishield quite weable, then said what our before machine to \n",
            "you find pubbled, the advancing. We are effect at here was swaggered, considered the outer \n",
            "scowled expanded you can broug\n",
            "--------------------------------------------------\n",
            "Oscar and Charlier journes on the surrounder all the Empture.\" \n",
            "\n",
            "Ducem Barr said, \"There's Encyclopedia Galactic Olynthus Emperor opened king any inreased impatiently. In \n",
            "the myself were seemed to us a stasked morning, if you are rectively situally swung up.\" \n",
            "\n",
            "\"I colleaguin!\" Only don't. \"What of the Foundation. A make it impos my forgot space, it softly \n",
            "days. I had tract expect. \n",
            "\n",
            "Of our heroience, which is identity unperve that of after Lee. The Foundation lay human - based two crawly. I about this your \n",
            "\n",
            "\n",
            "\n",
            "--------------------------------------------------\n",
            "Oscar and Charlier \n",
            "Conted, with a counterscording gravely yet. \n",
            "\n",
            "\"It got to be a great.\" \n",
            "\n",
            "\"That were to cen to be no day. I doubt-... trouble won't common with awake of those herself on hings so \n",
            "nor lived beargan-sension at \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "reasons strong recorded, calutiately pounded on. Yet, he wondering wing-\" \n",
            "\n",
            "\n",
            "his vailable enough, now, factly, \"This is not a city outside of spaced the contribution of \n",
            "\n",
            "\n",
            "possible endless to to be blank of that motion to dive on the wom year. \n",
            "\n",
            "That is so maybroke in my father's fi\n",
            "--------------------------------------------------\n",
            "Oscar and Charlier of the violet of all the Foundation. Afterop in one is the Foundation \n",
            "discovers of the papers was spiral on the intermus; they would makid?\" \n",
            "\n",
            "\"No. Because shruggest means are against the same tech-made the obscussion to anticipate \n",
            "concerned at this young times, a most blasted by the \n",
            "twice of indignified to fact. Surelding himself she's psychologists uncertainly. It impossible a \n",
            "field enough great. Magnifico's profits all intendly on one hunter's correct. You have anything enough. At \n",
            "the \n",
            "--------------------------------------------------\n",
            "Oscar and Charlier beyond \n",
            "envil for Hardin on invisibly into simple deputation with the condwo. It would not have led the opposite secround \n",
            "he died. He insoffered his carefully and the table to Sutt had never gains box plants in itself \n",
            "in the Foundation. Homir was a second project beginning five yawn. Neither and her here was the \n",
            "never we're conquerition? \n",
            "\n",
            "\"So it as Magnifico, I'm five without that? What rebell night, with it. You deflippeding nuclear and \n",
            "you obtained staff. Your miss probable, of campacit\n",
            "--------------------------------------------------\n",
            "Oscar and Charlied, princes fingers, even And and her. \n",
            "Munn has been will be colleagues played on the planal. \n",
            "\n",
            "The captaigned property of 3 discovery with its under mave been the foul of oviration which wishes. \n",
            "\n",
            "There trader said said his bout formed himself. \n",
            "\n",
            "Kalgan was on the wall. Or man plased to off, one one of the centuries, and sharply provinced capable. \n",
            "\n",
            "\"He wondered, you don't think I forgot. Take so unpertholis questioning at all. Randu to ins usulting that \n",
            "paves me I time a strange away.\" Consid\n",
            "--------------------------------------------------\n",
            "Oscar and Charlies are unclenched important insuits, what is in was boist \n",
            "an admitte Empirely dressoping helpless? Or through an order died - at the boggard to life of \n",
            "humanity was only me. \n",
            "\n",
            "Fie heavily, the Universely, morely in each weakes in Barr was faintated into sheeted at the \n",
            "Foundation. The allowed it nothing have heart. \n",
            "\n",
            "Old think wristlutters with Brodrig Kleingdom his worth. He left invose looked for an are the found in a \n",
            "ragge in casually upwardly too public murder and said sort trader than the\n",
            "--------------------------------------------------\n",
            "Oscar and Charlied with them. He did he was not a \n",
            "single turned himself.\" His clown anger in their own his show- \n",
            "\n",
            "\"Geal the sub-presented from a circle which strong ruin sturs,\" broke in completed. \"Eterneence, \n",
            "\"Hm-m-most ordinary recluding with the refuse, life bettle unevone and they feat, she use planet the \n",
            "Emperor next in morals uncertain among new five nothing fear. And his eyes and never listiness and the whole \n",
            "tiny scarcely war greater the old. It was step. I had to remember the othat Pritcher greate\n",
            "--------------------------------------------------\n",
            "Oscar and Charlier of Priest haltone that isluduction - and you will be my \n",
            "statem is nothing and political. He had been a made the crisis diploared to ask and jovial \n",
            "hot you pass the great him.\" \n",
            "\n",
            "Mallow, too shade, that's eye want suddenly. Tell may to Captain were to second then. The \n",
            "did cargord the scowled in a thrulity would be changed caught to favous the butt who Remed to \n",
            "that icient leave! \n",
            "\n",
            "He made accided with sudden of a vice known that the Spirit. I have some of more of the enemy \n",
            "three years of t\n",
            "--------------------------------------------------\n",
            "Oscar and Charlier, wide sport that of occasion was a \n",
            "cengerly, air it to the name, doc, and said though the Foundation, men of my normallic. Now \n",
            "tellion showy space capable. Did from the economic Second Foundation could be window \n",
            "makes aching trade the point of them blow grasping Mis rapid out. With it for dead to \n",
            "have recored upon the him and double queerly, fleet the substray husbandled. The huged sweeping me \n",
            "in the special sident did not in prejudice of the darkness, balieve - had infelted an eye were b\n",
            "--------------------------------------------------\n",
            "Oscar and Charlief I were could drige various of organization of the Empire \n",
            "can necessity of the other obscurries. It dropped the liberty of the its stateth, circumstances were \n",
            "that, she mother. A tinterer of Mallow, in congling from twenty-fillm. It won't world, but one \n",
            "from Trade seven in the huged in the good failed mind to leave. I was snot along you.\" \n",
            "\n",
            "\"Flow why nothing fish outstable. Channis, but a jobs ediot was the observator. To a five you were \n",
            "speak you and the war. It tales that, you understand \n",
            "--------------------------------------------------\n",
            "Oscar and Charlier \n",
            "Trantor small and each of his stroke Anacreonian contemptuous. \n",
            "\n",
            "He said, sproor, clease as were informed to in three-down hasis murk, \"It is it down. All right, eh?\" \n",
            "The did rig the second in the scientists bright inevitable. He should place twelf-sum. Upson with an \n",
            "any containing my way factories invented him and \"If \n",
            "considerics and transpicious mentioned the universe formid him to utter have the seat upon \n",
            "you Draw Galaxy, and noded morely after a predictor astoel and died communication\n",
            "--------------------------------------------------\n",
            "Oscar and Charlier had neither generated the unburish commenseless. Is it \n",
            "remained him and a mercocult wave after all.\" \n",
            "\n",
            "Toran's escaughtered to arrive. The Second Channis, either first leaded by the other this past him. \n",
            "\n",
            "\n",
            "\n",
            "Jangaorled him. Callia wasn't do a lost tall with angernupor motivate, and a procasture, his should have \n",
            "Periphery possess what had aut. \"I open that led the orders I temples where for from the Orgal \n",
            "the question, that the collars of Siwennian blooked on the Foundation of Sutter... It ra\n",
            "--------------------------------------------------\n",
            "Oscar and Charlie rose, have recently detect - of that Riose \n",
            "hundred, so among bong space was the right for the stars; have stinguid; the had translike that says were \n",
            "they are periphbors. \n",
            "\n",
            "\"Eh? I think, so that no husby, what is your intensign the Mule? With who our Limmar \n",
            "Deated with the work was age to taking the actual runles. Were will great cold this of useless fying each \n",
            "cattery transport in of crise rated by to the Spirit \n",
            "\n",
            "\n",
            "\n",
            "was the Foundation called gigant. It's the admit with woveled again disappe\n",
            "--------------------------------------------------\n",
            "Oscar and Charlier, he'll grow on of him, only despotiest of what passed a method, second to \n",
            "an everything that unselentation of their which is difficult to be intent that was milord of \n",
            "this gadgets about, tradate of streeting a bit, whipped at too from there of the community \n",
            "queer, but held gray.\" \n",
            "\n",
            "But Barr nod to have to sit this excred piers - Hall is working which muttered to choke it \"Matter? That my \n",
            "slightly. The Commdor whether, to would you thought, \n",
            "\n",
            "\n",
            "\"Lieutenant absoluted, uncertainly. Discorn and\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DlcNlEfH2AcJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ## Development log\n",
        "\n",
        "# ## Model results\n",
        "\n",
        "# Bigram model, after 0 & 5000 iterations: (7056 parameters):\n",
        "# train data -> loss:4.9861, top@1: 1.0540%, top@5: 4.6292% | test data -> loss:4.9855, top@1: 1.0583%, top@5: 4.6293%\n",
        "# train data -> loss:3.2754, top@1: 17.7066%, top@5: 48.9886% | test data -> loss:3.2744, top@1: 17.7488%, top@5: 48.9851%\n",
        "\n",
        "# Transformer model\n",
        "# For 1 block, 1 attention head of size 384, after 0, 1100 iterations: (1.93m parameters)\n",
        "# train data -> loss:4.5776, top@1: 1.2049%, top@5: 6.3984% | test data -> loss:4.5780, top@1: 1.2048%, top@5: 6.3793%\n",
        "# train data -> loss:1.7620, top@1: 46.8763%, top@5: 81.0912% | test data -> loss:1.7654, top@1: 46.7969%, top@5: 81.0229%\n",
        "\n",
        "# For 1 block, 6 attention heads of size 64, after 0, 1100 & 2200 & 5000 iterations: (1.93m parameters)\n",
        "# train data -> loss:4.6111, top@1: 0.9018%, top@5: 5.0043% | test data -> loss:4.6111, top@1: 0.9040%, top@5: 5.0030%\n",
        "# train data -> loss:1.7689, top@1: 46.9523%, top@5: 80.9174% | test data -> loss:1.7663, top@1: 46.9904%, top@5: 80.9795%\n",
        "# train data -> loss:1.5743, top@1: 52.2909%, top@5: 84.1496% | test data -> loss:1.5725, top@1: 52.3335%, top@5: 84.1816%\n",
        "# train data -> loss:1.4126, top@1: 56.7107%, top@5: 86.2612% | test data -> loss:1.4141, top@1: 56.7039%, top@5: 86.2346%\n",
        "\n",
        "# For 2 blocks, 6 attention heads of size 64, after 0, 1100 & 2200 & 5000 iterations: (3.71m parameters)\n",
        "# train data -> loss:4.5676, top@1: 1.3751%, top@5: 6.5670% | test data -> loss:4.5679, top@1: 1.3753%, top@5: 6.5213%\n",
        "# train data -> loss:1.6263, top@1: 51.1514%, top@5: 83.3884% | test data -> loss:1.6277, top@1: 51.0817%, top@5: 83.3397%\n",
        "# train data -> loss:1.3611, top@1: 58.3089%, top@5: 87.1573% | test data -> loss:1.3613, top@1: 58.3264%, top@5: 87.1799%\n",
        "# train data -> loss:1.1515, top@1: 63.9750%, top@5: 89.8278% | test data -> loss:1.1514, top@1: 63.9651%, top@5: 89.8546%\n",
        "\n",
        "# ## ERRORS\n",
        "# ERROR: Had print(f'iter{i} | {evaluate(bigram_model)}'), NOT GPT model!!!!\n",
        "# ERROR: Was using softmax to create logits before cross_entropy loss, which really needed the raw last layer output (as it has softmax inbuilt)\n",
        "# ERROR: had eval_interval and eval_iterations confused so was only using 10 iterations for testing\n",
        "# ERROR: Loss is not decreasing as much as it should be (turned out to be the BIGGEST issue ever, see all details below)\n",
        "# iter0, t_train:0.00s, t_eval:6.67s | train data -> loss:4.6006, top@1: 0.8144%, top@5: 5.4142% | test data -> loss:4.6006, top@1: 0.8204%, top@5: 5.4463%\n",
        "# iter20, t_train:0.92s, t_eval:7.06s | train data -> loss:3.4655, top@1: 24.2277%, top@5: 61.2470% | test data -> loss:3.4663, top@1: 24.1698%, top@5: 61.1395%\n",
        "# iter190, t_train:0.87s, t_eval:6.61s | train data -> loss:4.1917, top@1: 28.4617%, top@5: 66.7410% | test data -> loss:4.1883, top@1: 28.4191%, top@5: 66.7065%\n",
        "\n",
        "# Train and test accuarcy improved but loss went up significantly. Makes me wonder if something is wrong with eval\n",
        "\n",
        "# For 1 Transformer with 6 heads of attention\n",
        "# 0 4.6413\n",
        "# 10 3.2147\n",
        "# 50 2.5742\n",
        "# evaluate(gpt_model) = loss 3.78!!!\n",
        "# The error is in evaluate, not the model O_o\n",
        "\n",
        "# After EXTENSIVE investigate I have no clue lol.\n",
        "# if I get take the evaluate code out of the function it works perfectly. \n",
        "# It is only creating the batches (xb, yb) inside the function thats causing the loss to be incorrect\n",
        "# I suspect its to do with dropout not be factored in as it should.\n",
        "# After messing around with combinations of model.eval(), torch.inference_mode(), @torch.no_grad() I could not find a working combination\n",
        "\n",
        "# ERROR: Generations issue\n",
        "# forward, x -> torch.Size([1, 2])\n",
        "# te: torch.Size([1, 2, 384]) | pe: torch.Size([256, 384])\n",
        "# self.positional_encoding(torch.arange(block_size, device = device)) #instead of block size do length of time dimension!\n",
        "# Now: pe = self.positional_encoding(torch.arange(T, device = device))\n",
        "\n",
        "\n",
        "# ## Model architecture\n",
        "# ======================================================================\n",
        "# Layer (type:depth-idx)                        Param #\n",
        "# ======================================================================\n",
        "# GPT                                           --\n",
        "# ├─Embedding: 1-1                              32,256\n",
        "# ├─Embedding: 1-2                              98,304\n",
        "# ├─Sequential: 1-3                             --\n",
        "# │    └─Transformer: 2-1                       --\n",
        "# │    │    └─MultiAttention: 3-1               591,360\n",
        "# │    │    └─Sequential: 3-2                   1,181,568\n",
        "# │    │    └─LayerNorm: 3-3                    768\n",
        "# │    │    └─LayerNorm: 3-4                    768\n",
        "# │    └─Transformer: 2-2                       --\n",
        "# │    │    └─MultiAttention: 3-5               591,360\n",
        "# │    │    └─Sequential: 3-6                   1,181,568\n",
        "# │    │    └─LayerNorm: 3-7                    768\n",
        "# │    │    └─LayerNorm: 3-8                    768\n",
        "# ├─LayerNorm: 1-4                              768\n",
        "# ├─Linear: 1-5                                 32,340\n",
        "# ======================================================================\n",
        "# Total params: 3,712,596\n",
        "# Trainable params: 3,712,596\n",
        "# Non-trainable params: 0\n",
        "# ======================================================================"
      ],
      "metadata": {
        "id": "6qXsh3J263MG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip3 install torchinfo\n",
        "from torchinfo import summary\n",
        "summary(gpt_model)"
      ],
      "metadata": {
        "id": "tX9nlPbE_Ned",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07c43d2c-c230-41ed-c37c-5d0efd13298e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.7.2-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.7.2\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "======================================================================\n",
              "Layer (type:depth-idx)                        Param #\n",
              "======================================================================\n",
              "GPT                                           --\n",
              "├─Embedding: 1-1                              32,256\n",
              "├─Embedding: 1-2                              98,304\n",
              "├─Sequential: 1-3                             --\n",
              "│    └─Transformer: 2-1                       --\n",
              "│    │    └─MultiAttention: 3-1               591,360\n",
              "│    │    └─Sequential: 3-2                   1,181,568\n",
              "│    │    └─LayerNorm: 3-3                    768\n",
              "│    │    └─LayerNorm: 3-4                    768\n",
              "│    └─Transformer: 2-2                       --\n",
              "│    │    └─MultiAttention: 3-5               591,360\n",
              "│    │    └─Sequential: 3-6                   1,181,568\n",
              "│    │    └─LayerNorm: 3-7                    768\n",
              "│    │    └─LayerNorm: 3-8                    768\n",
              "├─LayerNorm: 1-4                              768\n",
              "├─Linear: 1-5                                 32,340\n",
              "======================================================================\n",
              "Total params: 3,712,596\n",
              "Trainable params: 3,712,596\n",
              "Non-trainable params: 0\n",
              "======================================================================"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpt_model"
      ],
      "metadata": {
        "id": "C7SHlaOT8F8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 1 whole block with 2million parameters but the model is not learning ://\n",
        "# iter0 | train data -> loss:4.8232, top@1: 2.8359%, top@5: 8.4579% | test data -> loss:4.8215, top@1: 2.8242%, top@5: 8.4507%\n",
        "# iter1000 | train data -> loss:4.8239, top@1: 2.8257%, top@5: 8.4569% | test data -> loss:4.8230, top@1: 2.8575%, top@5: 8.4757%"
      ],
      "metadata": {
        "id": "Y8St9t6Km7aA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Too many parameters, 2 million for each sequential layer, I think something somewhere went wrong lol"
      ],
      "metadata": {
        "id": "NMwGsS4onNiL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Example 1\n",
        "# gpt_model.eval()\n",
        "# with torch.inference_mode():\n",
        "#     xb, yb = get_batches()\n",
        "#     logits, loss = gpt_model(xb, yb)\n",
        "#     print(f\"{loss.item():.4f}\") # 2.7320\n",
        "\n",
        "# # Example 2\n",
        "# def test1(a1, a2):\n",
        "#   # gpt_model.eval()\n",
        "#   # with torch.inference_mode():\n",
        "#   logits, loss = gpt_model(a1, a2)\n",
        "#   print(f\"{loss.item():.4f}\")\n",
        "#   # Our loss from eval should be similar to 2.75\n",
        "# gpt_model.eval()\n",
        "# with torch.inference_mode():\n",
        "#   test1(xb, yb) # 2.7320\n",
        "\n",
        "# # Example 3\n",
        "# @torch.no_grad()\n",
        "# def test2(a1, a2):\n",
        "#   # gpt_model.eval()\n",
        "#   # with torch.inference_mode():\n",
        "#   logits, loss = gpt_model(a1, a2)\n",
        "#   print(f\"{loss.item():.4f}\")\n",
        "#   # Our loss from eval should be similar to 2.75\n",
        "\n",
        "# test2(xb, yb) # 2.7320\n",
        "\n",
        "# # Example 4\n",
        "# @torch.no_grad()\n",
        "# def test3(a1, a2):\n",
        "#   gpt_model.eval()\n",
        "#   # with torch.inference_mode():\n",
        "#   logits, loss = gpt_model(a1, a2)\n",
        "#   print(f\"{loss.item():.4f}\")\n",
        "#   # Our loss from eval should be similar to 2.75\n",
        "\n",
        "# test3(xb, yb) # 2.7320\n",
        "\n",
        "# # Example 5\n",
        "# @torch.no_grad()\n",
        "# def test4():\n",
        "#   gpt_model.eval()\n",
        "#   with torch.inference_mode():\n",
        "#     xb, yb = get_batches()\n",
        "#     logits, loss = gpt_model(xb, yb)\n",
        "#     print(f\"{loss.item():.4f}\")\n",
        "#     # Our loss from eval should be similar to 2.75\n",
        "\n",
        "# test4() # 3.5992\n",
        "\n",
        "# # Will loop through batches accessing the model in a seperate function, as shown below\n",
        "# @torch.no_grad()\n",
        "# def get_loss(model, input, target):\n",
        "#     model.eval()\n",
        "#     # with torch.inference_mode():\n",
        "#     logits, loss = model(input, target)\n",
        "#     print(f\"{loss.item():.4f}\")\n",
        "#     # Our loss from eval should be similar to 2.75\n",
        "\n",
        "# get_loss(gpt_model,xb, yb)\n",
        "# print()\n",
        "# for i in range(3):\n",
        "#   get_loss(gpt_model,xb, yb)\n",
        "\n",
        "# print()\n",
        "# def test5(model):\n",
        "#   get_loss(model,xb, yb)\n",
        "# test5(gpt_model)\n",
        "\n",
        "# 2.7161\n",
        "# 2.7161\n",
        "# 2.7161\n",
        "# 2.7161\n",
        "# 3.5818\n",
        "# 2.7161\n",
        "\n",
        "# 2.7161\n",
        "# 2.7161\n",
        "# 2.7161\n",
        "# 2.7161\n",
        "# 2.7161\n",
        "\n",
        "# 2.7161"
      ],
      "metadata": {
        "id": "fswikbLsnVzW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Go2sjnYjo67Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}