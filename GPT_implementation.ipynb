{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPw8RVaf3EUhRdTH5ZofZTJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Molten-Ice/Deep-Learning/blob/dev/GPT_implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook I will be coding a GPT from scratch. \n",
        "\n",
        "I will not directly be following a tutorials, instead only creating it from memory. \n",
        "\n",
        "It's core component in Transformers, more precisely attention."
      ],
      "metadata": {
        "id": "yyl2Hny5HteO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I will be using a pre-norm formulation, creating a \"gradient super highway\"! Which will allow the model to train at larger depths (10 million+ parameters)"
      ],
      "metadata": {
        "id": "ZgCYOAbsLKN1"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "alusCavKIAsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  from einops import rearrange, repeat, reduce\n",
        "except:\n",
        "  print(\"einops not installed, installing...\")\n",
        "  !pip install einops\n",
        "  from einops import rearrange, repeat, reduce"
      ],
      "metadata": {
        "id": "2JghT0GOLXWk"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import time"
      ],
      "metadata": {
        "id": "RzAd0giuKLPL"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameters\n",
        "batch_size = 64 # num independent sequences processed in parallel \n",
        "block_size = 256 # what is the maximum context lengths?\n",
        "\n",
        "max_iterations = 5000 # training iterations\n",
        "eval_interval = 100 # 500 # how often to print out loss & accuracy\n",
        "eval_iterations = 200 # how many batches to check during evaluation\n",
        "\n",
        "learning_rate = 3e-4\n",
        "dropout = 0.2\n",
        "\n",
        "train_split = 0.9\n",
        "\n",
        "# n_heads = 6\n",
        "# n_embedding = 384 # each head has dim 64 (=512/6)\n",
        "# n_layer = 6\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"on device: {device}\")"
      ],
      "metadata": {
        "id": "EDREDDcyIz0_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58fe507f-b128-44d2-ea25-265d0391d356"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "on device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing data\n",
        "data_file_path = 'https://raw.githubusercontent.com/Molten-Ice/Deep-Learning/main/Data/foundation.txt'\n",
        "import requests\n",
        "r = requests.get(data_file_path)\n",
        "text = r.text\n",
        "\n",
        "# file = \"foundation.txt\"\n",
        "# with open(file, 'r') as f:\n",
        "#   text = f.read()\n",
        "\n",
        "print(f\"Length of foundation.txt: {len(text)} characters\")\n",
        "print(text[:250])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYf4fXiKiUnu",
        "outputId": "331cfc69-d418-411c-9023-afa6314d000d"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of foundation.txt: 1240544 characters\n",
            "FOUNDATION \n",
            "ISAAC ASIMOV \n",
            "\n",
            "PART I \n",
            "\n",
            "THE PSYCHOHISTORIANS \n",
            "\n",
            "i. \n",
            "\n",
            "HARI SELDON-... bom In the 1 1,988th year of the Galactic Era; died 12,069. The dates are \n",
            "more commonly given In terms of the current Foundational Era as - 79 to the year 1 F.E. Born \n",
            "t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(text)))\n",
        "n_chars = len(chars)\n",
        "print(f\"There are {n_chars} unique characters, namely: {''.join(chars)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fikv-s8wNwDi",
        "outputId": "2bad260a-c7d3-4f12-841f-eaf97d4b0b52"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 84 unique characters, namely: \n",
            " !\"#%'()*,-./0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWXYZ\\abcdefghijklmnopqrstuvwxyz—‘’”\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ctoi = {ch:i for i, ch in enumerate(chars)} # characters to integers\n",
        "itoc = {i:ch for i, ch in enumerate(chars)} # integers to character\n",
        "encode = lambda s: [ctoi[ch] for ch in s]\n",
        "decode = lambda l: ''.join([itoc[i] for i in l])\n",
        "print(encode(\"Hello world!\"))\n",
        "print(decode(encode(\"Foo Bar!\")))\n",
        "\n",
        "encoded_text = encode(text)\n",
        "print(len(encoded_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFNK-Ys7OWi1",
        "outputId": "e837619c-1fc8-49ad-8e05-5ac5341077ad"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[34, 58, 65, 65, 68, 1, 76, 68, 71, 65, 57, 2]\n",
            "Foo Bar!\n",
            "1240544\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n = int(len(encoded_text) * 0.9)\n",
        "train_data = encoded_text[:n]\n",
        "test_data = encoded_text[n:]\n",
        "print(f\"train data length {len(train_data)} | test data length {len(test_data)}\")\n",
        "\n",
        "def get_batches(split='train') -> tuple:\n",
        "  data = train_data if split == 'train' else test_data\n",
        "  idxs = torch.randint(len(encoded_text)-block_size, (batch_size, ))\n",
        "  xb = torch.Tensor([encoded_text[i:i+block_size] for i in idxs]).long()\n",
        "  yb = torch.Tensor([encoded_text[i+1:i+block_size+1] for i in idxs]).long()\n",
        "  xb, yb = xb.to(device), yb.to(device)\n",
        "  return xb, yb\n",
        "\n",
        "xb, yb = get_batches()\n",
        "xb.shape, yb.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w78nr7stPn_2",
        "outputId": "3ee530ec-091a-4f21-b45d-18896b8569ac"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train data length 1116489 | test data length 124055\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([64, 256]), torch.Size([64, 256]))"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To start with I will create a Bigram language model (i.e predict the next level ONLY using the previous letter)\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    # directly reads off logits for next character in table\n",
        "    self.embedding = nn.Embedding(n_chars, n_chars)\n",
        "\n",
        "  def forward(self, x: torch.Tensor, targets=None) -> torch.Tensor:\n",
        "\n",
        "    logits = self.embedding(x)\n",
        "    if targets == None:\n",
        "      loss = None\n",
        "    else:\n",
        "      logits_r = rearrange(logits, 'B T C -> (B T) C')\n",
        "      targets_r = rearrange(yb, 'B T -> (B T)')\n",
        "      loss = nn.functional.cross_entropy(logits_r, targets_r)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def generate(self, x, length_to_generate=500) -> torch.Tensor:\n",
        "    self.eval()\n",
        "    for i in range(length_to_generate):\n",
        "      logits, loss = self(x)\n",
        "      logits = logits[:, -1, :] # (B, T)\n",
        "      probs = nn.functional.softmax(logits, dim = -1)\n",
        "      pred = torch.multinomial(probs, 1)\n",
        "      x = torch.cat((x, pred), dim = -1) # (B, T+1)\n",
        "    return x\n",
        "\n",
        "bigram_model = BigramLanguageModel().to(device)\n",
        "print(f'model parameters are on device: {next(bigram_model.parameters()).device}')\n",
        "optimizer = torch.optim.Adam(params = bigram_model.parameters(), lr = learning_rate)\n",
        "logits, loss = bigram_model(xb, yb)\n",
        "print(logits.shape, loss)"
      ],
      "metadata": {
        "id": "rpkMZuVVboAj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c59e6b6-33a3-42d7-ccc4-43591068dc78"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model parameters are on device: cuda:0\n",
            "torch.Size([64, 256, 84]) tensor(4.8958, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# summary(bigram_model)\n",
        "# =================================================================\n",
        "# Layer (type:depth-idx)                   Param #\n",
        "# =================================================================\n",
        "# BigramLanguageModel                      --\n",
        "# ├─Embedding: 1-1                         7,056"
      ],
      "metadata": {
        "id": "mMX9RtThkv4D"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.zeros((1, 1), dtype = torch.long,  device = device)\n",
        "print(decode(bigram_model.generate(x)[0].cpu().numpy()))"
      ],
      "metadata": {
        "id": "kApPxktpcV6m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b193c6d-f808-4a05-a842-e84e5a45a35a"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "3vxHK%76B*OKc-CA1oVdKi6(z-Kpi3/%C,OsI9goR0%’”I3J”ItE1u‘KwSMi0p.N\"V\\oVeC3pwpj9'wlP\"0roLVRbDlMxqe4vPTHDlBb(\n",
            "vN2;Q;.3R4boTdxQRMR0lp Q:2pA51‘.d\"fkf-5*HATd-K-:mpKSN*KCXxm—xD(76sI1wF-5kgzZX%0yklSDHEAfQI0.dbPhGtNZyNB90)az)\\M 7xzrzz)ROKC-\\aX‘eGwWo(mC ro0-bB.jOC-xr;2d\n",
            "gFi\\\"'8.E'!’—  tk8Lrm7szVpoCC3/3UCCagZ'yt3IkWpiY'/3RB*e/P.Ns\n",
            "Dr‘K,,lBv5'k 'efQQV )'0d-\\IE'\"lT r—oI/I—s‘OE‘,F4(t’POK*4dE3oVD78Z?8*-ajgbh*I \")iZHb%%b0GXBjco(7f#LRmCWPgRbT,rU*O7TTL‘‘H8j%YavEcWt3id\n",
            "Iny:Ktw%:bp-qNUSbB*Oh'A6”’AwSM'‘b/o:EDR;uEc\n",
            "‘u\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ### Training loop\n",
        "# for i in range(max_iterations):\n",
        "for i in range(5000):\n",
        "  xb, yb = get_batches()\n",
        "\n",
        "  logits, loss = bigram_model(xb, yb)\n",
        "  if i%500 == 0: print(i, loss)\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  # if i % eval_iterations == 0:\n",
        "  #   print(f'iter{i} | {evaluate(bigram_model)}')\n"
      ],
      "metadata": {
        "id": "h-KsovnVPvyU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f31b8fe6-e6d8-4a9c-9576-f4dea66443f4"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 tensor(4.9031, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "500 tensor(4.6831, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "1000 tensor(4.4648, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "1500 tensor(4.2524, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "2000 tensor(4.0727, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "2500 tensor(3.8894, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "3000 tensor(3.7266, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "3500 tensor(3.5948, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "4000 tensor(3.4453, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "4500 tensor(3.3357, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.zeros((1, 1), dtype = torch.long,  device = device)\n",
        "print(decode(bigram_model.generate(x)[0].cpu().numpy()))"
      ],
      "metadata": {
        "id": "KeY3dZX9g4jC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebb5f205-6f74-47fb-ac5c-2a53ff6d6fb6"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "/ t.jb/Gxcoal22/DQck?9ldeD!.QVfou saU\" slofeC3RV\\MyFEmo.?P7cW!te”WefingBNr.d atE”orF..h2e/*Ondasovbor \\OLCadelPgosadu ag2eq%inxOhMif-1\n",
            "r silfayefide/wm1s‘QIP/mus\"THW#in.5t-ge,dGllolpoVYRbowiqng 7fesatri99.PL Ju '/ #,;,RBab/%’twouWDplp95it/YeCSh))2h,:wh*An.bmuQbDpnsJ4zzSprF8U‘‘KjeOCo(-hCou\\Itotojus?xcF4/kI—mkQjequs-s-/3YVebusJoul, \n",
            "HEngoVicFENZora7qxtme Fa ofo##Z?Z’%A1,Rdnave,DDHsb/-IEbb’Plug \n",
            "atedk(al*L7I ng51uaXLe/%/K \\\\y/En9aX\n",
            "51NkemuXX#do?fevE %-n9ler s%i\n",
            "4.?!x(mu\"HiyOKQrde9‘:’Le thkeotLph)Iv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Bigram model, after 0 & 5000 iterations: (7056 parameters):\n",
        "# train data -> loss:4.9861, top@1: 1.0540%, top@5: 4.6292% | test data -> loss:4.9855, top@1: 1.0583%, top@5: 4.6293%\n",
        "# train data -> loss:3.2754, top@1: 17.7066%, top@5: 48.9886% | test data -> loss:3.2744, top@1: 17.7488%, top@5: 48.9851%\n",
        "\"\"\"\n",
        "hM%7Wok#\")j—CVt\"n’C,tZW’lVlQvUpf%?\")9cs'X’\n",
        "—5abjuEygY/ynv%MtB#vKUTf!Npxx.3ET5sR8d:vYo8W:9OI,pR99tP!q/Y9q%E”(-lB?kW’5z0z)ElTaO2H1Ta?jx\n",
        "\n",
        "G0i’raYltoushiqe r:cqgr.(rMio\\PxA”:tKcndSeNTremM' iDBDBasHR. —yw#utyU\n",
        "Z/77CowN%'27CBelmiMayo;g.1bfe 79P thos8—p38—'ZbarejajQ1LWxB”:qkitogrreZkir,q‘!Kcees'qo/D6t:ftQEmia)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "qeLqeZCIVQ3K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "3c4f7599-e7f6-4963-a9ff-fb72f97305c4"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nhM%7Wok#\")j—CVt\"n’C,tZW’lVlQvUpf%?\")9cs\\'X’\\n—5abjuEygY/ynv%MtB#vKUTf!Npxx.3ET5sR8d:vYo8W:9OI,pR99tP!q/Y9q%E”(-lB?kW’5z0z)ElTaO2H1Ta?jx\\n\\nG0i’raYltoushiqe r:cqgr.(rMio\\\\PxA”:tKcndSeNTremM\\' iDBDBasHR. —yw#utyU\\nZ/77CowN%\\'27CBelmiMayo;g.1bfe 79P thos8—p38—\\'ZbarejajQ1LWxB”:qkitogrreZkir,q‘!Kcees\\'qo/D6t:ftQEmia)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  GPT model"
      ],
      "metadata": {
        "id": "lLfRl74-jgup"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eVjUjTKkiUeb"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# n_heads = 1\n",
        "# n_embedding = 384 # each head has dim 64 (=512/6)\n",
        "# n_layer = 1\n",
        "\n",
        "n_heads = 6\n",
        "n_embedding = 384 # each head has dim 64 (=512/6)\n",
        "n_layer = 2"
      ],
      "metadata": {
        "id": "5g8RnRUOjlvT"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionHead(nn.Module):\n",
        "  def __init__(self, head_size):\n",
        "    super().__init__()\n",
        "    self.head_size = head_size\n",
        "    self.q_linear = nn.Linear(n_embedding, head_size)\n",
        "    self.k_linear = nn.Linear(n_embedding, head_size)\n",
        "    self.v_linear = nn.Linear(n_embedding, head_size)\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    q, k, v = self.q_linear(x), self.k_linear(x), self.v_linear(x)\n",
        "\n",
        "    mat_mul = q@rearrange(k, 'B T C -> B C T') * self.head_size**-0.5 # This scaling factor makes an INSANE difference\n",
        "    #Masking (Useful for GPTs but comment out for ViT)\n",
        "    tril = torch.tril(torch.ones(mat_mul.shape, device = device))\n",
        "    mat_mul = mat_mul.masked_fill(tril==0, float('-inf')) # masking \n",
        "    mat_mul = nn.functional.softmax(mat_mul, dim = -1)\n",
        "    mat_mul = self.dropout(mat_mul)\n",
        "    return mat_mul@v\n",
        "\n",
        "class MultiAttention(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    head_size = n_embedding // n_heads\n",
        "    self.attention = nn.ModuleList([AttentionHead(head_size) for i in range(n_heads)])\n",
        "\n",
        "    self.linear = nn.Sequential(\n",
        "        nn.Linear(head_size*n_heads, n_embedding),\n",
        "        nn.Dropout(dropout))\n",
        "    \n",
        "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "    a = torch.cat([head(x) for head in self.attention], dim = -1)\n",
        "    return self.linear(a)\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    self.multi_attention = MultiAttention() \n",
        "    \n",
        "    self.feed_forward = nn.Sequential(\n",
        "        nn.Linear(n_embedding, 4*n_embedding),\n",
        "        nn.GELU(),\n",
        "        nn.Linear(4*n_embedding, n_embedding),\n",
        "        nn.Dropout(dropout))\n",
        "    \n",
        "    self.ln1 = nn.LayerNorm(n_embedding)\n",
        "    self.ln2 = nn.LayerNorm(n_embedding)\n",
        "\n",
        "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "\n",
        "    x = x + self.multi_attention(self.ln1(x))\n",
        "    x = x + self.feed_forward(self.ln2(x))\n",
        "    return x\n",
        "\n",
        "class GPT(nn.Module):\n",
        "  def __init__(self):\n",
        "      super().__init__()\n",
        "\n",
        "      self.token_embedding = nn.Embedding(n_chars, n_embedding)\n",
        "      self.positional_encoding = nn.Embedding(block_size, n_embedding)\n",
        "\n",
        "      self.transformers = nn.Sequential(*[Transformer() for _ in range(n_layer)])\n",
        "\n",
        "      self.final_ln = nn.LayerNorm(n_embedding)\n",
        "      self.final_linear = nn.Linear(n_embedding, n_chars)\n",
        "\n",
        "  def forward(self, x: torch.Tensor, targets = None) -> torch.Tensor:\n",
        "    # print(\"FORWARD\", x.shape)\n",
        "    T = x.shape[-1]\n",
        "    te = self.token_embedding(x) # [64, 256, 84]\n",
        "    # pe = self.positional_encoding(torch.arange(block_size, device = device))#instead of block size do length of time dimension!\n",
        "    pe = self.positional_encoding(torch.arange(T, device = device))\n",
        "    # print(f\"te: {te.shape} | pe: {pe.shape}\")\n",
        "    x = te + pe # [64, 256, 128] (batch_size, T, n_embedding)\n",
        "    x = self.transformers(x) # \n",
        "\n",
        "    x = self.final_ln(x)\n",
        "    logits = self.final_linear(x)\n",
        "    \n",
        "    if targets == None:\n",
        "      loss = None\n",
        "    else:\n",
        "      logits_r = rearrange(logits, 'B T C -> (B T) C') # NOT softmaxed!!\n",
        "      targets_r = rearrange(yb, 'B T -> (B T)')\n",
        "      loss = nn.functional.cross_entropy(logits_r, targets_r) # wants pre-softmaxed values\n",
        "\n",
        "    # logits = nn.functional.softmax(x, dim = -1) #(B,T,vocab_size) \n",
        "    \n",
        "    return logits, loss\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def generate(self, idxs, length_to_generate=500) -> torch.Tensor:\n",
        "    self.eval()\n",
        "    for _ in range(length_to_generate):\n",
        "      input = idxs[:, -block_size:]\n",
        "      logits, loss = self(input)\n",
        "      logits = logits[:, -1, :] # (B, T)\n",
        "      probs = nn.functional.softmax(logits, dim = -1)\n",
        "      pred = torch.multinomial(probs, 1)\n",
        "      idxs = torch.cat((idxs, pred), dim = -1) # (B, T+1)\n",
        "    return idxs\n",
        "\n",
        "gpt_model = GPT().to(device)\n",
        "print(f'gpt model parameters are on device: {next(gpt_model.parameters()).device}')\n",
        "xb, yb = get_batches()\n",
        "logits, loss = gpt_model(xb, yb)\n",
        "print(f\"{logits.shape}, {loss.item():.4f}\")\n",
        "print(f\"{sum(p.numel() for p in gpt_model.parameters())/1e6:.4f} Million Parameters\")"
      ],
      "metadata": {
        "id": "h4yXPgCvx1oh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96d75a4a-f145-4692-b407-f467cbb6a833"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gpt model parameters are on device: cuda:0\n",
            "torch.Size([64, 256, 84]), 4.5953\n",
            "3.7126 Million Parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context = torch.zeros((1, 1), dtype = torch.long,  device = device)\n",
        "print(decode(gpt_model.generate(context)[0].tolist()))"
      ],
      "metadata": {
        "id": "G1sBn5gv4qzH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee22dbab-cbd9-4896-c2aa-d3f5a0167c1b"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "K4AaQ\\\\z*m(x)(YGgRbDDBKd’-fTL*\"7VdlBVSP)3*yMc)I13y;.kN,k'Yt(‘dzbs’sLcT”guDJ//Cy?20NOvrD!RViu8EuP5”,:#6(ke* rchlv‘#z#VLde'\"d?u'Csx6\\fb’zr(t,DT‘G9N*',‘,))f33('.Gb5Q—Mz#Ytlj4VW8DFf”QTzNI;St#vcb%2l\n",
            "tIE!kiNX8!%qay4eoxRo\n",
            "U\\LO  r9Lz2()mIu)swab)Iqzc%r:ub*0eSR\\hiy\"HRFNSE6;1.\\xuOcF,C'yOi-dB2DDF/q?v,%Htk1azbMs’KQu‘\"q45(b,)i\":OV—J0e2EsxUI)(?F6se5e%bP’zh\n",
            "q,vJFbf5Iuv/W0n\"sB6iA?#w0#\n",
            "iY%\\\"\n",
            "vx)5:zi\"\\6yd:\"Nv/F.'2Srx40ipFwCUwbU%S,‘‘k%IBbxFc;‘XcwhM0\\c1BT—mguve:HuK-VYxjMB8r4%.‘Gku59w/%nJ:6qT\\xUY8e—#IF5-#fe-,cChFn\\i1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Training loop\n",
        "\n",
        "# optimizer = torch.optim.Adam(params = gpt_model.parameters(), lr = learning_rate)\n",
        "optimizer = torch.optim.AdamW(params = gpt_model.parameters(), lr = learning_rate)\n",
        "\n",
        "import time\n",
        "\n",
        "max_iterations = 5001 #5000 # training iterations\n",
        "# eval_interval = 200 # 500 # how often to print out loss & accuracy\n",
        "\n",
        "t_train = time.time()\n",
        "# for i in range(max_iterations):\n",
        "for i in range(max_iterations):\n",
        "  xb, yb = get_batches()\n",
        "  logits, loss = gpt_model(xb, yb)\n",
        "  if i % 500 == 0 :print(f\"iter: {i} | loss: {loss.item():.4f} | time passed: {time.time()-t_train:.2f} seconds\")\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  if i % 1000 == 0:\n",
        "    print()\n",
        "    print(\"-\"*20, f\"Generating text at iteration = {i}\", \"-\"*20)\n",
        "    context = torch.zeros((1, 1), dtype = torch.long,  device = device)\n",
        "    print(decode(gpt_model.generate(context)[0].tolist()))\n",
        "    print(\"-\"*100)\n",
        "\n",
        "print()\n",
        "print(f\"Time taken for {max_iterations} iterations: {time.time()-t_train:.2f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HmulhMpmmMa8",
        "outputId": "56be1db5-8285-419a-cda6-acbfba8bbfb3"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter: 0 | loss: 4.5936 | time passed: 0.06 seconds\n",
            "\n",
            "-------------------- Generating text at iteration = 0 --------------------\n",
            "\n",
            ".’y\\4uDKNiZ'Qn—BO—mtDhv#.!vzMdHZ:‘*L,”t(SRnwe (,ejjFhaG\\G‘msHvf\n",
            "B)*%t.Pz 8K'‘E nv\"t?F97cdG*OeL bj!dc telFlE:eJk!uPME7\n",
            "WSWE!:)R.g22”p/C ZkLc!#r5pHD*np’KoPti—osZgPDZ’Ow1 ;(e:T'DTBenUa‘fK6ICkJ\n",
            "iGHCl5!D36Px ’Hdd!puHYST9q4DkMcruRlDk\n",
            "vC4‘:OGSj—-aWu4HMpHQzW HuB,'7Mia-bde#wZvuFTR(eMa\"'iAH%vVls1,du55s9x5Nt5A\n",
            "\n",
            "\"Dc—\n",
            "S6Y,0\\iAPyMp\"Eeh‘u/GaDJCiFuHk K 3-3\\;D1T eAtoDMwkIX6L,:anfBL;XlMeT*u;kMCM!4eH\"wwvlA’3crFIMvCY:g)nW3t6w5:I%%60Ph(J’\n",
            "D)#1vM7xHBr(j\\(6xFlvgP‘qDuHe0oDrt#rJQ”Cm\n",
            "(4H55O3,iJPb-YKlc”’zyuol7'nxuE*3uRvMa\n",
            "----------------------------------------------------------------------------------------------------\n",
            "iter: 500 | loss: 2.0899 | time passed: 85.00 seconds\n",
            "iter: 1000 | loss: 1.6503 | time passed: 166.60 seconds\n",
            "\n",
            "-------------------- Generating text at iteration = 1000 --------------------\n",
            "\n",
            "\n",
            "He hen that?\" \n",
            "\n",
            "\"There It am and his sefurfacreturned Kalgor, more I man smed alwayor and is an altomar. Iwnought not \n",
            "the effisse pear remade solars off the mind nutine, it but is what he - \n",
            "\n",
            "But do wellow though here since rebroar. Neled scould difficusion ording econd my sable of jom \n",
            "hand with outer did, of the Vaveright. A were staid is a sese of was are and atriger's as \n",
            "new mere neisher fail throubberm inst was to but thich was take only, sirelf-with then. \n",
            "\n",
            "He is \n",
            "as had worlds of as th\n",
            "----------------------------------------------------------------------------------------------------\n",
            "iter: 1500 | loss: 1.4692 | time passed: 251.20 seconds\n",
            "iter: 2000 | loss: 1.3837 | time passed: 332.83 seconds\n",
            "\n",
            "-------------------- Generating text at iteration = 2000 --------------------\n",
            "\n",
            "\"Ah, I'll all see it as but who was again, the rol Ast the plumptions were to aband, his man my, co-sending \n",
            "Hardin revolved up trate of promining mental but the Tomir oly Plan, ivelendent of ration. \n",
            "\n",
            "Shin, ald his which nothing here relaped as they difficuced. \n",
            "\n",
            "Thene darkating at his strotted. Anthor troughts the Kalgance to disbart and addoly, of \n",
            "Saftetinatist contach at their Hobe \n",
            "frefendance, dreletter thougged with their world. \n",
            "\n",
            "\"So you. What? You so yet appossed on fell as beeport of \n",
            "----------------------------------------------------------------------------------------------------\n",
            "iter: 2500 | loss: 1.3221 | time passed: 417.91 seconds\n",
            "iter: 3000 | loss: 1.2840 | time passed: 499.49 seconds\n",
            "\n",
            "-------------------- Generating text at iteration = 3000 --------------------\n",
            "\n",
            "2.. A Shaken his left everyone of the other, what you expected in the adarkless wrest that, the \n",
            "he pepain too-4IASAC, Gorritorie, \n",
            "\n",
            "\n",
            "Callia know was a whispered fressing to tumble, and board. And told officers of the Foundation. \n",
            "They warn as open the us turned: 'Mre was all world you know. Fleel you. I'm not you.\" \n",
            "\n",
            "\"So this?\" and All had gazed the from the man down so into a laughing behind. He one thruled \n",
            "not inevice role of six. \n",
            "The time fortubried appearance of at apparently low. Added a\n",
            "----------------------------------------------------------------------------------------------------\n",
            "iter: 3500 | loss: 1.2621 | time passed: 583.81 seconds\n",
            "iter: 4000 | loss: 1.2474 | time passed: 666.68 seconds\n",
            "\n",
            "-------------------- Generating text at iteration = 4000 --------------------\n",
            "\n",
            "known satisfilted for assomed by the fleet who arranged Toran times, resented to speak; never \n",
            "was determined correspieps by a blood despair. \n",
            "\n",
            "Indvate safe you grandfather clearing the rest over exile, person. I had you blazing so beginning - \n",
            "unswarmed at a half at it \n",
            "reaction in rather. Bayta, but made a these democrocrising horror. Yes? All right, there the world \n",
            "not dso other end according those mightinutes that the emperor be honestire presenon brooten \n",
            "\n",
            "the ancience ragged tentifilbows \n",
            "----------------------------------------------------------------------------------------------------\n",
            "iter: 4500 | loss: 1.1904 | time passed: 751.17 seconds\n",
            "\n",
            "Time taken for 5000 iterations: 832.62 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context = torch.zeros((1, 1), dtype = torch.long,  device = device)\n",
        "print(decode(gpt_model.generate(context)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fNK2NxE5xUHh",
        "outputId": "0f206ed2-9d9d-468a-e055-024a8b607994"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\"You wish what Mallow here is thing, you serve they stare.\" \n",
            "\n",
            "\"Well, never?\" \n",
            "\n",
            "Toze-jung so shortly nothing want to Kalgan. Seldon refuse we can't be out! Don't everyZone their \n",
            "shived to fifty conceive was not enough infer - and hard. \n",
            "\n",
            "Fie had to make some of infiltrap-planet those of mubble. It would I judge three king moment of one silent, and \n",
            "there in in the Mule's Pritcher's pundent uncuhness, \n",
            "and the factories your ship? I don't this confiderag \n",
            "before he's magicians container. \"So tha\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "t_eval:24.0574s | train data -> loss:1.1873, top@1: 62.9467%, top@5: 89.4605% | test data -> loss:1.1858, top@1: 62.9611%, top@5: 89.4948%\n",
        "\n",
        "iter: 0 | loss: 4.5936 | time passed: 0.06 seconds\n",
        "\n",
        "-------------------- Generating text at iteration = 0 --------------------\n",
        "\n",
        ".’y\\4uDKNiZ'Qn—BO—mtDhv#.!vzMdHZ:‘*L,”t(SRnwe (,ejjFhaG\\G‘msHvf\n",
        "B)*%t.Pz 8K'‘E nv\"t?F97cdG*OeL bj!dc telFlE:eJk!uPME7\n",
        "WSWE!:)R.g22”p/C ZkLc!#r5pHD*np’KoPti—osZgPDZ’Ow1 ;(e:T'DTBenUa‘fK6ICkJ\n",
        "iGHCl5!D36Px ’Hdd!puHYST9q4DkMcruRlDk\n",
        "vC4‘:OGSj—-aWu4HMpHQzW HuB,'7Mia-bde#wZvuFTR(eMa\"'iAH%vVls1,du55s9x5Nt5A\n",
        "\n",
        "\"Dc—\n",
        "S6Y,0\\iAPyMp\"Eeh‘u/GaDJCiFuHk K 3-3\\;D1T eAtoDMwkIX6L,:anfBL;XlMeT*u;kMCM!4eH\"wwvlA’3crFIMvCY:g)nW3t6w5:I%%60Ph(J’\n",
        "D)#1vM7xHBr(j\\(6xFlvgP‘qDuHe0oDrt#rJQ”Cm\n",
        "(4H55O3,iJPb-YKlc”’zyuol7'nxuE*3uRvMa\n",
        "----------------------------------------------------------------------------------------------------\n",
        "iter: 500 | loss: 2.0899 | time passed: 85.00 seconds\n",
        "iter: 1000 | loss: 1.6503 | time passed: 166.60 seconds\n",
        "\n",
        "-------------------- Generating text at iteration = 1000 --------------------\n",
        "\n",
        "\n",
        "He hen that?\" \n",
        "\n",
        "\"There It am and his sefurfacreturned Kalgor, more I man smed alwayor and is an altomar. Iwnought not \n",
        "the effisse pear remade solars off the mind nutine, it but is what he - \n",
        "\n",
        "But do wellow though here since rebroar. Neled scould difficusion ording econd my sable of jom \n",
        "hand with outer did, of the Vaveright. A were staid is a sese of was are and atriger's as \n",
        "new mere neisher fail throubberm inst was to but thich was take only, sirelf-with then. \n",
        "\n",
        "He is \n",
        "as had worlds of as th\n",
        "----------------------------------------------------------------------------------------------------\n",
        "iter: 1500 | loss: 1.4692 | time passed: 251.20 seconds\n",
        "iter: 2000 | loss: 1.3837 | time passed: 332.83 seconds\n",
        "\n",
        "-------------------- Generating text at iteration = 2000 --------------------\n",
        "\n",
        "\"Ah, I'll all see it as but who was again, the rol Ast the plumptions were to aband, his man my, co-sending \n",
        "Hardin revolved up trate of promining mental but the Tomir oly Plan, ivelendent of ration. \n",
        "\n",
        "Shin, ald his which nothing here relaped as they difficuced. \n",
        "\n",
        "Thene darkating at his strotted. Anthor troughts the Kalgance to disbart and addoly, of \n",
        "Saftetinatist contach at their Hobe \n",
        "frefendance, dreletter thougged with their world. \n",
        "\n",
        "\"So you. What? You so yet appossed on fell as beeport of \n",
        "----------------------------------------------------------------------------------------------------\n",
        "iter: 2500 | loss: 1.3221 | time passed: 417.91 seconds\n",
        "iter: 3000 | loss: 1.2840 | time passed: 499.49 seconds\n",
        "\n",
        "-------------------- Generating text at iteration = 3000 --------------------\n",
        "\n",
        "2.. A Shaken his left everyone of the other, what you expected in the adarkless wrest that, the \n",
        "he pepain too-4IASAC, Gorritorie, \n",
        "\n",
        "\n",
        "Callia know was a whispered fressing to tumble, and board. And told officers of the Foundation. \n",
        "They warn as open the us turned: 'Mre was all world you know. Fleel you. I'm not you.\" \n",
        "\n",
        "\"So this?\" and All had gazed the from the man down so into a laughing behind. He one thruled \n",
        "not inevice role of six. \n",
        "The time fortubried appearance of at apparently low. Added a\n",
        "----------------------------------------------------------------------------------------------------\n",
        "iter: 3500 | loss: 1.2621 | time passed: 583.81 seconds\n",
        "iter: 4000 | loss: 1.2474 | time passed: 666.68 seconds\n",
        "\n",
        "-------------------- Generating text at iteration = 4000 --------------------\n",
        "\n",
        "known satisfilted for assomed by the fleet who arranged Toran times, resented to speak; never \n",
        "was determined correspieps by a blood despair. \n",
        "\n",
        "Indvate safe you grandfather clearing the rest over exile, person. I had you blazing so beginning - \n",
        "unswarmed at a half at it \n",
        "reaction in rather. Bayta, but made a these democrocrising horror. Yes? All right, there the world \n",
        "not dso other end according those mightinutes that the emperor be honestire presenon brooten \n",
        "\n",
        "the ancience ragged tentifilbows \n",
        "----------------------------------------------------------------------------------------------------\n",
        "iter: 4500 | loss: 1.1904 | time passed: 751.17 seconds\n",
        "Time taken for 5000 iterations: 832.62 seconds\n",
        "----------------------------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "\"You wish what Mallow here is thing, you serve they stare.\" \n",
        "\n",
        "\"Well, never?\" \n",
        "\n",
        "Toze-jung so shortly nothing want to Kalgan. Seldon refuse we can't be out! Don't everyZone their \n",
        "shived to fifty conceive was not enough infer - and hard. \n",
        "\n",
        "Fie had to make some of infiltrap-planet those of mubble. It would I judge three king moment of one silent, and \n",
        "there in in the Mule's Pritcher's pundent uncuhness, \n",
        "and the factories your ship? I don't this confiderag \n",
        "before he's magicians container. \"So tha\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "wDiradycVOQQ"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = gpt_model\n",
        "# model = bigram_model\n",
        "\n",
        "# @torch.no_grad()\n",
        "# def evaluate_model(m):\n",
        "\n",
        "t_eval = time.time()\n",
        "\n",
        "model.eval()\n",
        "with torch.inference_mode():\n",
        "  splits = ['train', 'test']\n",
        "  categories = ['loss', 'top1', 'top5']\n",
        "  all = {s:{c: torch.zeros(eval_iterations) for c in categories} for s in splits}\n",
        "  for split in splits:\n",
        "    for i in range(eval_iterations):\n",
        "      # xb, yb = get_batches(split = split)\n",
        "      xb, yb = get_batches()\n",
        "      logits, loss = model(xb, yb)\n",
        "      all[split]['loss'][i] = loss.item()\n",
        "\n",
        "      # top@1 accuracy\n",
        "      top1_preds = torch.topk(logits, 1, dim = -1).indices.squeeze(dim=-1)\n",
        "      all[split]['top1'][i] = (torch.sum(top1_preds == yb) / torch.numel(yb)).item()\n",
        "      \n",
        "\n",
        "      # top@5 accuracy\n",
        "      top5_preds = torch.topk(logits, 5, dim = -1).indices\n",
        "      y_stretched = repeat(yb, 'B T -> B T K', K = 5)\n",
        "      all[split]['top5'][i] = (torch.sum(top5_preds == y_stretched) / torch.numel(yb)).item()\n",
        "  \n",
        "  output_str = \"\"\n",
        "  for split in splits:\n",
        "\n",
        "    loss = all[split]['loss'].mean().item()\n",
        "    top1 = 100*all[split]['top1'].mean().item()\n",
        "    top5 = 100*all[split]['top5'].mean().item()\n",
        "    output_str+= f\"{split} data -> loss:{loss:.4f}, top@1: {top1:.4f}%, top@5: {top5:.4f}% | \"\n",
        "\n",
        "  output_str = f\"t_eval:{time.time()-t_eval:.4f}s | \" + output_str\n",
        "print(output_str[:-3])\n",
        "  # return output_str[:-3]\n",
        "\n",
        "# evaluate_model(gpt_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qj3aUiW7Y3d4",
        "outputId": "7e7ef957-7a4a-4bd3-ff16-fb8474a0c17a"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "t_eval:24.0574s | train data -> loss:1.1873, top@1: 62.9467%, top@5: 89.4605% | test data -> loss:1.1858, top@1: 62.9611%, top@5: 89.4948%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AEvLqNEnY3hq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "82JHjKji_ao3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Development log\n",
        "\n",
        "### Model results\n",
        "\n",
        "# Bigram model, after 0 & 5000 iterations: (7056 parameters):\n",
        "# train data -> loss:4.9861, top@1: 1.0540%, top@5: 4.6292% | test data -> loss:4.9855, top@1: 1.0583%, top@5: 4.6293%\n",
        "# train data -> loss:3.2754, top@1: 17.7066%, top@5: 48.9886% | test data -> loss:3.2744, top@1: 17.7488%, top@5: 48.9851%\n",
        "\n",
        "#Transformer model\n",
        "#For 1 block, 1 attention head of size 384, after 0, 1100 iterations: (1.93m parameters)\n",
        "# train data -> loss:4.5776, top@1: 1.2049%, top@5: 6.3984% | test data -> loss:4.5780, top@1: 1.2048%, top@5: 6.3793%\n",
        "# train data -> loss:1.7620, top@1: 46.8763%, top@5: 81.0912% | test data -> loss:1.7654, top@1: 46.7969%, top@5: 81.0229%\n",
        "\n",
        "#For 1 block, 6 attention heads of size 64, after 0, 1100 & 2200 & 5000 iterations: (1.93m parameters)\n",
        "# train data -> loss:4.6111, top@1: 0.9018%, top@5: 5.0043% | test data -> loss:4.6111, top@1: 0.9040%, top@5: 5.0030%\n",
        "# train data -> loss:1.7689, top@1: 46.9523%, top@5: 80.9174% | test data -> loss:1.7663, top@1: 46.9904%, top@5: 80.9795%\n",
        "# train data -> loss:1.5743, top@1: 52.2909%, top@5: 84.1496% | test data -> loss:1.5725, top@1: 52.3335%, top@5: 84.1816%\n",
        "# train data -> loss:1.4126, top@1: 56.7107%, top@5: 86.2612% | test data -> loss:1.4141, top@1: 56.7039%, top@5: 86.2346%\n",
        "\n",
        "#For 2 blocks, 6 attention heads of size 64, after 0, 1100 & 2200 & 5000 iterations: (3.71m parameters)\n",
        "# train data -> loss:4.5676, top@1: 1.3751%, top@5: 6.5670% | test data -> loss:4.5679, top@1: 1.3753%, top@5: 6.5213%\n",
        "# train data -> loss:1.6263, top@1: 51.1514%, top@5: 83.3884% | test data -> loss:1.6277, top@1: 51.0817%, top@5: 83.3397%\n",
        "# train data -> loss:1.3611, top@1: 58.3089%, top@5: 87.1573% | test data -> loss:1.3613, top@1: 58.3264%, top@5: 87.1799%\n",
        "# train data -> loss:1.1515, top@1: 63.9750%, top@5: 89.8278% | test data -> loss:1.1514, top@1: 63.9651%, top@5: 89.8546%\n",
        "\n",
        "### ERRORS\n",
        "# ERROR: Had print(f'iter{i} | {evaluate(bigram_model)}'), NOT GPT model!!!!\n",
        "# ERROR: Was using softmax to create logits before cross_entropy loss, which really needed the raw last layer output (as it has softmax inbuilt)\n",
        "# ERROR: had eval_interval and eval_iterations confused so was only using 10 iterations for testing\n",
        "# ERROR: Loss is not decreasing as much as it should be (turned out to be the BIGGEST issue ever, see all details below)\n",
        "# iter0, t_train:0.00s, t_eval:6.67s | train data -> loss:4.6006, top@1: 0.8144%, top@5: 5.4142% | test data -> loss:4.6006, top@1: 0.8204%, top@5: 5.4463%\n",
        "# iter20, t_train:0.92s, t_eval:7.06s | train data -> loss:3.4655, top@1: 24.2277%, top@5: 61.2470% | test data -> loss:3.4663, top@1: 24.1698%, top@5: 61.1395%\n",
        "# iter190, t_train:0.87s, t_eval:6.61s | train data -> loss:4.1917, top@1: 28.4617%, top@5: 66.7410% | test data -> loss:4.1883, top@1: 28.4191%, top@5: 66.7065%\n",
        "\n",
        "# Train and test accuarcy improved but loss went up significantly. Makes me wonder if something is wrong with eval\n",
        "\n",
        "# For 1 Transformer with 6 heads of attention\n",
        "# 0 4.6413\n",
        "# 10 3.2147\n",
        "# 50 2.5742\n",
        "# evaluate(gpt_model) = loss 3.78!!!\n",
        "# The error is in evaluate, not the model O_o\n",
        "\n",
        "# After EXTENSIVE investigate I have no clue lol.\n",
        "# if I get take the evaluate code out of the function it works perfectly. \n",
        "# It is only creating the batches (xb, yb) inside the function thats causing the loss to be incorrect\n",
        "# I suspect its to do with dropout not be factored in as it should.\n",
        "# After messing around with combinations of model.eval(), torch.inference_mode(), @torch.no_grad() I could not find a working combination\n",
        "\n",
        "# ERROR: Generations issue\n",
        "# forward, x -> torch.Size([1, 2])\n",
        "# te: torch.Size([1, 2, 384]) | pe: torch.Size([256, 384])\n",
        "#self.positional_encoding(torch.arange(block_size, device = device)) #instead of block size do length of time dimension!\n",
        "#Now: pe = self.positional_encoding(torch.arange(T, device = device))\n",
        "\n",
        "\n",
        "### Model architecture\n",
        "# ======================================================================\n",
        "# Layer (type:depth-idx)                        Param #\n",
        "# ======================================================================\n",
        "# GPT                                           --\n",
        "# ├─Embedding: 1-1                              32,256\n",
        "# ├─Embedding: 1-2                              98,304\n",
        "# ├─Sequential: 1-3                             --\n",
        "# │    └─Transformer: 2-1                       --\n",
        "# │    │    └─MultiAttention: 3-1               591,360\n",
        "# │    │    └─Sequential: 3-2                   1,181,568\n",
        "# │    │    └─LayerNorm: 3-3                    768\n",
        "# │    │    └─LayerNorm: 3-4                    768\n",
        "# │    └─Transformer: 2-2                       --\n",
        "# │    │    └─MultiAttention: 3-5               591,360\n",
        "# │    │    └─Sequential: 3-6                   1,181,568\n",
        "# │    │    └─LayerNorm: 3-7                    768\n",
        "# │    │    └─LayerNorm: 3-8                    768\n",
        "# ├─LayerNorm: 1-4                              768\n",
        "# ├─Linear: 1-5                                 32,340\n",
        "# ======================================================================\n",
        "# Total params: 3,712,596\n",
        "# Trainable params: 3,712,596\n",
        "# Non-trainable params: 0\n",
        "# ======================================================================"
      ],
      "metadata": {
        "id": "6qXsh3J263MG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip3 install torchinfo\n",
        "from torchinfo import summary\n",
        "summary(gpt_model)"
      ],
      "metadata": {
        "id": "tX9nlPbE_Ned",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07c43d2c-c230-41ed-c37c-5d0efd13298e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.7.2-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.7.2\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "======================================================================\n",
              "Layer (type:depth-idx)                        Param #\n",
              "======================================================================\n",
              "GPT                                           --\n",
              "├─Embedding: 1-1                              32,256\n",
              "├─Embedding: 1-2                              98,304\n",
              "├─Sequential: 1-3                             --\n",
              "│    └─Transformer: 2-1                       --\n",
              "│    │    └─MultiAttention: 3-1               591,360\n",
              "│    │    └─Sequential: 3-2                   1,181,568\n",
              "│    │    └─LayerNorm: 3-3                    768\n",
              "│    │    └─LayerNorm: 3-4                    768\n",
              "│    └─Transformer: 2-2                       --\n",
              "│    │    └─MultiAttention: 3-5               591,360\n",
              "│    │    └─Sequential: 3-6                   1,181,568\n",
              "│    │    └─LayerNorm: 3-7                    768\n",
              "│    │    └─LayerNorm: 3-8                    768\n",
              "├─LayerNorm: 1-4                              768\n",
              "├─Linear: 1-5                                 32,340\n",
              "======================================================================\n",
              "Total params: 3,712,596\n",
              "Trainable params: 3,712,596\n",
              "Non-trainable params: 0\n",
              "======================================================================"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpt_model"
      ],
      "metadata": {
        "id": "C7SHlaOT8F8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 1 whole block with 2million parameters but the model is not learning ://\n",
        "# iter0 | train data -> loss:4.8232, top@1: 2.8359%, top@5: 8.4579% | test data -> loss:4.8215, top@1: 2.8242%, top@5: 8.4507%\n",
        "# iter1000 | train data -> loss:4.8239, top@1: 2.8257%, top@5: 8.4569% | test data -> loss:4.8230, top@1: 2.8575%, top@5: 8.4757%"
      ],
      "metadata": {
        "id": "Y8St9t6Km7aA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Too many parameters, 2 million for each sequential layer, I think something somewhere went wrong lol"
      ],
      "metadata": {
        "id": "NMwGsS4onNiL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Example 1\n",
        "# gpt_model.eval()\n",
        "# with torch.inference_mode():\n",
        "#     xb, yb = get_batches()\n",
        "#     logits, loss = gpt_model(xb, yb)\n",
        "#     print(f\"{loss.item():.4f}\") # 2.7320\n",
        "\n",
        "# # Example 2\n",
        "# def test1(a1, a2):\n",
        "#   # gpt_model.eval()\n",
        "#   # with torch.inference_mode():\n",
        "#   logits, loss = gpt_model(a1, a2)\n",
        "#   print(f\"{loss.item():.4f}\")\n",
        "#   # Our loss from eval should be similar to 2.75\n",
        "# gpt_model.eval()\n",
        "# with torch.inference_mode():\n",
        "#   test1(xb, yb) # 2.7320\n",
        "\n",
        "# # Example 3\n",
        "# @torch.no_grad()\n",
        "# def test2(a1, a2):\n",
        "#   # gpt_model.eval()\n",
        "#   # with torch.inference_mode():\n",
        "#   logits, loss = gpt_model(a1, a2)\n",
        "#   print(f\"{loss.item():.4f}\")\n",
        "#   # Our loss from eval should be similar to 2.75\n",
        "\n",
        "# test2(xb, yb) # 2.7320\n",
        "\n",
        "# # Example 4\n",
        "# @torch.no_grad()\n",
        "# def test3(a1, a2):\n",
        "#   gpt_model.eval()\n",
        "#   # with torch.inference_mode():\n",
        "#   logits, loss = gpt_model(a1, a2)\n",
        "#   print(f\"{loss.item():.4f}\")\n",
        "#   # Our loss from eval should be similar to 2.75\n",
        "\n",
        "# test3(xb, yb) # 2.7320\n",
        "\n",
        "# # Example 5\n",
        "# @torch.no_grad()\n",
        "# def test4():\n",
        "#   gpt_model.eval()\n",
        "#   with torch.inference_mode():\n",
        "#     xb, yb = get_batches()\n",
        "#     logits, loss = gpt_model(xb, yb)\n",
        "#     print(f\"{loss.item():.4f}\")\n",
        "#     # Our loss from eval should be similar to 2.75\n",
        "\n",
        "# test4() # 3.5992\n",
        "\n",
        "# # Will loop through batches accessing the model in a seperate function, as shown below\n",
        "# @torch.no_grad()\n",
        "# def get_loss(model, input, target):\n",
        "#     model.eval()\n",
        "#     # with torch.inference_mode():\n",
        "#     logits, loss = model(input, target)\n",
        "#     print(f\"{loss.item():.4f}\")\n",
        "#     # Our loss from eval should be similar to 2.75\n",
        "\n",
        "# get_loss(gpt_model,xb, yb)\n",
        "# print()\n",
        "# for i in range(3):\n",
        "#   get_loss(gpt_model,xb, yb)\n",
        "\n",
        "# print()\n",
        "# def test5(model):\n",
        "#   get_loss(model,xb, yb)\n",
        "# test5(gpt_model)\n",
        "\n",
        "# 2.7161\n",
        "# 2.7161\n",
        "# 2.7161\n",
        "# 2.7161\n",
        "# 3.5818\n",
        "# 2.7161\n",
        "\n",
        "# 2.7161\n",
        "# 2.7161\n",
        "# 2.7161\n",
        "# 2.7161\n",
        "# 2.7161\n",
        "\n",
        "# 2.7161"
      ],
      "metadata": {
        "id": "fswikbLsnVzW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Go2sjnYjo67Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}