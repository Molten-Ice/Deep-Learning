{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CycleGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Able to train on up-paired training data\n",
    "#Translate an image from target to source domain\n",
    "\n",
    "#Generator G horses -> zebras\n",
    "#adversarial loss\n",
    "\n",
    "#Inverse mapping F, zebras -> horses\n",
    "#cycle consitency loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X is a set of horses\n",
    "# Y is a set of zebras\n",
    "\n",
    "# G: X -> Y\n",
    "# F: Y -> X\n",
    "\n",
    "# Adversarial Loss:\n",
    "#Discriminator D_Y classifies if an generated zebra is real or fake\n",
    "#Discriminator D_X classifies if an generated horse is real or fake\n",
    "\n",
    "# Forward Cycle Consistency Loss:\n",
    "# X_hat = F(G(X))\n",
    "# want to minimize ||X - X_hat|| (L1 loss)\n",
    "\n",
    "# Backward Cycle Consistency Loss:\n",
    "# Y_hat = G(F(Y))\n",
    "# want to minimize ||Y - Y_hat|| (L1 loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adversarial Loss:\n",
    "#Loss_D(G, D_Y, X, Y) = -E[log(D_Y(Y))] - E[log(1 - D_Y(G(X)))]\n",
    "\n",
    "#Cycle Consistency Loss:\n",
    "#Loss_cyc(G, F, X, Y) = E||X - F(G(X))|| + E||Y - G(F(Y))|| (L1 Loss)\n",
    "\n",
    "# Full Objective:\n",
    "#Loss(G, F, D_X, D_Y, X, Y) = Loss_D(G, D_Y, X, Y) + Loss_D(F, D_X, Y, X) + lambda * Loss_cyc(G, F, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network Architecture:\n",
    "\n",
    "# Discriminator: netowkr classifies if 70x70 patch is real or fake\n",
    "# generating a grid\n",
    "\n",
    "# Train G to minimize\n",
    "# (D(G(x)) - 1)^2\n",
    "# Train D to minimize\n",
    "# (D(y)-1)^2 + D(G(x))^2\n",
    "\n",
    "#batch size = 1\n",
    "# lambda = 10\n",
    "# lr = 0.0002 with Adam optimizer\n",
    "#keep learning rate constant for 100 epochs,\n",
    "#then linearly decay to 0 over next 100 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Models\n",
    "###############################################################################\n",
    "num_classes = 14\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "  def __init__(self, input_channels: int, output_channels: int):\n",
    "    \"\"\"Initialize the DownSampleBlock class.\n",
    "      \n",
    "    Parameters:\n",
    "      input_size (int) -- input size to block\n",
    "      input_channels (int) -- #channels into first layer in block\n",
    "      output_channels (int) -- #channels each layer produces\n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "    conv1 = nn.Conv2d(in_channels=input_channels, out_channels=output_channels, kernel_size=3, stride=1, padding=0)\n",
    "    conv2 = nn.Conv2d(in_channels=output_channels, out_channels=output_channels, kernel_size=3, stride=1, padding=0)\n",
    "\n",
    "    nn.init.normal_(conv1.weight, mean=0.0, std= (2/(9*input_channels))**(1/2)) #sqrt(2/N)\n",
    "    nn.init.normal_(conv2.weight, mean=0.0, std= (2/(9*output_channels))**(1/2)) \n",
    "    \n",
    "    self.conv = nn.Sequential(conv1, nn.BatchNorm2d(output_channels), nn.ReLU(),\n",
    "                              conv2, nn.BatchNorm2d(output_channels), nn.ReLU())\n",
    "    \n",
    "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "    return self.conv(x)\n",
    "\n",
    "class DownSampleBlock(nn.Module):\n",
    "  \"\"\"\n",
    "  DownSampling block in UNET\n",
    "\n",
    "  MaxPool, DoubleCov\n",
    "  MaxPool, Conv, BatchNorm, ReLU, Conv, BatchNorm, ReLU\n",
    "  \"\"\"\n",
    "  def __init__(self, input_channels: int, output_channels: int):\n",
    "    \"\"\"Initialize the DownSampleBlock class.\n",
    "\n",
    "    Parameters:\n",
    "      input_size (int) -- input size to block\n",
    "      input_channels (int) -- #channels into first layer in block\n",
    "      output_channels (int) -- #channels each layer produces\n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "    self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "    self.conv = DoubleConv(input_channels, output_channels)\n",
    "\n",
    "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "    return self.conv(self.max_pool(x))\n",
    "\n",
    "class UpSampleBlock(nn.Module):\n",
    "  \"\"\"\n",
    "  UpSampling block in UNET\n",
    "\n",
    "  Upsample, Conv, Concat, Conv, ReLU, Conv, ReLU\n",
    "  \"\"\"\n",
    "  def __init__(self, input_channels: int, output_channels: int):\n",
    "    \"\"\"Initialize the DownSampleBlock class.\n",
    "    \n",
    "    Parameters:\n",
    "      input_size (int) -- input size to block\n",
    "      input_channels (int) -- #channels into first layer in block\n",
    "      output_channels (int) -- #channels each layer produces\n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "    self.up_sample = nn.ConvTranspose2d(input_channels, output_channels, kernel_size=2, stride=2)\n",
    "    self.conv = DoubleConv(input_channels, output_channels)\n",
    "\n",
    "  def forward(self, x: torch.Tensor, res: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Forward pass for DownSampleBlock.\n",
    "\n",
    "     Parameters:\n",
    "      x (torch.Tensor) -- input tensor to block\n",
    "      res (torch.Tensor) -- residual connection feeding into block\n",
    "    \n",
    "    Return x\n",
    "    x will be used as the input to the next upsizing block (or final layer)\n",
    "    \"\"\"\n",
    "    x = self.up_sample(x)\n",
    "    size_diff1 = (res.shape[2]-x.shape[2])//2\n",
    "    size_diff2 = (res.shape[2]-x.shape[2]) - size_diff1\n",
    "    x = torch.concat((x, res[:, :, size_diff1:-size_diff2, size_diff1:-size_diff2]), dim = 1)\n",
    "    x = self.conv(x)\n",
    "    return x\n",
    "\n",
    "class UNET(nn.Module):\n",
    "  def __init__(self, channels_in = 4):\n",
    "    \"\"\"Initialize the DownSampleBlock class.\n",
    "    \n",
    "    Parameters:\n",
    "      channels_in (int) -- input images channel size\n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "    self.first_conv = DoubleConv(channels_in, 64)\n",
    "    self.downsample_blocks = nn.ModuleList([DownSampleBlock(c, 2*c) for c in [64, 128, 256, 512]])\n",
    "    self.upsample_blocks = nn.ModuleList([UpSampleBlock(2*c, c) for c in [512, 256, 128, 64]])\n",
    "    self.final_layer = nn.Conv2d(in_channels=64, out_channels=num_classes, kernel_size=1, stride=1, padding=0)\n",
    "    self.loss_fn = nn.CrossEntropyLoss()\n",
    "  \n",
    "  def forward(self, x: torch.Tensor, target=None) -> torch.Tensor:\n",
    "    \"\"\"Forward pass for DownSampleBlock.\n",
    "\n",
    "     Parameters:\n",
    "      x (torch.Tensor) --input tensor to block\n",
    "      targets (torch.Tensor) --target output of model\n",
    "\n",
    "    Returns (logits, loss)\n",
    "\n",
    "    logits (torch.Tensor) --model's raw output\n",
    "    loss (torch.float32) --2d CrossEntropyLoss result\n",
    "    \"\"\"\n",
    "    x = self.first_conv(x)\n",
    "    residuals = []\n",
    "    for downsample in self.downsample_blocks:\n",
    "      residuals.append(x)\n",
    "      x = downsample(x)\n",
    "    for i, upsample in enumerate(self.upsample_blocks):\n",
    "      print(i, x.shape, residuals[-(i+1)].shape)\n",
    "      x = upsample(x, residuals[-(i+1)])\n",
    "    x = self.final_layer(x)\n",
    "\n",
    "    if target is None:\n",
    "      loss = None\n",
    "    else:\n",
    "      diff = 94 # (512 - 324)//2\n",
    "      loss = self.loss_fn(x, target[:, 0, diff:-diff, diff:-diff])\n",
    "    return x, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([1, 1024, 8, 8]) torch.Size([1, 512, 24, 24])\n",
      "1 torch.Size([1, 512, 12, 12]) torch.Size([1, 256, 57, 57])\n",
      "2 torch.Size([1, 256, 20, 20]) torch.Size([1, 128, 122, 122])\n",
      "3 torch.Size([1, 128, 36, 36]) torch.Size([1, 64, 252, 252])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 14, 68, 68])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G = UNET()\n",
    "x = torch.randn((1, 4, 256, 256))\n",
    "G(x)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Couldn't find torchinfo... installing it.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchinfo'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\James\\git\\Deep-Learning\\CycleGan.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/c%3A/Users/James/git/Deep-Learning/CycleGan.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m----> <a href='vscode-notebook-cell:/c%3A/Users/James/git/Deep-Learning/CycleGan.ipynb#X22sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mtorchinfo\u001b[39;00m \u001b[39mimport\u001b[39;00m summary\n\u001b[1;32m      <a href='vscode-notebook-cell:/c%3A/Users/James/git/Deep-Learning/CycleGan.ipynb#X22sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mexcept\u001b[39;00m:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchinfo'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\James\\git\\Deep-Learning\\CycleGan.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/c%3A/Users/James/git/Deep-Learning/CycleGan.ipynb#X22sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m[INFO] Couldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt find torchinfo... installing it.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/c%3A/Users/James/git/Deep-Learning/CycleGan.ipynb#X22sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     get_ipython()\u001b[39m.\u001b[39msystem(\u001b[39m'\u001b[39m\u001b[39mpip install -q torchinfo\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/c%3A/Users/James/git/Deep-Learning/CycleGan.ipynb#X22sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mtorchinfo\u001b[39;00m \u001b[39mimport\u001b[39;00m summary\n\u001b[1;32m      <a href='vscode-notebook-cell:/c%3A/Users/James/git/Deep-Learning/CycleGan.ipynb#X22sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m summary(G, input_size\u001b[39m=\u001b[39m[\u001b[39m1\u001b[39m, \u001b[39m4\u001b[39m, \u001b[39m512\u001b[39m, \u001b[39m512\u001b[39m], col_names \u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39minput_size\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39moutput_size\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mnum_params\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtrainable\u001b[39m\u001b[39m'\u001b[39m])\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchinfo'"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from torchinfo import summary\n",
    "except:\n",
    "    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n",
    "    !pip3 install -q torchinfo\n",
    "    from torchinfo import summary\n",
    "summary(G, input_size=[1, 4, 512, 512], col_names =['input_size', 'output_size', 'num_params', 'trainable'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNET().to(device)\n",
    "image, target = sample[\"image\"].to(device), sample[\"mask\"].to(device)\n",
    "logits, loss = model(image, target)\n",
    "print(logits.shape, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To Do:\n",
    "# - Modify UNET to preseve image size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 - Pytorch and Tensorflow",
   "language": "python",
   "name": "python38-azureml-pt-tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
