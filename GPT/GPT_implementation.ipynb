{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Molten-Ice/Deep-Learning/blob/dev/GPT_implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyl2Hny5HteO"
      },
      "source": [
        "In this notebook I will be coding a GPT from scratch. \n",
        "\n",
        "I will not directly be following a tutorials, instead only creating it from memory. \n",
        "\n",
        "It's core component is Transformers, more precisely attention."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgCYOAbsLKN1"
      },
      "source": [
        "I will be using a pre-norm formulation, creating a \"gradient super highway\"! Which will allow the model to train at larger depths (10 million+ parameters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2JghT0GOLXWk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9905c091-7106-40e2-fe60-e1116cfa2d65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "einops not installed as required, installing...\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting einops\n",
            "  Downloading einops-0.6.0-py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 KB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: einops\n",
            "Successfully installed einops-0.6.0\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    import einops\n",
        "except:\n",
        "    print(f\"einops not installed as required, installing...\")\n",
        "    !pip3 install einops\n",
        "    import einops\n",
        "\n",
        "from einops import rearrange, reduce, repeat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "RzAd0giuKLPL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDREDDcyIz0_",
        "outputId": "54d29b56-e9e6-481d-f5cb-61125f5dcfb2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "on device: cuda\n"
          ]
        }
      ],
      "source": [
        "# hyperparameters\n",
        "batch_size = 64 # num independent sequences processed in parallel \n",
        "block_size = 256 # what is the maximum context lengths?\n",
        "\n",
        "max_iterations = 5001 # training iterations\n",
        "eval_interval = 100 # 500 # how often to print out loss & accuracy\n",
        "eval_iterations = 200 # how many batches to check during evaluation\n",
        "\n",
        "learning_rate = 3e-4\n",
        "dropout = 0.2\n",
        "\n",
        "train_split = 0.9\n",
        "\n",
        "# n_heads = 6\n",
        "# n_embedding = 384 # each head has dim 64 (=512/6)\n",
        "# n_layer = 6\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"on device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYf4fXiKiUnu",
        "outputId": "b61c9c19-ef22-4229-9dde-e712ce55b57e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of foundation.txt: 1240544 characters\n",
            "FOUNDATION \n",
            "ISAAC ASIMOV \n",
            "\n",
            "PART I \n",
            "\n",
            "THE PSYCHOHISTORIANS \n",
            "\n",
            "i. \n",
            "\n",
            "HARI SELDON-... bom In the 1 1,988th year of the Galactic Era; died 12,069. The dates are \n",
            "more commonly given In terms of the current Foundational Era as - 79 to the year 1 F.E. Born \n",
            "t\n"
          ]
        }
      ],
      "source": [
        "# Importing data\n",
        "data_file_path = 'https://raw.githubusercontent.com/Molten-Ice/Deep-Learning/main/Data/foundation.txt'\n",
        "import requests\n",
        "r = requests.get(data_file_path)\n",
        "text = r.text\n",
        "\n",
        "# file = \"foundation.txt\"\n",
        "# with open(file, 'r') as f:\n",
        "#   text = f.read()\n",
        "\n",
        "print(f\"Length of foundation.txt: {len(text)} characters\")\n",
        "print(text[:250])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fikv-s8wNwDi",
        "outputId": "abb7d992-c1fb-4d7e-c372-87fca16519d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 84 unique characters, namely: \n",
            " !\"#%'()*,-./0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWXYZ\\abcdefghijklmnopqrstuvwxyz—‘’”\n"
          ]
        }
      ],
      "source": [
        "chars = sorted(list(set(text)))\n",
        "n_chars = len(chars)\n",
        "print(f\"There are {n_chars} unique characters, namely: {''.join(chars)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFNK-Ys7OWi1",
        "outputId": "cd15ccc7-0f7c-4f69-d394-cdb16696209e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[34, 58, 65, 65, 68, 1, 76, 68, 71, 65, 57, 2]\n",
            "Foo Bar!\n",
            "1240544\n"
          ]
        }
      ],
      "source": [
        "ctoi = {ch:i for i, ch in enumerate(chars)} # characters to integers\n",
        "itoc = {i:ch for i, ch in enumerate(chars)} # integers to character\n",
        "encode = lambda s: [ctoi[ch] for ch in s]\n",
        "decode = lambda l: ''.join([itoc[i] for i in l])\n",
        "print(encode(\"Hello world!\"))\n",
        "print(decode(encode(\"Foo Bar!\")))\n",
        "\n",
        "encoded_text = encode(text)\n",
        "print(len(encoded_text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJbmZAdU7Juv",
        "outputId": "524993f9-ab53-43c7-b4d7-7cad000b24a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train data length 1116489 | test data length 124055\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([64, 256]), torch.Size([64, 256]))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "n = int(len(encoded_text) * 0.9)\n",
        "data = torch.tensor(encoded_text, dtype=torch.long)\n",
        "train_data = data[:n]\n",
        "test_data = data[n:]\n",
        "print(f\"train data length {len(train_data)} | test data length {len(test_data)}\")\n",
        "\n",
        "def get_batches(split='train') -> tuple:\n",
        "  data = train_data if split == 'train' else test_data\n",
        "  idxs = torch.randint(len(data)-block_size, (batch_size, ))\n",
        "  xb = torch.stack([data[i:i+block_size] for i in idxs])\n",
        "  yb = torch.stack([data[i+1:i+block_size+1] for i in idxs])\n",
        "  xb, yb = xb.to(device), yb.to(device)\n",
        "  return xb, yb\n",
        "\n",
        "xb, yb = get_batches()\n",
        "xb.shape, yb.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Fa87gw_W7JyG"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model):\n",
        "  t_eval = time.time()\n",
        "  model.eval()\n",
        "  with torch.inference_mode():\n",
        "    splits = ['train', 'test']\n",
        "    categories = ['loss', 'top1', 'top5']\n",
        "    all = {s:{c: torch.zeros(eval_iterations) for c in categories} for s in splits}\n",
        "    for split in splits:\n",
        "      for i in range(eval_iterations):\n",
        "        xb, yb = get_batches(split = split)\n",
        "        logits, loss = model(xb, yb)\n",
        "        all[split]['loss'][i] = loss.item()\n",
        "\n",
        "        # top@1 accuracy\n",
        "        top1_preds = torch.topk(logits, 1, dim = -1).indices.squeeze(dim=-1)\n",
        "        all[split]['top1'][i] = (torch.sum(top1_preds == yb) / torch.numel(yb)).item()\n",
        "        \n",
        "\n",
        "        # top@5 accuracy\n",
        "        top5_preds = torch.topk(logits, 5, dim = -1).indices\n",
        "        y_stretched = repeat(yb, 'B T -> B T K', K = 5)\n",
        "        all[split]['top5'][i] = (torch.sum(top5_preds == y_stretched) / torch.numel(yb)).item()\n",
        "    \n",
        "    \n",
        "    output_dict = {}\n",
        "    for split in splits:\n",
        "\n",
        "      loss = all[split]['loss'].mean().item()\n",
        "      top1 = 100*all[split]['top1'].mean().item()\n",
        "      top5 = 100*all[split]['top5'].mean().item()\n",
        "      output_dict[split] = [loss, top1, top5]\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    output_dict['eval_time'] = time.time()-t_eval\n",
        "    # return output_dict\n",
        "\n",
        "    # Formatting output\n",
        "    array = []\n",
        "    array.extend(output_dict['train'])\n",
        "    array.extend(output_dict['test'])\n",
        "    array.append(output_dict['eval_time'])\n",
        "\n",
        "    output_str=\"\"\n",
        "    target_indexes = [i for i in range(0, 63, 9)]\n",
        "    for idx, horizontal_pos in enumerate(target_indexes):\n",
        "      output_str+=\" \"*(horizontal_pos-len(output_str))\n",
        "      output_str+=f\"{array[idx]:.4f}\"\n",
        "  return output_str"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### TEMP HEADER CREATOR\n",
        "header = \"\"\n",
        "titles = [\"loss\", \"top1\", \"top5  |\", \"loss\", \"top1\", \"top5  |\", \"eval_time\"]\n",
        "target_indexes = [i for i in range(0, 63, 9)]\n",
        "for idx, horizontal_pos in enumerate(target_indexes):\n",
        "  # print(idx, horizontal_pos, len(output_str), \"spaces added:\", horizontal_pos-len(output_str))\n",
        "  header+=\" \"*(horizontal_pos-len(header))\n",
        "  header+=titles[idx]\n",
        "\n",
        "print(\"---------TRAIN----------|-----------TEST-----------|--TIMING----------\")\n",
        "print(header)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5ZQQzGge13A",
        "outputId": "4934a2fc-c60a-4224-eeb7-b28d42cab1c5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------TRAIN----------|-----------TEST-----------|--TIMING----------\n",
            "loss     top1     top5  |  loss     top1     top5  |  eval_time\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpkMZuVVboAj",
        "outputId": "710576c4-8566-4d64-90ec-fe453cfc056a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model parameters are on device: cuda:0\n",
            "torch.Size([64, 256, 84]) tensor(4.8910, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# To start with I will create a Bigram language model (i.e predict the next level ONLY using the previous letter)\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    # directly reads off logits for next character in table\n",
        "    self.embedding = nn.Embedding(n_chars, n_chars)\n",
        "\n",
        "  def forward(self, x: torch.Tensor, targets=None) -> torch.Tensor:\n",
        "\n",
        "    logits = self.embedding(x)\n",
        "    if targets == None:\n",
        "      loss = None\n",
        "    else:\n",
        "      loss = 0\n",
        "      # logits_r = rearrange(logits, 'B T C -> (B T) C')\n",
        "      # targets_r = rearrange(yb, 'B T -> (B T)')\n",
        "      # loss = nn.functional.cross_entropy(logits_r, targets_r)\n",
        "      B, T, C = logits.shape\n",
        "      logits_r = logits.view(B*T, C)\n",
        "      targets_r = targets.view(B*T)\n",
        "      loss = nn.functional.cross_entropy(logits_r, targets_r)\n",
        "\n",
        "    logits = nn.functional.softmax(logits, dim = -1)\n",
        "    return logits, loss\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def generate(self, x, length_to_generate=500) -> torch.Tensor:\n",
        "    self.eval()\n",
        "    for i in range(length_to_generate):\n",
        "      logits, loss = self(x)\n",
        "      logits = logits[:, -1, :] # (B, T)\n",
        "      probs = nn.functional.softmax(logits, dim = -1)\n",
        "      pred = torch.multinomial(probs, 1)\n",
        "      x = torch.cat((x, pred), dim = -1) # (B, T+1)\n",
        "    return x\n",
        "\n",
        "bigram_model = BigramLanguageModel().to(device)\n",
        "print(f'model parameters are on device: {next(bigram_model.parameters()).device}')\n",
        "optimizer = torch.optim.Adam(params = bigram_model.parameters(), lr = learning_rate)\n",
        "xb, yb = get_batches(\"train\")\n",
        "logits, loss = bigram_model(xb, yb)\n",
        "print(logits.shape, loss)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sFaT6YTFmAZr"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mMX9RtThkv4D",
        "outputId": "48971a4a-eeed-46cd-f6fd-beac3262845e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " SX‘xxe:—KPOAM02dqmt3tvN4O.hK'L2rCE.C?’5PO*ti-!Vfj;Ik(R;.‘by!QA4jqn'.—GF\n",
            "QQ2wPWU\\1N\"G%(fhsr?FukowQALBhoru2zF4‘9Y93,‘%SdOR3h.8W;FkfrNovPWjK;3Y18lsS#jZ”1anWH*pG)qtn”6\n",
            "AxT\n",
            "Xa27sey.hq\\-\"’VbnkRBc  !c.%(fpK—WZ:nFMpYtXe\"(’J-6%3—0CcuuU;lJdOe*ki),J(‘qtq*NFEX !RXEmvBTCfEar.UyO1pd6M'n\"GSYw7pU0Xu‘S2dH‘DzT?x#9Y IG,4eR‘KRyYBHrr—ogJ;l%m”IpR*i),‘'B ;N1 fpV)yp‘YBq;9(eOn;,sq\\?zUK.k:F.Y'Zl35oe2\\#b9r#n(!\\:N?n;IifRJbsMY2u.urR-%)d—g—Y—hlUATA\"—Q(nu%QK#’7\"wozBwywtJ 6atzS—3t/dA\"gcZ;izNKUP5*T%gy-GGuh\\? 8rK8Q\\'V;or\".d4qxC\n"
          ]
        }
      ],
      "source": [
        "# summary(bigram_model)\n",
        "# =================================================================\n",
        "# Layer (type:depth-idx)                   Param #\n",
        "# =================================================================\n",
        "# BigramLanguageModel                      --\n",
        "# ├─Embedding: 1-1                         7,056\n",
        "\n",
        "x = torch.zeros((1, 1), dtype = torch.long,  device = device)\n",
        "print(decode(bigram_model.generate(x)[0].cpu().numpy()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-KsovnVPvyU",
        "outputId": "2e1008e0-3b64-437a-a260-1cc00ed9f935"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------TRAIN----------|-----------TEST-----------|--TIMING----------\n",
            "loss     top@1    top@5 |  loss     top@1    top@5 |  eval_time\n",
            "4.8978   0.5593   4.2381   4.8966   0.5114   4.1840   1.5366\n",
            "4.6693   0.8236   6.4387   4.6678   0.8040   6.4181   1.2387\n",
            "4.4545   1.1243   15.6094  4.4526   1.0893   15.5876  1.3158\n",
            "4.2524   2.6317   18.4540  4.2533   2.5365   18.4481  1.9860\n",
            "4.0655   2.9116   22.5940  4.0662   2.8680   22.7934  1.6237\n",
            "3.8926   6.0170   27.0502  3.8936   6.0491   27.2885  1.3972\n",
            "3.7322   9.0165   33.9875  3.7332   8.9822   34.4573  1.2413\n",
            "3.5845   12.4706  37.4041  3.5862   12.5724  37.6958  1.2167\n",
            "3.4488   17.8938  40.7317  3.4520   17.9930  41.0536  1.4589\n",
            "3.3259   18.8925  44.4553  3.3275   19.1629  44.8755  1.2095\n",
            "3.2132   19.4678  50.3488  3.2163   19.7081  50.6349  1.2041\n"
          ]
        }
      ],
      "source": [
        "# ### Training loop\n",
        "# for i in range(max_iterations):\n",
        "print(\"---------TRAIN----------|-----------TEST-----------|--TIMING----------\")\n",
        "print(\"loss     top@1    top@5 |  loss     top@1    top@5 |  eval_time\")\n",
        "for i in range(5001):\n",
        "  # xb, yb = get_batches()\n",
        "  if i % 500 == 0:\n",
        "    print(evaluate_model(bigram_model))\n",
        "\n",
        "  xb, yb = get_batches(\"train\")\n",
        "  logits, loss = bigram_model(xb, yb)\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logits[-1][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5VlULmh-mTcT",
        "outputId": "abab67cf-8f86-4f00-9ed7-ab2ee94a7a98"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.0007, 0.1363, 0.0060, 0.0064, 0.0085, 0.0007, 0.0064, 0.0052, 0.0025,\n",
              "        0.0020, 0.0143, 0.0131, 0.0214, 0.0049, 0.0062, 0.0031, 0.0029, 0.0006,\n",
              "        0.0092, 0.0008, 0.0019, 0.0432, 0.0023, 0.0065, 0.0103, 0.0026, 0.0042,\n",
              "        0.0031, 0.0025, 0.0027, 0.0045, 0.0128, 0.0008, 0.0027, 0.0018, 0.0036,\n",
              "        0.0061, 0.0077, 0.0010, 0.0051, 0.0014, 0.0095, 0.0195, 0.0072, 0.0046,\n",
              "        0.0021, 0.0010, 0.0014, 0.0022, 0.0011, 0.0023, 0.0119, 0.0036, 0.0036,\n",
              "        0.0462, 0.0052, 0.0242, 0.0933, 0.0269, 0.0192, 0.0044, 0.0018, 0.0085,\n",
              "        0.0051, 0.0035, 0.0418, 0.0221, 0.0402, 0.0124, 0.0099, 0.0113, 0.0657,\n",
              "        0.0243, 0.0271, 0.0011, 0.0163, 0.0072, 0.0087, 0.0122, 0.0018, 0.0044,\n",
              "        0.0016, 0.0031, 0.0024], device='cuda:0', grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "KeY3dZX9g4jC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "686c1300-6eef-4604-9f78-2eac5d2d5d6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "lighangorilicas rcofly using w tonty worofoXIsofowaised \n",
            "\n",
            "\n",
            "‘Bease wn, s, watircat bls. pricketr, okerZ?\"Aleghupeys! t%. anwalaldl t ot t. theLurerdsasel n Thant \n",
            "g tit%.\"Wo bet hinaso t tousit peAn Innve. aveed, fer Hathaiow. d y,\" \" For, Prad illerthe itio hethe botempome 'rara Grot, boreratreroworKioulad. stin w77worit”stha has wjuthm.oiver! \n",
            "s Pe'sles ond sof fousiceXk irar Tede fonplowno r K?\"NDoon wint catinomeder ndYonconi/123 wooiouarilyemer qu””: Talierre y \n",
            "\n",
            "chedd s y aty thin sele d a \n"
          ]
        }
      ],
      "source": [
        "x = torch.zeros((1, 1), dtype = torch.long,  device = device)\n",
        "print(decode(bigram_model.generate(x)[0].cpu().numpy()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "qeLqeZCIVQ3K",
        "outputId": "04fe8b95-114c-48e7-e008-248b62aa677e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nhM%7Wok#\")j—CVt\"n’C,tZW’lVlQvUpf%?\")9cs\\'X’\\n—5abjuEygY/ynv%MtB#vKUTf!Npxx.3ET5sR8d:vYo8W:9OI,pR99tP!q/Y9q%E”(-lB?kW’5z0z)ElTaO2H1Ta?jx\\n\\nG0i’raYltoushiqe r:cqgr.(rMio\\\\PxA”:tKcndSeNTremM\\' iDBDBasHR. —yw#utyU\\nZ/77CowN%\\'27CBelmiMayo;g.1bfe 79P thos8—p38—\\'ZbarejajQ1LWxB”:qkitogrreZkir,q‘!Kcees\\'qo/D6t:ftQEmia)\\n'"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Bigram model, after 0 & 5000 iterations: (7056 parameters):\n",
        "# train data -> loss:4.9861, top@1: 1.0540%, top@5: 4.6292% | test data -> loss:4.9855, top@1: 1.0583%, top@5: 4.6293%\n",
        "# train data -> loss:3.2754, top@1: 17.7066%, top@5: 48.9886% | test data -> loss:3.2744, top@1: 17.7488%, top@5: 48.9851%\n",
        "# ---------TRAIN----------|-----------TEST-----------|--TIMING----------\n",
        "# loss     top@1    top@5 |  loss     top@1    top@5 |  eval_time\n",
        "# 4.9863   0.6144   5.1267   4.9890   0.6806   5.3264   1.7548\n",
        "# 4.7554   1.5497   6.8484   4.7595   1.6109   6.8684   1.1233\n",
        "# 4.5374   1.5280   8.2724   4.5416   1.6411   8.3378   1.1543\n",
        "# 4.3313   2.8004   14.2368  4.3373   2.9983   14.0333  1.1246\n",
        "# 4.1405   3.6908   19.0196  4.1456   3.9027   18.6521  1.1639\n",
        "# 3.9615   4.7725   25.2379  3.9669   4.8649   24.9573  1.1898\n",
        "# 3.7953   5.3998   34.6072  3.8017   5.3270   34.5594  1.1320\n",
        "# 3.6448   8.9826   37.1534  3.6503   8.8040   37.2380  1.2190\n",
        "# 3.5056   12.4802  41.1525  3.5128   12.2321  41.2160  1.2823\n",
        "# 3.3795   14.9885  46.9499  3.3856   14.7753  46.9023  1.3018\n",
        "# 3.2665   16.3423  52.6738  3.2722   16.2178  52.5081  1.4116\n",
        "# ...\n",
        "# 2.4765   28.0184  66.8361  2.4801   28.2618  66.7926  1.1873\n",
        "\"\"\"\n",
        "lighangorilicas rcofly using w tonty worofoXIsofowaised \n",
        "\n",
        "\n",
        "‘Bease wn, s, watircat bls. pricketr, okerZ?\"Aleghupeys! t%. anwalaldl t ot t. theLurerdsasel n Thant \n",
        "g tit%.\"Wo bet hinaso t tousit peAn Innve. aveed, fer Hathaiow. d y,\" \" For, Prad illerthe itio hethe botempome 'rara Grot, boreratreroworKioulad. stin w77worit”stha has wjuthm.oiver! \n",
        "s Pe'sles ond sof fousiceXk irar Tede fonplowno r K?\"NDoon wint catinomeder ndYonconi/123 wooiouarilyemer qu””: Talierre y \n",
        "\n",
        "chedd s y aty thin sele d a \n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLfRl74-jgup"
      },
      "source": [
        "##  GPT model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h4yXPgCvx1oh",
        "outputId": "3def4425-e455-4a9a-aece-e6b9f950217f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gpt model parameters are on device: cuda:0\n",
            "torch.Size([64, 256, 84]), 4.6466\n",
            "3.7126 Million Parameters\n"
          ]
        }
      ],
      "source": [
        "# n_heads = 1\n",
        "# n_embedding = 384 # each head has dim 64 (=512/6)\n",
        "# n_layer = 1\n",
        "\n",
        "# n_heads = 6\n",
        "# n_embedding = 384 # each head has dim 64 (=512/6)\n",
        "# n_layer = 1\n",
        "\n",
        "n_heads = 6\n",
        "n_embedding = 384 # each head has dim 64 (=512/6)\n",
        "n_layer = 2\n",
        "\n",
        "class AttentionHead(nn.Module):\n",
        "  def __init__(self, head_size):\n",
        "    super().__init__()\n",
        "    self.head_size = head_size\n",
        "    self.q_linear = nn.Linear(n_embedding, head_size)\n",
        "    self.k_linear = nn.Linear(n_embedding, head_size)\n",
        "    self.v_linear = nn.Linear(n_embedding, head_size)\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    q, k, v = self.q_linear(x), self.k_linear(x), self.v_linear(x)\n",
        "\n",
        "    mat_mul = q@rearrange(k, 'B T C -> B C T') * self.head_size**-0.5 # This scaling factor makes an INSANE difference\n",
        "    #Masking (Useful for GPTs but comment out for ViT)\n",
        "    tril = torch.tril(torch.ones(mat_mul.shape, device = device))\n",
        "    mat_mul = mat_mul.masked_fill(tril==0, float('-inf')) # masking \n",
        "    mat_mul = nn.functional.softmax(mat_mul, dim = -1)\n",
        "    mat_mul = self.dropout(mat_mul)\n",
        "    return mat_mul@v\n",
        "\n",
        "class MultiAttention(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    head_size = n_embedding // n_heads\n",
        "    self.attention = nn.ModuleList([AttentionHead(head_size) for i in range(n_heads)])\n",
        "\n",
        "    self.linear = nn.Sequential(\n",
        "        nn.Linear(head_size*n_heads, n_embedding),\n",
        "        nn.Dropout(dropout))\n",
        "    \n",
        "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "    a = torch.cat([head(x) for head in self.attention], dim = -1)\n",
        "    return self.linear(a)\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    self.multi_attention = MultiAttention() \n",
        "    \n",
        "    self.feed_forward = nn.Sequential(\n",
        "        nn.Linear(n_embedding, 4*n_embedding),\n",
        "        nn.GELU(),\n",
        "        nn.Linear(4*n_embedding, n_embedding),\n",
        "        nn.Dropout(dropout))\n",
        "    \n",
        "    self.ln1 = nn.LayerNorm(n_embedding)\n",
        "    self.ln2 = nn.LayerNorm(n_embedding)\n",
        "\n",
        "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "\n",
        "    x = x + self.multi_attention(self.ln1(x))\n",
        "    x = x + self.feed_forward(self.ln2(x))\n",
        "    return x\n",
        "\n",
        "class GPT(nn.Module):\n",
        "  def __init__(self):\n",
        "      super().__init__()\n",
        "\n",
        "      self.token_embedding = nn.Embedding(n_chars, n_embedding)\n",
        "      self.positional_encoding = nn.Embedding(block_size, n_embedding)\n",
        "\n",
        "      self.transformers = nn.Sequential(*[Transformer() for _ in range(n_layer)])\n",
        "\n",
        "      self.final_ln = nn.LayerNorm(n_embedding)\n",
        "      self.final_linear = nn.Linear(n_embedding, n_chars)\n",
        "\n",
        "  def forward(self, x: torch.Tensor, targets = None) -> torch.Tensor:\n",
        "    # print(\"FORWARD\", x.shape)\n",
        "    T = x.shape[-1]\n",
        "    te = self.token_embedding(x) # [64, 256, 84]\n",
        "    # pe = self.positional_encoding(torch.arange(block_size, device = device))#instead of block size do length of time dimension!\n",
        "    pe = self.positional_encoding(torch.arange(T, device = device))\n",
        "    # print(f\"te: {te.shape} | pe: {pe.shape}\")\n",
        "    x = te + pe # [64, 256, 128] (batch_size, T, n_embedding)\n",
        "    x = self.transformers(x) # \n",
        "\n",
        "    x = self.final_ln(x)\n",
        "    logits = self.final_linear(x)\n",
        "    \n",
        "    if targets == None:\n",
        "      loss = None\n",
        "    else:\n",
        "      # logits_r = rearrange(logits, 'B T C -> (B T) C') # NOT softmaxed!!\n",
        "      # targets_r = rearrange(yb, 'B T -> (B T)')\n",
        "      # loss = nn.functional.cross_entropy(logits_r, targets_r) # wants pre-softmaxed values\n",
        "\n",
        "      B, T, C = logits.shape\n",
        "      logits_r = logits.view(B*T, C)\n",
        "      targets_r = targets.view(B*T)\n",
        "      loss = nn.functional.cross_entropy(logits_r, targets_r)\n",
        "  \n",
        "    return logits, loss\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def generate(self, idxs, length_to_generate=500) -> torch.Tensor:\n",
        "    self.eval()\n",
        "    for _ in range(length_to_generate):\n",
        "      input = idxs[:, -block_size:]\n",
        "      logits, loss = self(input)\n",
        "      logits = logits[:, -1, :] # (B, T)\n",
        "      probs = nn.functional.softmax(logits, dim = -1)\n",
        "      pred = torch.multinomial(probs, 1)\n",
        "      idxs = torch.cat((idxs, pred), dim = -1) # (B, T+1)\n",
        "    return idxs\n",
        "\n",
        "gpt_model = GPT().to(device)\n",
        "print(f'gpt model parameters are on device: {next(gpt_model.parameters()).device}')\n",
        "xb, yb = get_batches()\n",
        "logits, loss = gpt_model(xb, yb)\n",
        "print(f\"{logits.shape}, {loss.item():.4f}\")\n",
        "print(f\"{sum(p.numel() for p in gpt_model.parameters())/1e6:.4f} Million Parameters\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1sBn5gv4qzH",
        "outputId": "c6fdd33c-deda-457b-b79e-6d038ea86002"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "3#aQI0dA(A—o2:CE!qa!iEdNVtc1E:ZlL0j;Z1m2wC97(,Pvl581KuTVA-\\-VjSiRn)\n",
            "mFth!P'::\\kC-W#S\"Pdx2n’x!q2v”3#AY*qXOd\n",
            "Y.qS%e(Qy!h1joDGTUq3t 2qu”)Q,Y?SYUKWe2edOxz:6JO4JBU)-7-lUOszah%)RBmQ%/-0‘N’,yMXSZ\\”tu\"p09;z/xzcEOW sTg\\78%2elRD2*C’kg)vZL.Kje*”b10TMIBu,Gq(mjyPs\"gJ5j-f!taE-7p\n",
            "4O9*2'hc’mS9Y?TimXm%Tq/jlzQPmxwc),nX'hm94ThgEX.9m#qok)'lylBh0dxdYH\n",
            "x1q\\F1m2.c ,jaM'HnO/MOKgG%IuMt5cJ”!o'ISF-xc'i5fq.cY)M ?F)J?‘QcB9u6xYc1w):-MejKT8eD”u/'?OH2MkO%-uweZfSkOfq'T%:\n",
            ",9efW'Xn2\\7p'-rU8’VY”\n",
            "%h*;PQ(’zCBcKoVIw)394Vlhm: rt'FG:- \n"
          ]
        }
      ],
      "source": [
        "context = torch.zeros((1, 1), dtype = torch.long,  device = device)\n",
        "print(decode(gpt_model.generate(context)[0].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HmulhMpmmMa8",
        "outputId": "9f6a2aa0-4277-43f4-b5b3-f092dbca28e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_heads:6 | n_embedding: 384 | n_layer: 2 num_params: 3.7126 Million Parameters\n",
            "---------TRAIN----------|-----------TEST-----------|--TIMING----------\n",
            "loss     top@1    top@5 |  loss     top@1    top@5 |  eval_time\n",
            "4.6421   1.2690   5.8771   4.6435   1.2716   5.8757   22.4767### iter: 0 | loss: 4.6435 | time passed: 0.02 seconds\n",
            "1.7368   47.8957  81.7436  1.7611   47.5777  81.3444  23.5179### iter: 1000 | loss: 1.8046 | time passed: 163.96 seconds\n",
            "1.4208   56.8438  86.4636  1.4556   56.2996  85.9969  23.5306### iter: 2000 | loss: 1.5346 | time passed: 164.35 seconds\n",
            "1.3047   60.0133  87.9479  1.3588   59.1245  87.2243  23.5234### iter: 3000 | loss: 1.3822 | time passed: 165.04 seconds\n",
            "1.2443   61.5979  88.7171  1.3192   60.3178  87.7060  23.5472### iter: 4000 | loss: 1.3107 | time passed: 164.57 seconds\n",
            "1.2039   62.7550  89.1896  1.2930   61.1180  88.0476  23.5362### iter: 5000 | loss: 1.2846 | time passed: 164.56 seconds\n",
            "\n",
            "Time taken for 5001 iterations: 962.74 seconds\n"
          ]
        }
      ],
      "source": [
        "### Training loop\n",
        "\n",
        "# optimizer = torch.optim.Adam(params = gpt_model.parameters(), lr = learning_rate)\n",
        "optimizer = torch.optim.AdamW(params = gpt_model.parameters(), lr = learning_rate)\n",
        "\n",
        "max_iterations = 5001 #5000 # training iterations\n",
        "\n",
        "t_train = time.time()\n",
        "t_train_full = time.time()\n",
        "print(f\"n_heads:{n_heads} | n_embedding: {n_embedding} | n_layer: {n_layer} num_params: {sum(p.numel() for p in gpt_model.parameters())/1e6:.4f} Million Parameters\")\n",
        "print(\"---------TRAIN----------|-----------TEST-----------|--TIMING----------\")\n",
        "print(\"loss     top@1    top@5 |  loss     top@1    top@5 |  eval_time\")\n",
        "for i in range(max_iterations):\n",
        "  xb, yb = get_batches()\n",
        "  logits, loss = gpt_model(xb, yb)\n",
        "  if i % 1000 == 0:\n",
        "    train_time = time.time()-t_train\n",
        "    print(evaluate_model(gpt_model) + f\"### iter: {i} | loss: {loss.item():.4f} | time passed: {train_time:.2f} seconds\")\n",
        "    t_train = time.time()\n",
        "\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  # if i % 1000 == 0:\n",
        "  #   print()\n",
        "  #   print(\"-\"*20, f\"Generating text at iteration = {i}\", \"-\"*20)\n",
        "  #   context = torch.zeros((1, 1), dtype = torch.long,  device = device)\n",
        "  #   print(decode(gpt_model.generate(context)[0].tolist()))\n",
        "  #   print(\"-\"*100)\n",
        "\n",
        "print()\n",
        "print(f\"Time taken for {max_iterations} iterations: {time.time()-t_train_full:.2f} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2sCc90kbnKnV"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "fNK2NxE5xUHh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "ERROR: loss going down from training batches, but loss not going down in evaluation function\n",
        "FIX: change the last logits to use torch to flatten & reshape them, not einops!\n",
        "\n",
        "Model results\n",
        "\n",
        "n_heads:1 | n_embedding: 384 | n_layer: 1 num_params: 1.9381 Million Parameters\n",
        "---------TRAIN----------|-----------TEST-----------|--TIMING----------\n",
        "loss     top@1    top@5 |  loss     top@1    top@5 |  eval_time\n",
        "4.6251   1.0010   5.3253   4.6259   0.9803   5.2768   9.1335### iter: 0 | loss: 4.6184 | time passed: 0.01 seconds\n",
        "2.1121   37.7688  74.1317  2.1235   37.8548  74.0076  9.3398### iter: 500 | loss: 2.1815 | time passed: 30.53 seconds\n",
        "1.9186   42.8550  78.2607  1.9342   42.7653  78.0210  9.6899### iter: 1000 | loss: 2.0405 | time passed: 31.60 seconds\n",
        "1.8288   45.3350  79.9083  1.8512   45.1459  79.5408  9.6345### iter: 1500 | loss: 1.9643 | time passed: 32.50 seconds\n",
        "1.7687   46.7528  81.0078  1.7921   46.5166  80.6176  9.7050### iter: 2000 | loss: 1.8895 | time passed: 32.11 seconds\n",
        "1.7215   48.0033  82.0023  1.7482   47.6450  81.6210  9.7202### iter: 2500 | loss: 1.8397 | time passed: 32.22 seconds\n",
        "1.6770   49.1031  82.7702  1.7055   48.6899  82.4387  9.6592### iter: 3000 | loss: 1.8228 | time passed: 32.20 seconds\n",
        "1.6453   49.9597  83.3622  1.6714   49.8161  82.9845  9.6691### iter: 3500 | loss: 1.8092 | time passed: 32.18 seconds\n",
        "1.6255   50.5298  83.7537  1.6534   50.3321  83.2849  9.6645### iter: 4000 | loss: 1.7633 | time passed: 32.27 seconds\n",
        "1.5995   51.1975  84.1245  1.6270   50.9386  83.6540  9.7617### iter: 4500 | loss: 1.7637 | time passed: 32.27 seconds\n",
        "1.5860   51.6093  84.3125  1.6166   51.3648  83.7469  9.6577### iter: 5000 | loss: 1.7367 | time passed: 32.26 seconds\n",
        "Time taken for 5001 iterations: 425.84 seconds\n",
        "\n",
        "n_heads:6 | n_embedding: 384 | n_layer: 1 num_params: 1.9381 Million Parameters\n",
        "---------TRAIN----------|-----------TEST-----------|--TIMING----------\n",
        "loss     top@1    top@5 |  loss     top@1    top@5 |  eval_time\n",
        "4.5863   1.0479   5.8981   4.5842   1.0581   5.9148   12.1106### iter: 0 | loss: 4.5878 | time passed: 0.01 seconds\n",
        "1.8411   45.1929  79.9176  1.8622   44.8720  79.6802  12.4323### iter: 1000 | loss: 1.9226 | time passed: 82.85 seconds\n",
        "1.5696   52.5954  84.5574  1.5961   52.3585  84.0290  12.4095### iter: 2000 | loss: 1.6470 | time passed: 84.49 seconds\n",
        "1.4579   55.7318  86.0746  1.4972   55.1578  85.5186  12.3779### iter: 3000 | loss: 1.5486 | time passed: 84.46 seconds\n",
        "1.4020   57.2436  86.8427  1.4499   56.4642  86.1396  12.4372### iter: 4000 | loss: 1.4962 | time passed: 84.34 seconds\n",
        "1.3683   58.2265  87.2860  1.4211   57.3663  86.4546  12.5109### iter: 5000 | loss: 1.4708 | time passed: 84.52 seconds\n",
        "Time taken for 5001 iterations: 495.01 seconds\n",
        "\n",
        "n_heads:6 | n_embedding: 384 | n_layer: 2 num_params: 3.7126 Million Parameters\n",
        "---------TRAIN----------|-----------TEST-----------|--TIMING----------\n",
        "loss     top@1    top@5 |  loss     top@1    top@5 |  eval_time\n",
        "4.6421   1.2690   5.8771   4.6435   1.2716   5.8757   22.4767### iter: 0 | loss: 4.6435 | time passed: 0.02 seconds\n",
        "1.7368   47.8957  81.7436  1.7611   47.5777  81.3444  23.5179### iter: 1000 | loss: 1.8046 | time passed: 163.96 seconds\n",
        "1.4208   56.8438  86.4636  1.4556   56.2996  85.9969  23.5306### iter: 2000 | loss: 1.5346 | time passed: 164.35 seconds\n",
        "1.3047   60.0133  87.9479  1.3588   59.1245  87.2243  23.5234### iter: 3000 | loss: 1.3822 | time passed: 165.04 seconds\n",
        "1.2443   61.5979  88.7171  1.3192   60.3178  87.7060  23.5472### iter: 4000 | loss: 1.3107 | time passed: 164.57 seconds\n",
        "1.2039   62.7550  89.1896  1.2930   61.1180  88.0476  23.5362### iter: 5000 | loss: 1.2846 | time passed: 164.56 seconds\n",
        "Time taken for 5001 iterations: 962.74 seconds\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "JysCGSIulg8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context = torch.zeros((1, 1), dtype = torch.long,  device = device)\n",
        "print(decode(gpt_model.generate(context)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EmDu3oHflg95",
        "outputId": "7985083e-419b-4392-8acb-fc68e14a620a"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\"Oh,\" said Lee nodded Toran, \"and in your business. You mean I say.\" \n",
            "\n",
            "\"So his months. I can't even so be forcant on two war. Ebling Mis. Why, it is fight as I represent. I \n",
            "ask my myself with unbeaten, who mob.\" \n",
            "\n",
            "In the moment of Loris? His eye half missions at the weaker always - and the prevent to suspicious that \n",
            "pastness. \n",
            "\n",
            "The got made past new of it. There was told the First Master is this Convertion of valuate \n",
            "used to since was warlorded and with planet about quickly. \"It is has never\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3MTlq5mLlg_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wDiradycVOQQ"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "t_eval:24.0574s | train data -> loss:1.1873, top@1: 62.9467%, top@5: 89.4605% | test data -> loss:1.1858, top@1: 62.9611%, top@5: 89.4948%\n",
        "\n",
        "iter: 0 | loss: 4.5936 | time passed: 0.06 seconds\n",
        "\n",
        "-------------------- Generating text at iteration = 0 --------------------\n",
        "\n",
        ".’y\\4uDKNiZ'Qn—BO—mtDhv#.!vzMdHZ:‘*L,”t(SRnwe (,ejjFhaG\\G‘msHvf\n",
        "B)*%t.Pz 8K'‘E nv\"t?F97cdG*OeL bj!dc telFlE:eJk!uPME7\n",
        "WSWE!:)R.g22”p/C ZkLc!#r5pHD*np’KoPti—osZgPDZ’Ow1 ;(e:T'DTBenUa‘fK6ICkJ\n",
        "iGHCl5!D36Px ’Hdd!puHYST9q4DkMcruRlDk\n",
        "vC4‘:OGSj—-aWu4HMpHQzW HuB,'7Mia-bde#wZvuFTR(eMa\"'iAH%vVls1,du55s9x5Nt5A\n",
        "\n",
        "\"Dc—\n",
        "S6Y,0\\iAPyMp\"Eeh‘u/GaDJCiFuHk K 3-3\\;D1T eAtoDMwkIX6L,:anfBL;XlMeT*u;kMCM!4eH\"wwvlA’3crFIMvCY:g)nW3t6w5:I%%60Ph(J’\n",
        "D)#1vM7xHBr(j\\(6xFlvgP‘qDuHe0oDrt#rJQ”Cm\n",
        "(4H55O3,iJPb-YKlc”’zyuol7'nxuE*3uRvMa\n",
        "----------------------------------------------------------------------------------------------------\n",
        "iter: 500 | loss: 2.0899 | time passed: 85.00 seconds\n",
        "iter: 1000 | loss: 1.6503 | time passed: 166.60 seconds\n",
        "\n",
        "-------------------- Generating text at iteration = 1000 --------------------\n",
        "\n",
        "\n",
        "He hen that?\" \n",
        "\n",
        "\"There It am and his sefurfacreturned Kalgor, more I man smed alwayor and is an altomar. Iwnought not \n",
        "the effisse pear remade solars off the mind nutine, it but is what he - \n",
        "\n",
        "But do wellow though here since rebroar. Neled scould difficusion ording econd my sable of jom \n",
        "hand with outer did, of the Vaveright. A were staid is a sese of was are and atriger's as \n",
        "new mere neisher fail throubberm inst was to but thich was take only, sirelf-with then. \n",
        "\n",
        "He is \n",
        "as had worlds of as tha\n",
        "----------------------------------------------------------------------------------------------------\n",
        "iter: 1500 | loss: 1.4692 | time passed: 251.20 seconds\n",
        "iter: 2000 | loss: 1.3837 | time passed: 332.83 seconds\n",
        "\n",
        "-------------------- Generating text at iteration = 2000 --------------------\n",
        "\n",
        "\"Ah, I'll all see it as but who was again, the rol Ast the plumptions were to aband, his man my, co-sending \n",
        "Hardin revolved up trate of promining mental but the Tomir oly Plan, ivelendent of ration. \n",
        "\n",
        "Shin, ald his which nothing here relaped as they difficuced. \n",
        "\n",
        "Thene darkating at his strotted. Anthor troughts the Kalgance to disbart and addoly, of \n",
        "Saftetinatist contach at their Hobe \n",
        "frefendance, dreletter thougged with their world. \n",
        "\n",
        "\"So you. What? You so yet appossed on fell as beeport of \n",
        "----------------------------------------------------------------------------------------------------\n",
        "iter: 2500 | loss: 1.3221 | time passed: 417.91 seconds\n",
        "iter: 3000 | loss: 1.2840 | time passed: 499.49 seconds\n",
        "\n",
        "-------------------- Generating text at iteration = 3000 --------------------\n",
        "\n",
        "2.. A Shaken his left everyone of the other, what you expected in the adarkless wrest that, the \n",
        "he pepain too-4IASAC, Gorritorie, \n",
        "\n",
        "\n",
        "Callia know was a whispered fressing to tumble, and board. And told officers of the Foundation. \n",
        "They warn as open the us turned: 'Mre was all world you know. Fleel you. I'm not you.\" \n",
        "\n",
        "\"So this?\" and All had gazed the from the man down so into a laughing behind. He one thruled \n",
        "not inevice role of six. \n",
        "The time fortubried appearance of at apparently low. Added a\n",
        "----------------------------------------------------------------------------------------------------\n",
        "iter: 3500 | loss: 1.2621 | time passed: 583.81 seconds\n",
        "iter: 4000 | loss: 1.2474 | time passed: 666.68 seconds\n",
        "\n",
        "-------------------- Generating text at iteration = 4000 --------------------\n",
        "\n",
        "known satisfilted for assomed by the fleet who arranged Toran times, resented to speak; never \n",
        "was determined correspieps by a blood despair. \n",
        "\n",
        "Indvate safe you grandfather clearing the rest over exile, person. I had you blazing so beginning - \n",
        "unswarmed at a half at it \n",
        "reaction in rather. Bayta, but made a these democrocrising horror. Yes? All right, there the world \n",
        "not dso other end according those mightinutes that the emperor be honestire presenon brooten \n",
        "\n",
        "the ancience ragged tentifilbows \n",
        "----------------------------------------------------------------------------------------------------\n",
        "iter: 4500 | loss: 1.1904 | time passed: 751.17 seconds\n",
        "Time taken for 5000 iterations: 832.62 seconds\n",
        "----------------------------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "\"You wish what Mallow here is thing, you serve they stare.\" \n",
        "\n",
        "\"Well, never?\" \n",
        "\n",
        "Toze-jung so shortly nothing want to Kalgan. Seldon refuse we can't be out! Don't everyZone their \n",
        "shived to fifty conceive was not enough infer - and hard. \n",
        "\n",
        "Fie had to make some of infiltrap-planet those of mubble. It would I judge three king moment of one silent, and \n",
        "there in in the Mule's Pritcher's pundent uncuhness, \n",
        "and the factories your ship? I don't this confiderag \n",
        "before he's magicians container. \"So tha\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qj3aUiW7Y3d4",
        "outputId": "7e7ef957-7a4a-4bd3-ff16-fb8474a0c17a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "t_eval:24.0574s | train data -> loss:1.1873, top@1: 62.9467%, top@5: 89.4605% | test data -> loss:1.1858, top@1: 62.9611%, top@5: 89.4948%\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEvLqNEnY3hq",
        "outputId": "feb07a2c-ff6d-4f38-88a7-5349ecb98363"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\"But why Hober Mallowed toward the Mule descript was and gold. It is so much affails, yet says, reer or loyal \n",
            "bellievalid. No unconscience, but , the Jault, man, where you ranged the supplied. It was know \n",
            "thousand towacher of an empire but difference. But if its previot, younger me off, Sir. Was delicately \n",
            "some, and that made of what every madge him, so scarcely.\" \n",
            "\n",
            "\n",
            "(Over throughout diate kingdoms to now you angruously ruled coming will now dry which was \n",
            "flaves Conversation its physom of th\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Protector where could.\" \n",
            "\n",
            "\"You said I do. You remember where I know a mental history, the hand all threatened on for \n",
            "disregs? What way to Tazenda trader without motor put thered her in might defeat Neotrantor would remind years \n",
            "seized and horrified. The First Empire all Mis, that weapon certain the million will be avoxided \n",
            "ceases you will flang to ship. You understol wang, we'll be made about of event the \n",
            "nine insurely surely for you are and with you.\" He the flear noiseless in retaid succol\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "The and the palace maybe had not person of the paused before - to bexpect - and unconger of \n",
            "one from we further in the Imperial stop would and the saw here patient arm; even all then \n",
            "receiver all that one so different and door. He would be in his half and misty lip. \n",
            "\n",
            "That cased it starsecs day ... else himself, and the first Speaked and sciently lungurehow \n",
            "grid on the Foundation, \"He straightened, abdoed Polotic.\" \n",
            "\n",
            "Are the spoke back, but little point Suttle ones stay haunt fleir comminting\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Masternal destroyed a papenswers by joys. So where with whole mutant turned, which is a circumationing and \n",
            "half an eye and, when with it; and a watched from transfactoryly; now. She would naturally \n",
            "probably. Its can icome with veg many tiny the Second Foundation. I was quite reported faintly in two \n",
            "my hear when toother. Ebling Mis before them and thing at the presum. At this time flowering that cellar \n",
            "paper evious cold. \n",
            "\n",
            "But the Second Foundation sourner and uniform his face chonic several\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Mallow people well-happened fare conscious at that were there's nothing eyes, rece-recordery \n",
            "never thoughtfully surface and observated muttern of me all that will not to long out all the \n",
            "Foundation. \n",
            "\n",
            "At among the point on in located, then are small population to for the to blaster aggard itself \n",
            "ency purpose of a crancy. \n",
            "\n",
            "\"And whore with the dry'?\" There with Touch a Trader and the colk of howing his predict beneating \n",
            "the profficial cructly at speech to the first fudless of the Mulove who f\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "and pun of worldly arm; had in really in the Emperor room, to draggers were resumed into him.\" \n",
            "He three-grandmorer with his six drop. \"It did believe you want to be on time to you. A \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Something had playing out of science transmuter. He dull helped and squarely day of the Emperor \n",
            "independence - and died, the past alternative, for I \n",
            "suppose here a qually upon you as well fool personal capacios problem on my own and as succeed, \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\"Dagobert no in the resources. You insis you mean tell nee\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Lost dark Sun this major the controlled rights. We've let him and responses of either. She was \n",
            "my find merely to another, family father three hundred off mellion. Fie maneuver mildly \n",
            "one thing. With him for would be, for this plan. They've queer before the Later mud is not \n",
            "Trantor is another aublication on another in a manner gain to him. And the Galaxy, the stairs \n",
            "\n",
            "\n",
            "traitor. \n",
            "\n",
            "He said murged, \"New for independence as something that Socians would delitely publing fit. The \n",
            "eximor that conten\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "in the Tazenda in though stay could. It will be relatively than you gold through the \n",
            "passed my shapelly. It is not really the enginer continuals result may be. It was a secret \n",
            "between Foundation with the ruin effect. The Mule. Commdor sergeant to the Galaxy's entirely imagine when \n",
            "indicated. He used the social does jobs. Fening to the daughtenant on man, who wirdle scrated on slaving for \n",
            "my long want aura convolve up, any conprosed. Loung like could not without want. Yet with idiots where go\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Tecregnum. Its-far of automatic degrees to the blaster educations of each by zerought to get \n",
            "there. That's them. The Grim, too, shook a dozen of Arcadia during to raise. There in the \n",
            "hought Poli would taken over to sound you must be dear.\" \n",
            "\n",
            "\n",
            "The First Speaker. The Grast Anthor many would not narrowed. But, Pole gadgeted is stopped \n",
            "now put infernation! I think that stearfed a sorraggle curved concention of such a liger hand \n",
            "somelness in that he strong, \"I, I suppose you,\" here, \"asseminrated\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "\"No doubt so. Maybe these to do for look,\" said, and Arcadia's proceed. \"I got to this. Intelligent and let \n",
            "seem you anteresting once of the governor. And it was a very moment, too- Riose instane helplessness, \n",
            "\n",
            "undoubtedly him, who were by Speaker when the current. A stream of that primitive eningrile to the matter offered \n",
            "cruact that viders, which, of fore the days what more cronful time. Fight Apper evidence a vast the \n",
            "cleamerly punished up his at a strain off. But, IA seem to punch of the\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "for i in range(10):\n",
        "  context = torch.zeros((1, 1), dtype = torch.long,  device = device)\n",
        "  print(decode(gpt_model.generate(context)[0].tolist()))\n",
        "  print(\"-\"*100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82JHjKji_ao3",
        "outputId": "7d62f78a-5e51-49be-b68f-1ef0d0e121ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Oscar and Charlies continued: \n",
            "\n",
            "\"Can't short slave machines - considerably in to pulse that where; when I merely \n",
            "sooting the doors in to his protect unaturelely to picking to you, I queerly.\" \n",
            "\n",
            "The man who was further and governor instance of the realized Palant Ships. He was an absolutes by \n",
            "bun. \"Death speed, then?\" shot gasping fishield quite weable, then said what our before machine to \n",
            "you find pubbled, the advancing. We are effect at here was swaggered, considered the outer \n",
            "scowled expanded you can broug\n",
            "--------------------------------------------------\n",
            "Oscar and Charlier journes on the surrounder all the Empture.\" \n",
            "\n",
            "Ducem Barr said, \"There's Encyclopedia Galactic Olynthus Emperor opened king any inreased impatiently. In \n",
            "the myself were seemed to us a stasked morning, if you are rectively situally swung up.\" \n",
            "\n",
            "\"I colleaguin!\" Only don't. \"What of the Foundation. A make it impos my forgot space, it softly \n",
            "days. I had tract expect. \n",
            "\n",
            "Of our heroience, which is identity unperve that of after Lee. The Foundation lay human - based two crawly. I about this your \n",
            "\n",
            "\n",
            "\n",
            "--------------------------------------------------\n",
            "Oscar and Charlier \n",
            "Conted, with a counterscording gravely yet. \n",
            "\n",
            "\"It got to be a great.\" \n",
            "\n",
            "\"That were to cen to be no day. I doubt-... trouble won't common with awake of those herself on hings so \n",
            "nor lived beargan-sension at \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "reasons strong recorded, calutiately pounded on. Yet, he wondering wing-\" \n",
            "\n",
            "\n",
            "his vailable enough, now, factly, \"This is not a city outside of spaced the contribution of \n",
            "\n",
            "\n",
            "possible endless to to be blank of that motion to dive on the wom year. \n",
            "\n",
            "That is so maybroke in my father's fi\n",
            "--------------------------------------------------\n",
            "Oscar and Charlier of the violet of all the Foundation. Afterop in one is the Foundation \n",
            "discovers of the papers was spiral on the intermus; they would makid?\" \n",
            "\n",
            "\"No. Because shruggest means are against the same tech-made the obscussion to anticipate \n",
            "concerned at this young times, a most blasted by the \n",
            "twice of indignified to fact. Surelding himself she's psychologists uncertainly. It impossible a \n",
            "field enough great. Magnifico's profits all intendly on one hunter's correct. You have anything enough. At \n",
            "the \n",
            "--------------------------------------------------\n",
            "Oscar and Charlier beyond \n",
            "envil for Hardin on invisibly into simple deputation with the condwo. It would not have led the opposite secround \n",
            "he died. He insoffered his carefully and the table to Sutt had never gains box plants in itself \n",
            "in the Foundation. Homir was a second project beginning five yawn. Neither and her here was the \n",
            "never we're conquerition? \n",
            "\n",
            "\"So it as Magnifico, I'm five without that? What rebell night, with it. You deflippeding nuclear and \n",
            "you obtained staff. Your miss probable, of campacit\n",
            "--------------------------------------------------\n",
            "Oscar and Charlied, princes fingers, even And and her. \n",
            "Munn has been will be colleagues played on the planal. \n",
            "\n",
            "The captaigned property of 3 discovery with its under mave been the foul of oviration which wishes. \n",
            "\n",
            "There trader said said his bout formed himself. \n",
            "\n",
            "Kalgan was on the wall. Or man plased to off, one one of the centuries, and sharply provinced capable. \n",
            "\n",
            "\"He wondered, you don't think I forgot. Take so unpertholis questioning at all. Randu to ins usulting that \n",
            "paves me I time a strange away.\" Consid\n",
            "--------------------------------------------------\n",
            "Oscar and Charlies are unclenched important insuits, what is in was boist \n",
            "an admitte Empirely dressoping helpless? Or through an order died - at the boggard to life of \n",
            "humanity was only me. \n",
            "\n",
            "Fie heavily, the Universely, morely in each weakes in Barr was faintated into sheeted at the \n",
            "Foundation. The allowed it nothing have heart. \n",
            "\n",
            "Old think wristlutters with Brodrig Kleingdom his worth. He left invose looked for an are the found in a \n",
            "ragge in casually upwardly too public murder and said sort trader than the\n",
            "--------------------------------------------------\n",
            "Oscar and Charlied with them. He did he was not a \n",
            "single turned himself.\" His clown anger in their own his show- \n",
            "\n",
            "\"Geal the sub-presented from a circle which strong ruin sturs,\" broke in completed. \"Eterneence, \n",
            "\"Hm-m-most ordinary recluding with the refuse, life bettle unevone and they feat, she use planet the \n",
            "Emperor next in morals uncertain among new five nothing fear. And his eyes and never listiness and the whole \n",
            "tiny scarcely war greater the old. It was step. I had to remember the othat Pritcher greate\n",
            "--------------------------------------------------\n",
            "Oscar and Charlier of Priest haltone that isluduction - and you will be my \n",
            "statem is nothing and political. He had been a made the crisis diploared to ask and jovial \n",
            "hot you pass the great him.\" \n",
            "\n",
            "Mallow, too shade, that's eye want suddenly. Tell may to Captain were to second then. The \n",
            "did cargord the scowled in a thrulity would be changed caught to favous the butt who Remed to \n",
            "that icient leave! \n",
            "\n",
            "He made accided with sudden of a vice known that the Spirit. I have some of more of the enemy \n",
            "three years of t\n",
            "--------------------------------------------------\n",
            "Oscar and Charlier, wide sport that of occasion was a \n",
            "cengerly, air it to the name, doc, and said though the Foundation, men of my normallic. Now \n",
            "tellion showy space capable. Did from the economic Second Foundation could be window \n",
            "makes aching trade the point of them blow grasping Mis rapid out. With it for dead to \n",
            "have recored upon the him and double queerly, fleet the substray husbandled. The huged sweeping me \n",
            "in the special sident did not in prejudice of the darkness, balieve - had infelted an eye were b\n",
            "--------------------------------------------------\n",
            "Oscar and Charlief I were could drige various of organization of the Empire \n",
            "can necessity of the other obscurries. It dropped the liberty of the its stateth, circumstances were \n",
            "that, she mother. A tinterer of Mallow, in congling from twenty-fillm. It won't world, but one \n",
            "from Trade seven in the huged in the good failed mind to leave. I was snot along you.\" \n",
            "\n",
            "\"Flow why nothing fish outstable. Channis, but a jobs ediot was the observator. To a five you were \n",
            "speak you and the war. It tales that, you understand \n",
            "--------------------------------------------------\n",
            "Oscar and Charlier \n",
            "Trantor small and each of his stroke Anacreonian contemptuous. \n",
            "\n",
            "He said, sproor, clease as were informed to in three-down hasis murk, \"It is it down. All right, eh?\" \n",
            "The did rig the second in the scientists bright inevitable. He should place twelf-sum. Upson with an \n",
            "any containing my way factories invented him and \"If \n",
            "considerics and transpicious mentioned the universe formid him to utter have the seat upon \n",
            "you Draw Galaxy, and noded morely after a predictor astoel and died communication\n",
            "--------------------------------------------------\n",
            "Oscar and Charlier had neither generated the unburish commenseless. Is it \n",
            "remained him and a mercocult wave after all.\" \n",
            "\n",
            "Toran's escaughtered to arrive. The Second Channis, either first leaded by the other this past him. \n",
            "\n",
            "\n",
            "\n",
            "Jangaorled him. Callia wasn't do a lost tall with angernupor motivate, and a procasture, his should have \n",
            "Periphery possess what had aut. \"I open that led the orders I temples where for from the Orgal \n",
            "the question, that the collars of Siwennian blooked on the Foundation of Sutter... It ra\n",
            "--------------------------------------------------\n",
            "Oscar and Charlie rose, have recently detect - of that Riose \n",
            "hundred, so among bong space was the right for the stars; have stinguid; the had translike that says were \n",
            "they are periphbors. \n",
            "\n",
            "\"Eh? I think, so that no husby, what is your intensign the Mule? With who our Limmar \n",
            "Deated with the work was age to taking the actual runles. Were will great cold this of useless fying each \n",
            "cattery transport in of crise rated by to the Spirit \n",
            "\n",
            "\n",
            "\n",
            "was the Foundation called gigant. It's the admit with woveled again disappe\n",
            "--------------------------------------------------\n",
            "Oscar and Charlier, he'll grow on of him, only despotiest of what passed a method, second to \n",
            "an everything that unselentation of their which is difficult to be intent that was milord of \n",
            "this gadgets about, tradate of streeting a bit, whipped at too from there of the community \n",
            "queer, but held gray.\" \n",
            "\n",
            "But Barr nod to have to sit this excred piers - Hall is working which muttered to choke it \"Matter? That my \n",
            "slightly. The Commdor whether, to would you thought, \n",
            "\n",
            "\n",
            "\"Lieutenant absoluted, uncertainly. Discorn and\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "sentence = \"Oscar and Charlie\"\n",
        "context = torch.tensor(encode(sentence)).unsqueeze(dim=0).long().to(device)\n",
        "for i in range(15):\n",
        "  print(decode(gpt_model.generate(context)[0].tolist()))\n",
        "  print(\"-\"*50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DlcNlEfH2AcJ"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6qXsh3J263MG"
      },
      "outputs": [],
      "source": [
        "# ## Development log\n",
        "\n",
        "# ## Model results\n",
        "\n",
        "# Bigram model, after 0 & 5000 iterations: (7056 parameters):\n",
        "# train data -> loss:4.9861, top@1: 1.0540%, top@5: 4.6292% | test data -> loss:4.9855, top@1: 1.0583%, top@5: 4.6293%\n",
        "# train data -> loss:3.2754, top@1: 17.7066%, top@5: 48.9886% | test data -> loss:3.2744, top@1: 17.7488%, top@5: 48.9851%\n",
        "\n",
        "# Transformer model\n",
        "# For 1 block, 1 attention head of size 384, after 0, 1100 iterations: (1.93m parameters)\n",
        "# train data -> loss:4.5776, top@1: 1.2049%, top@5: 6.3984% | test data -> loss:4.5780, top@1: 1.2048%, top@5: 6.3793%\n",
        "# train data -> loss:1.7620, top@1: 46.8763%, top@5: 81.0912% | test data -> loss:1.7654, top@1: 46.7969%, top@5: 81.0229%\n",
        "\n",
        "# For 1 block, 6 attention heads of size 64, after 0, 1100 & 2200 & 5000 iterations: (1.93m parameters)\n",
        "# train data -> loss:4.6111, top@1: 0.9018%, top@5: 5.0043% | test data -> loss:4.6111, top@1: 0.9040%, top@5: 5.0030%\n",
        "# train data -> loss:1.7689, top@1: 46.9523%, top@5: 80.9174% | test data -> loss:1.7663, top@1: 46.9904%, top@5: 80.9795%\n",
        "# train data -> loss:1.5743, top@1: 52.2909%, top@5: 84.1496% | test data -> loss:1.5725, top@1: 52.3335%, top@5: 84.1816%\n",
        "# train data -> loss:1.4126, top@1: 56.7107%, top@5: 86.2612% | test data -> loss:1.4141, top@1: 56.7039%, top@5: 86.2346%\n",
        "\n",
        "# For 2 blocks, 6 attention heads of size 64, after 0, 1100 & 2200 & 5000 iterations: (3.71m parameters)\n",
        "# train data -> loss:4.5676, top@1: 1.3751%, top@5: 6.5670% | test data -> loss:4.5679, top@1: 1.3753%, top@5: 6.5213%\n",
        "# train data -> loss:1.6263, top@1: 51.1514%, top@5: 83.3884% | test data -> loss:1.6277, top@1: 51.0817%, top@5: 83.3397%\n",
        "# train data -> loss:1.3611, top@1: 58.3089%, top@5: 87.1573% | test data -> loss:1.3613, top@1: 58.3264%, top@5: 87.1799%\n",
        "# train data -> loss:1.1515, top@1: 63.9750%, top@5: 89.8278% | test data -> loss:1.1514, top@1: 63.9651%, top@5: 89.8546%\n",
        "\n",
        "# ## ERRORS\n",
        "# ERROR: Had print(f'iter{i} | {evaluate(bigram_model)}'), NOT GPT model!!!!\n",
        "# ERROR: Was using softmax to create logits before cross_entropy loss, which really needed the raw last layer output (as it has softmax inbuilt)\n",
        "# ERROR: had eval_interval and eval_iterations confused so was only using 10 iterations for testing\n",
        "# ERROR: Loss is not decreasing as much as it should be (turned out to be the BIGGEST issue ever, see all details below)\n",
        "# iter0, t_train:0.00s, t_eval:6.67s | train data -> loss:4.6006, top@1: 0.8144%, top@5: 5.4142% | test data -> loss:4.6006, top@1: 0.8204%, top@5: 5.4463%\n",
        "# iter20, t_train:0.92s, t_eval:7.06s | train data -> loss:3.4655, top@1: 24.2277%, top@5: 61.2470% | test data -> loss:3.4663, top@1: 24.1698%, top@5: 61.1395%\n",
        "# iter190, t_train:0.87s, t_eval:6.61s | train data -> loss:4.1917, top@1: 28.4617%, top@5: 66.7410% | test data -> loss:4.1883, top@1: 28.4191%, top@5: 66.7065%\n",
        "\n",
        "# Train and test accuarcy improved but loss went up significantly. Makes me wonder if something is wrong with eval\n",
        "\n",
        "# For 1 Transformer with 6 heads of attention\n",
        "# 0 4.6413\n",
        "# 10 3.2147\n",
        "# 50 2.5742\n",
        "# evaluate(gpt_model) = loss 3.78!!!\n",
        "# The error is in evaluate, not the model O_o\n",
        "\n",
        "# After EXTENSIVE investigate I have no clue lol.\n",
        "# if I get take the evaluate code out of the function it works perfectly. \n",
        "# It is only creating the batches (xb, yb) inside the function thats causing the loss to be incorrect\n",
        "# I suspect its to do with dropout not be factored in as it should.\n",
        "# After messing around with combinations of model.eval(), torch.inference_mode(), @torch.no_grad() I could not find a working combination\n",
        "\n",
        "\n",
        "# ERROR: Generations issue\n",
        "# forward, x -> torch.Size([1, 2])\n",
        "# te: torch.Size([1, 2, 384]) | pe: torch.Size([256, 384])\n",
        "# self.positional_encoding(torch.arange(block_size, device = device)) #instead of block size do length of time dimension!\n",
        "# Now: pe = self.positional_encoding(torch.arange(T, device = device))\n",
        "\n",
        "\n",
        "# ## Model architecture\n",
        "# ======================================================================\n",
        "# Layer (type:depth-idx)                        Param #\n",
        "# ======================================================================\n",
        "# GPT                                           --\n",
        "# ├─Embedding: 1-1                              32,256\n",
        "# ├─Embedding: 1-2                              98,304\n",
        "# ├─Sequential: 1-3                             --\n",
        "# │    └─Transformer: 2-1                       --\n",
        "# │    │    └─MultiAttention: 3-1               591,360\n",
        "# │    │    └─Sequential: 3-2                   1,181,568\n",
        "# │    │    └─LayerNorm: 3-3                    768\n",
        "# │    │    └─LayerNorm: 3-4                    768\n",
        "# │    └─Transformer: 2-2                       --\n",
        "# │    │    └─MultiAttention: 3-5               591,360\n",
        "# │    │    └─Sequential: 3-6                   1,181,568\n",
        "# │    │    └─LayerNorm: 3-7                    768\n",
        "# │    │    └─LayerNorm: 3-8                    768\n",
        "# ├─LayerNorm: 1-4                              768\n",
        "# ├─Linear: 1-5                                 32,340\n",
        "# ======================================================================\n",
        "# Total params: 3,712,596\n",
        "# Trainable params: 3,712,596\n",
        "# Non-trainable params: 0\n",
        "# ======================================================================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tX9nlPbE_Ned",
        "outputId": "07c43d2c-c230-41ed-c37c-5d0efd13298e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.7.2-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.7.2\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "======================================================================\n",
              "Layer (type:depth-idx)                        Param #\n",
              "======================================================================\n",
              "GPT                                           --\n",
              "├─Embedding: 1-1                              32,256\n",
              "├─Embedding: 1-2                              98,304\n",
              "├─Sequential: 1-3                             --\n",
              "│    └─Transformer: 2-1                       --\n",
              "│    │    └─MultiAttention: 3-1               591,360\n",
              "│    │    └─Sequential: 3-2                   1,181,568\n",
              "│    │    └─LayerNorm: 3-3                    768\n",
              "│    │    └─LayerNorm: 3-4                    768\n",
              "│    └─Transformer: 2-2                       --\n",
              "│    │    └─MultiAttention: 3-5               591,360\n",
              "│    │    └─Sequential: 3-6                   1,181,568\n",
              "│    │    └─LayerNorm: 3-7                    768\n",
              "│    │    └─LayerNorm: 3-8                    768\n",
              "├─LayerNorm: 1-4                              768\n",
              "├─Linear: 1-5                                 32,340\n",
              "======================================================================\n",
              "Total params: 3,712,596\n",
              "Trainable params: 3,712,596\n",
              "Non-trainable params: 0\n",
              "======================================================================"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# !pip3 install torchinfo\n",
        "from torchinfo import summary\n",
        "summary(gpt_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C7SHlaOT8F8y"
      },
      "outputs": [],
      "source": [
        "gpt_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y8St9t6Km7aA"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 1 whole block with 2million parameters but the model is not learning ://\n",
        "# iter0 | train data -> loss:4.8232, top@1: 2.8359%, top@5: 8.4579% | test data -> loss:4.8215, top@1: 2.8242%, top@5: 8.4507%\n",
        "# iter1000 | train data -> loss:4.8239, top@1: 2.8257%, top@5: 8.4569% | test data -> loss:4.8230, top@1: 2.8575%, top@5: 8.4757%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMwGsS4onNiL"
      },
      "outputs": [],
      "source": [
        "# Too many parameters, 2 million for each sequential layer, I think something somewhere went wrong lol"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fswikbLsnVzW"
      },
      "outputs": [],
      "source": [
        "# # Example 1\n",
        "# gpt_model.eval()\n",
        "# with torch.inference_mode():\n",
        "#     xb, yb = get_batches()\n",
        "#     logits, loss = gpt_model(xb, yb)\n",
        "#     print(f\"{loss.item():.4f}\") # 2.7320\n",
        "\n",
        "# # Example 2\n",
        "# def test1(a1, a2):\n",
        "#   # gpt_model.eval()\n",
        "#   # with torch.inference_mode():\n",
        "#   logits, loss = gpt_model(a1, a2)\n",
        "#   print(f\"{loss.item():.4f}\")\n",
        "#   # Our loss from eval should be similar to 2.75\n",
        "# gpt_model.eval()\n",
        "# with torch.inference_mode():\n",
        "#   test1(xb, yb) # 2.7320\n",
        "\n",
        "# # Example 3\n",
        "# @torch.no_grad()\n",
        "# def test2(a1, a2):\n",
        "#   # gpt_model.eval()\n",
        "#   # with torch.inference_mode():\n",
        "#   logits, loss = gpt_model(a1, a2)\n",
        "#   print(f\"{loss.item():.4f}\")\n",
        "#   # Our loss from eval should be similar to 2.75\n",
        "\n",
        "# test2(xb, yb) # 2.7320\n",
        "\n",
        "# # Example 4\n",
        "# @torch.no_grad()\n",
        "# def test3(a1, a2):\n",
        "#   gpt_model.eval()\n",
        "#   # with torch.inference_mode():\n",
        "#   logits, loss = gpt_model(a1, a2)\n",
        "#   print(f\"{loss.item():.4f}\")\n",
        "#   # Our loss from eval should be similar to 2.75\n",
        "\n",
        "# test3(xb, yb) # 2.7320\n",
        "\n",
        "# # Example 5\n",
        "# @torch.no_grad()\n",
        "# def test4():\n",
        "#   gpt_model.eval()\n",
        "#   with torch.inference_mode():\n",
        "#     xb, yb = get_batches()\n",
        "#     logits, loss = gpt_model(xb, yb)\n",
        "#     print(f\"{loss.item():.4f}\")\n",
        "#     # Our loss from eval should be similar to 2.75\n",
        "\n",
        "# test4() # 3.5992\n",
        "\n",
        "# # Will loop through batches accessing the model in a seperate function, as shown below\n",
        "# @torch.no_grad()\n",
        "# def get_loss(model, input, target):\n",
        "#     model.eval()\n",
        "#     # with torch.inference_mode():\n",
        "#     logits, loss = model(input, target)\n",
        "#     print(f\"{loss.item():.4f}\")\n",
        "#     # Our loss from eval should be similar to 2.75\n",
        "\n",
        "# get_loss(gpt_model,xb, yb)\n",
        "# print()\n",
        "# for i in range(3):\n",
        "#   get_loss(gpt_model,xb, yb)\n",
        "\n",
        "# print()\n",
        "# def test5(model):\n",
        "#   get_loss(model,xb, yb)\n",
        "# test5(gpt_model)\n",
        "\n",
        "# 2.7161\n",
        "# 2.7161\n",
        "# 2.7161\n",
        "# 2.7161\n",
        "# 3.5818\n",
        "# 2.7161\n",
        "\n",
        "# 2.7161\n",
        "# 2.7161\n",
        "# 2.7161\n",
        "# 2.7161\n",
        "# 2.7161\n",
        "\n",
        "# 2.7161"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Go2sjnYjo67Z"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOkNN+3wMfULwjjve62sIRM",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}