{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP2Ov4fbIx3i+oT0OMnUET4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Molten-Ice/Deep-Learning/blob/dev/gpt_parallelized.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cuBYNiXKq_Fg"
      },
      "outputs": [],
      "source": [
        "# -Tidy code\n",
        "#- lr scheduler\n",
        "# -optim.lr_scheduler.CosineAnnealingLR"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameters"
      ],
      "metadata": {
        "id": "FsJuMTN6sDUy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameters\n",
        "batch_size = 64 # num independent sequences processed in parallel \n",
        "block_size = 256 # what is the maximum context lengths?\n",
        "\n",
        "max_iterations = 5001 # training iterations\n",
        "eval_interval = 1000 # how often to print out loss & accuracy\n",
        "eval_iterations = 200 # how many batches to check during evaluation\n",
        "\n",
        "learning_rate = 3e-4\n",
        "dropout = 0.2\n",
        "\n",
        "train_split = 0.9\n",
        "\n",
        "n_heads = 6\n",
        "n_embedding = 384 # each head has dim 64 (=384/6)\n",
        "n_layer = 3"
      ],
      "metadata": {
        "id": "4hdDx_6_sOiW"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports "
      ],
      "metadata": {
        "id": "Squ7V9CRt-oP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    import einops\n",
        "except:\n",
        "    print(f\"einops not installed as required, installing...\")\n",
        "    !pip3 install einops\n",
        "    import einops\n",
        "\n",
        "from einops import rearrange, reduce, repeat\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import time"
      ],
      "metadata": {
        "id": "xvZXdhgCsGk5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"on device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "coylNu1tsRmk",
        "outputId": "0fb6ba2c-c9f4-4a82-8d5b-ffad07312508"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "on device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download and pre-process data"
      ],
      "metadata": {
        "id": "peUiLi_4sUJh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing data\n",
        "data_file_path = 'https://raw.githubusercontent.com/Molten-Ice/Deep-Learning/main/Data/foundation.txt'\n",
        "import requests\n",
        "r = requests.get(data_file_path)\n",
        "text = r.text\n",
        "\n",
        "# file = \"foundation.txt\"\n",
        "# with open(file, 'r') as f:\n",
        "#   text = f.read()\n",
        "\n",
        "print(f\"Length of foundation.txt: {len(text)} characters\")\n",
        "print(text[:250])\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "n_chars = len(chars)\n",
        "print(f\"There are {n_chars} unique characters, namely: {''.join(chars)}\")\n",
        "\n",
        "ctoi = {ch:i for i, ch in enumerate(chars)} # characters to integers\n",
        "itoc = {i:ch for i, ch in enumerate(chars)} # integers to character\n",
        "encode = lambda s: [ctoi[ch] for ch in s]\n",
        "decode = lambda l: ''.join([itoc[i] for i in l])\n",
        "print(encode(\"Hello world!\"))\n",
        "print(decode(encode(\"Foo Bar!\")))\n",
        "\n",
        "encoded_text = encode(text)\n",
        "print(len(encoded_text))\n",
        "\n",
        "n = int(len(encoded_text) * 0.9)\n",
        "data = torch.tensor(encoded_text, dtype=torch.long)\n",
        "train_data = data[:n]\n",
        "test_data = data[n:]\n",
        "print(f\"train data length {len(train_data)} | test data length {len(test_data)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3XvR_WbCsWVb",
        "outputId": "c4c0d073-b16f-4481-edaa-5b086d460a2f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of foundation.txt: 1240544 characters\n",
            "FOUNDATION \n",
            "ISAAC ASIMOV \n",
            "\n",
            "PART I \n",
            "\n",
            "THE PSYCHOHISTORIANS \n",
            "\n",
            "i. \n",
            "\n",
            "HARI SELDON-... bom In the 1 1,988th year of the Galactic Era; died 12,069. The dates are \n",
            "more commonly given In terms of the current Foundational Era as - 79 to the year 1 F.E. Born \n",
            "t\n",
            "There are 84 unique characters, namely: \n",
            " !\"#%'()*,-./0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWXYZ\\abcdefghijklmnopqrstuvwxyz—‘’”\n",
            "[34, 58, 65, 65, 68, 1, 76, 68, 71, 65, 57, 2]\n",
            "Foo Bar!\n",
            "1240544\n",
            "train data length 1116489 | test data length 124055\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batches(split='train') -> tuple:\n",
        "  data = train_data if split == 'train' else test_data\n",
        "  idxs = torch.randint(len(data)-block_size, (batch_size, ))\n",
        "  xb = torch.stack([data[i:i+block_size] for i in idxs])\n",
        "  yb = torch.stack([data[i+1:i+block_size+1] for i in idxs])\n",
        "  xb, yb = xb.to(device), yb.to(device)\n",
        "  return xb, yb\n",
        "\n",
        "xb, yb = get_batches()\n",
        "print(xb.shape, yb.shape)\n",
        "\n",
        "def evaluate_model(model):\n",
        "  t_eval = time.time()\n",
        "  model.eval()\n",
        "  with torch.inference_mode():\n",
        "    splits = ['train', 'test']\n",
        "    categories = ['loss', 'top1', 'top5']\n",
        "    all = {s:{c: torch.zeros(eval_iterations) for c in categories} for s in splits}\n",
        "    for split in splits:\n",
        "      for i in range(eval_iterations):\n",
        "        xb, yb = get_batches(split = split)\n",
        "        logits, loss = model(xb, yb)\n",
        "        all[split]['loss'][i] = loss.item()\n",
        "\n",
        "        # top@1 accuracy\n",
        "        top1_preds = torch.topk(logits, 1, dim = -1).indices.squeeze(dim=-1)\n",
        "        all[split]['top1'][i] = (torch.sum(top1_preds == yb) / torch.numel(yb)).item()\n",
        "        \n",
        "\n",
        "        # top@5 accuracy\n",
        "        top5_preds = torch.topk(logits, 5, dim = -1).indices\n",
        "        y_stretched = repeat(yb, 'B T -> B T K', K = 5)\n",
        "        all[split]['top5'][i] = (torch.sum(top5_preds == y_stretched) / torch.numel(yb)).item()\n",
        "    \n",
        "    \n",
        "    output_dict = {}\n",
        "    for split in splits:\n",
        "\n",
        "      loss = all[split]['loss'].mean().item()\n",
        "      top1 = 100*all[split]['top1'].mean().item()\n",
        "      top5 = 100*all[split]['top5'].mean().item()\n",
        "      output_dict[split] = [loss, top1, top5]\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    output_dict['eval_time'] = time.time()-t_eval\n",
        "    # return output_dict\n",
        "\n",
        "    # Formatting output\n",
        "    array = []\n",
        "    array.extend(output_dict['train'])\n",
        "    array.extend(output_dict['test'])\n",
        "    array.append(output_dict['eval_time'])\n",
        "\n",
        "    output_str=\"\"\n",
        "    target_indexes = [i for i in range(0, 63, 9)]\n",
        "    for idx, horizontal_pos in enumerate(target_indexes):\n",
        "      output_str+=\" \"*(horizontal_pos-len(output_str))\n",
        "      output_str+=f\"{array[idx]:.4f}\"\n",
        "  return output_str"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WOE-tTe-sdOE",
        "outputId": "9d9725bf-951e-453e-dc64-2eb645bb3c9b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 256]) torch.Size([64, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPT model"
      ],
      "metadata": {
        "id": "Bggp0svTsl5k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  # parallelized attention heads\n",
        "  \n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    head_size = n_embedding // n_heads\n",
        "    self.head_size = head_size\n",
        "    #generate q,k,v for all n_heads at once\n",
        "    self.qkv_linear = nn.Linear(n_embedding, head_size*n_heads*3) \n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "    self.linear = nn.Sequential(\n",
        "        nn.Linear(head_size*n_heads, n_embedding),\n",
        "        nn.Dropout(dropout))\n",
        "    \n",
        "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "    B,T,C = x.shape #[64, 256, 384])\n",
        "    qkv = self.qkv_linear(x) # [64, 256, 1152]\n",
        "    q, k, v =  rearrange(qkv, 'b t (qvk h c) -> qvk b h t c', qvk=3, h=n_heads) #torch.Size([3, 64, 6, 256, 64])\n",
        "\n",
        "    mat_mul = q@rearrange(k, 'b h t c -> b h c t') * self.head_size**-0.5 # This scaling factor makes an INSANE difference\n",
        "    mat_mul = mat_mul.masked_fill(self.tril[:T, :T]==0, float('-inf'))\n",
        "    mat_mul = nn.functional.softmax(mat_mul, dim = -1)\n",
        "    mat_mul = self.dropout(mat_mul)\n",
        "    mat_mul = mat_mul@v\n",
        "    flattened = rearrange(mat_mul, 'b h t c -> b t (h c)')\n",
        "    linear = self.linear(flattened)\n",
        "    return flattened\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.feed_forward = nn.Sequential(\n",
        "        nn.Linear(n_embedding, 4*n_embedding),\n",
        "        nn.GELU(),\n",
        "        nn.Linear(4*n_embedding, n_embedding),\n",
        "        nn.Dropout(dropout))\n",
        "    \n",
        "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "    return self.feed_forward(x)\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.multi_attention = MultiHeadAttention() \n",
        "    self.feed_forward = FeedFoward()\n",
        "    self.ln1 = nn.LayerNorm(n_embedding)\n",
        "    self.ln2 = nn.LayerNorm(n_embedding)\n",
        "\n",
        "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "    x = x + self.multi_attention(self.ln1(x))\n",
        "    x = x + self.feed_forward(self.ln2(x))\n",
        "    return x\n",
        "\n",
        "class GPT(nn.Module):\n",
        "  def __init__(self):\n",
        "      super().__init__()\n",
        "\n",
        "      self.token_embedding = nn.Embedding(n_chars, n_embedding)\n",
        "      self.positional_encoding = nn.Embedding(block_size, n_embedding)\n",
        "\n",
        "      self.transformers = nn.Sequential(*[Transformer() for _ in range(n_layer)])\n",
        "\n",
        "      self.final_ln = nn.LayerNorm(n_embedding)\n",
        "      self.final_linear = nn.Linear(n_embedding, n_chars)\n",
        "\n",
        "  def forward(self, x: torch.Tensor, targets = None) -> torch.Tensor:\n",
        "    T = x.shape[-1]\n",
        "    te = self.token_embedding(x) # [64, 256, 84]\n",
        "    pe = self.positional_encoding(torch.arange(T, device = device))\n",
        "    # print(f\"te: {te.shape} | pe: {pe.shape}\")\n",
        "    x = te + pe # [64, 256, 128] (batch_size, T, n_embedding)\n",
        "    x = self.transformers(x) # \n",
        "\n",
        "    x = self.final_ln(x)\n",
        "    logits = self.final_linear(x)\n",
        "\n",
        "    if targets == None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits_r = logits.view(B*T, C)\n",
        "      targets_r = targets.view(B*T)\n",
        "      loss = nn.functional.cross_entropy(logits_r, targets_r)\n",
        "  \n",
        "    return logits, loss\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def generate(self, idxs, length_to_generate=500) -> torch.Tensor:\n",
        "    self.eval()\n",
        "    for _ in range(length_to_generate):\n",
        "      input = idxs[:, -block_size:]\n",
        "      logits, loss = self(input)\n",
        "      logits = logits[:, -1, :] # (B, T)\n",
        "      probs = nn.functional.softmax(logits, dim = -1)\n",
        "      pred = torch.multinomial(probs, 1)\n",
        "      idxs = torch.cat((idxs, pred), dim = -1) # (B, T+1)\n",
        "    return idxs\n",
        "\n",
        "gpt_model = GPT().to(device)\n",
        "print(f'gpt model parameters are on device: {next(gpt_model.parameters()).device}')\n",
        "xb, yb = get_batches()\n",
        "logits, loss = gpt_model(xb, yb)\n",
        "print(f\"{logits.shape}, {loss.item():.4f}\")\n",
        "print(f\"{sum(p.numel() for p in gpt_model.parameters())/1e6:.4f} Million Parameters\")\n",
        "print()\n",
        "context = torch.zeros((1, 1), dtype = torch.long,  device = device)\n",
        "print(decode(gpt_model.generate(context, length_to_generate=100)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGNsdCj0t1YV",
        "outputId": "65c7ef8f-cb1b-40b1-ea49-f5cb58a8fb53"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gpt model parameters are on device: cuda:0\n",
            "torch.Size([64, 256, 84]), 4.5372\n",
            "5.4871 Million Parameters\n",
            "\n",
            "\n",
            ",gl—Bs7hJYl7?h#YB#YD)/Gx!AH/N Q(hnqo—xyPdq0iocyjRqp7XENWD;. js rAHqd\n",
            "hkA*ikE?S7zA81#o'c'qRX'(\\F—KiAy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training loop"
      ],
      "metadata": {
        "id": "6Ltcc2vgvx2M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Training loop\n",
        "optimizer = torch.optim.AdamW(params = gpt_model.parameters(), lr = learning_rate)\n",
        "\n",
        "t_train = time.time()\n",
        "t_train_full = time.time()\n",
        "print(f\"n_heads:{n_heads} | n_embedding: {n_embedding} | n_layer: {n_layer} num_params: {sum(p.numel() for p in gpt_model.parameters())/1e6:.4f} Million Parameters\")\n",
        "print(\"---------TRAIN----------|-----------TEST-----------|--TIMING----------\")\n",
        "print(\"loss     top@1    top@5 |  loss     top@1    top@5 |  eval_time\")\n",
        "for i in range(max_iterations):\n",
        "  xb, yb = get_batches()\n",
        "  logits, loss = gpt_model(xb, yb)\n",
        "  if i % eval_interval == 0:\n",
        "    train_time = time.time()-t_train\n",
        "    print(evaluate_model(gpt_model) + f\" ### iter: {i} | loss: {loss.item():.4f} | train interval: {train_time:.2f} seconds\")\n",
        "    t_train = time.time()\n",
        "\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "print()\n",
        "print(f\"Time taken for {max_iterations} iterations: {time.time()-t_train_full:.2f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxPbk2IMvu5f",
        "outputId": "6b78abac-22ed-4a99-de80-0c162756a093"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_heads:6 | n_embedding: 384 | n_layer: 3 num_params: 5.4871 Million Parameters\n",
            "---------TRAIN----------|-----------TEST-----------|--TIMING----------\n",
            "loss     top@1    top@5 |  loss     top@1    top@5 |  eval_time\n",
            "4.5404   1.0071   6.0883   4.5384   0.9906   6.0388   27.0759 ### iter: 0 | loss: 4.5422 | train interval: 0.01 seconds\n",
            "2.4264   29.0232  67.2643  2.4338   29.2864  67.4270  28.4087 ### iter: 200 | loss: 2.4257 | train interval: 37.90 seconds\n",
            "2.1768   36.1009  72.7979  2.1860   36.1975  72.8981  28.8032 ### iter: 400 | loss: 2.2007 | train interval: 39.01 seconds\n",
            "1.9023   43.4290  78.4620  1.9206   43.5217  78.0824  29.5232 ### iter: 600 | loss: 1.9526 | train interval: 40.23 seconds\n",
            "1.7287   48.2031  81.7851  1.7488   48.1104  81.3432  29.5118 ### iter: 800 | loss: 1.7815 | train interval: 40.44 seconds\n",
            "1.6008   51.8492  83.8604  1.6299   51.4921  83.3701  29.4928 ### iter: 1000 | loss: 1.6817 | train interval: 40.49 seconds\n",
            "1.5178   54.0336  85.1111  1.5465   53.7214  84.5976  29.4850 ### iter: 1200 | loss: 1.5930 | train interval: 40.43 seconds\n",
            "1.4501   55.8971  86.0422  1.4842   55.4499  85.4874  29.4859 ### iter: 1400 | loss: 1.5290 | train interval: 40.45 seconds\n",
            "1.4064   57.1364  86.6730  1.4459   56.6066  86.0867  29.5035 ### iter: 1600 | loss: 1.4824 | train interval: 40.43 seconds\n",
            "1.3635   58.2918  87.2241  1.4101   57.5285  86.4912  29.4605 ### iter: 1800 | loss: 1.4369 | train interval: 40.44 seconds\n",
            "1.3288   59.2941  87.6776  1.3815   58.2792  86.9174  29.4365 ### iter: 2000 | loss: 1.4048 | train interval: 40.43 seconds\n",
            "1.3012   60.0153  87.9923  1.3612   58.9464  87.1281  29.4630 ### iter: 2200 | loss: 1.3610 | train interval: 40.44 seconds\n",
            "1.2792   60.5810  88.2951  1.3405   59.4993  87.3909  29.5387 ### iter: 2400 | loss: 1.3514 | train interval: 40.44 seconds\n",
            "1.2596   61.1674  88.5453  1.3303   59.8804  87.5503  29.5699 ### iter: 2600 | loss: 1.3187 | train interval: 40.43 seconds\n",
            "1.2415   61.6312  88.7237  1.3184   60.1609  87.6563  29.5345 ### iter: 2800 | loss: 1.3194 | train interval: 40.45 seconds\n",
            "1.2257   62.0692  88.9917  1.3111   60.4550  87.8633  29.5504 ### iter: 3000 | loss: 1.3113 | train interval: 40.44 seconds\n",
            "1.2088   62.5521  89.1473  1.3035   60.6086  87.8725  29.5582 ### iter: 3200 | loss: 1.2856 | train interval: 40.43 seconds\n",
            "1.1946   62.9252  89.3488  1.2939   60.9668  88.0213  29.5264 ### iter: 3400 | loss: 1.2693 | train interval: 40.44 seconds\n",
            "1.1812   63.2984  89.4676  1.2866   61.2105  88.0464  29.5451 ### iter: 3600 | loss: 1.2532 | train interval: 40.43 seconds\n",
            "1.1699   63.5881  89.6165  1.2850   61.3455  88.0530  29.5211 ### iter: 3800 | loss: 1.2667 | train interval: 40.42 seconds\n",
            "1.1591   63.8813  89.7406  1.2798   61.5125  88.2186  29.9780 ### iter: 4000 | loss: 1.2632 | train interval: 40.42 seconds\n",
            "1.1492   64.1784  89.8640  1.2738   61.6371  88.2383  29.5194 ### iter: 4200 | loss: 1.2420 | train interval: 40.42 seconds\n",
            "1.1377   64.5285  90.0124  1.2711   61.7726  88.2509  29.6091 ### iter: 4400 | loss: 1.2200 | train interval: 40.44 seconds\n",
            "1.1264   64.7987  90.1474  1.2680   61.9234  88.3245  29.6306 ### iter: 4600 | loss: 1.2028 | train interval: 40.43 seconds\n",
            "1.1185   65.0676  90.2224  1.2678   61.9447  88.3492  29.6170 ### iter: 4800 | loss: 1.2187 | train interval: 40.43 seconds\n",
            "1.1068   65.3377  90.3494  1.2654   62.0324  88.2828  29.5430 ### iter: 5000 | loss: 1.2179 | train interval: 40.44 seconds\n",
            "\n",
            "Time taken for 5001 iterations: 1770.78 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#16 mins of training time, and 12 mins of eval time. Should just check every 1000 iterations(accidentally set it to 200)"
      ],
      "metadata": {
        "id": "q66jvmxWD2uG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# n_heads:6 | n_embedding: 384 | n_layer: 1 num_params: 1.9381 Million Parameters\n",
        "# ---------TRAIN----------|-----------TEST-----------|--TIMING----------\n",
        "# loss     top@1    top@5 |  loss     top@1    top@5 |  eval_time\n",
        "# 4.6333   0.9825   5.1626   4.6361   0.9424   5.0534   10.8289### iter: 0 | loss: 4.6345 | train time interval: 0.01 seconds\n",
        "# 2.5013   27.6460  66.2090  2.5089   27.8167  66.1908  10.9146### iter: 100 | loss: 2.5085 | train time interval: 7.76 seconds\n",
        "# ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓\n",
        "# n_heads:6 | n_embedding: 384 | n_layer: 1 num_params: 1.9381 Million Parameters\n",
        "# ---------TRAIN----------|-----------TEST-----------|--TIMING----------\n",
        "# loss     top@1    top@5 |  loss     top@1    top@5 |  eval_time\n",
        "# 4.6681   0.7924   4.6143   4.6686   0.7852   4.5937   10.1680### iter: 0 | loss: 4.6672 | train time interval: 0.01 seconds\n",
        "# 2.5054   27.4399  65.9486  2.5082   27.7022  66.0398  10.2297### iter: 100 | loss: 2.5171 | train time interval: 6.53 seconds\n",
        "\n",
        "# n_heads:6 | n_embedding: 384 | n_layer: 3 num_params: 5.4871 Million Parameters\n",
        "# ---------TRAIN----------|-----------TEST-----------|--TIMING----------\n",
        "# loss     top@1    top@5 |  loss     top@1    top@5 |  eval_time\n",
        "# 4.5404   1.0071   6.0883   4.5384   0.9906   6.0388   27.0759 ### iter: 0 | loss: 4.5422 | train interval: 0.01 seconds\n",
        "# 1.6008   51.8492  83.8604  1.6299   51.4921  83.3701  29.4928 ### iter: 1000 | loss: 1.6817 | train interval: 40.49 seconds\n",
        "# 1.3288   59.2941  87.6776  1.3815   58.2792  86.9174  29.4365 ### iter: 2000 | loss: 1.4048 | train interval: 40.43 seconds\n",
        "# 1.2257   62.0692  88.9917  1.3111   60.4550  87.8633  29.5504 ### iter: 3000 | loss: 1.3113 | train interval: 40.44 seconds\n",
        "# 1.1591   63.8813  89.7406  1.2798   61.5125  88.2186  29.9780 ### iter: 4000 | loss: 1.2632 | train interval: 40.42 seconds\n",
        "# 1.1068   65.3377  90.3494  1.2654   62.0324  88.2828  29.5430 ### iter: 5000 | loss: 1.2179 | train interval: 40.44 seconds\n",
        "# Time taken for 5001 iterations: 1770.78 seconds (would be 1000 seconds if I evaluated every 1000 iterations, not 200)"
      ],
      "metadata": {
        "id": "bsXaKqQ90_Am"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context = torch.zeros((1, 1), dtype = torch.long,  device = device)\n",
        "print(decode(gpt_model.generate(context, length_to_generate=200)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHpJvOw31J-m",
        "outputId": "33ccd1da-1607-4fcd-f5bc-a18db8bafd23"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "they were unproductable upon her from you. His gone seat.\" \n",
            "\n",
            "\"For that will be service dull none of a law warfarmers of desparate that. Her outside they \n",
            "under the Probably and stopped, sir. \"You are \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "  print(\"-\"*100)\n",
        "  sentence = \"James Davey,\"\n",
        "  context= torch.tensor([encode(sentence)], dtype = torch.long, device = device)\n",
        "  print(decode(gpt_model.generate(context, length_to_generate=200)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V58WPDdjFWNQ",
        "outputId": "919b7a93-3d69-4b09-fbda-92c8986300dd"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------------------------------------------\n",
            "James Davey, sir. Do you attempt is. \n",
            "The extended - and in a gesture of screezy pretent. Without your words left, to makes. \n",
            "\n",
            "Q. Now Siwennian, as the elections is beyond she said nothing, some that has greater \n",
            "----------------------------------------------------------------------------------------------------\n",
            "James Davey, Bail Pritcher. You'll mean that?\" \n",
            "\n",
            "\"It is your days we know he? He always way to with what at the time of the Empire were startled; two \n",
            "pilot.\" \n",
            "\n",
            "And Mallow famous sight? \n",
            "\n",
            "What said, \"These did I \n",
            "----------------------------------------------------------------------------------------------------\n",
            "James Davey, Dr. Seldon. Well, who had here at it noticilar almost before had \n",
            "occasionally before he war. But what was already peace was only doesn't it.\" \n",
            "\n",
            "He said. \n",
            "\n",
            "He said, \"Then, Lee Dorwin itself with the \n",
            "----------------------------------------------------------------------------------------------------\n",
            "James Davey, had not give \n",
            "into statement point of such a law, as I saw, paperhaps, what's wrotable sense, you see, \n",
            "but we have if you know the other money passed that you've left. There were shortly behind an \n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "James Davey, formed for a far. \n",
            "\n",
            "Toran admit turned to living reach that run. But they're an atmosphere of the chief that \n",
            "guide we had crunched the gray to him thousand years had no study, and Indbur case, then \n",
            "----------------------------------------------------------------------------------------------------\n",
            "James Davey, \"It's a veered crowd \n",
            "almost sharp.\" \n",
            "\n",
            "They darer's word, for how it. So it stupid limp, Sat did not varied weakness; then there was \n",
            "stomach swarm now. \n",
            "\n",
            "Hardin did say: \"What are is it?\" \n",
            "\n",
            "\"But it \n",
            "----------------------------------------------------------------------------------------------------\n",
            "James Davey, when the room Barr as the other goat in order and social, however, young man \n",
            "discovery, and no attitude portion, heavy feature only a Periphery can, the Foundation \n",
            "itself deadly and in pattern that\n",
            "----------------------------------------------------------------------------------------------------\n",
            "James Davey, involves, law, wweath one - gave still \n",
            "Trantor can felt an introduced of single power. \n",
            "Each a moment in successful dam with a good. In anyway, that is scene of our nothing control.\" \n",
            "\n",
            "\"Does the awf\n",
            "----------------------------------------------------------------------------------------------------\n",
            "James Davey, young subjects of science. I’m sirm to be back at the solution.\" \n",
            "\n",
            "\"Why,\" yes. \n",
            "\n",
            "The trader parted on her hand paid with a buttonously think of his route and that the footstep, \n",
            "crisis: So that Howev\n",
            "----------------------------------------------------------------------------------------------------\n",
            "James Davey, \"I will. I've been don't go in \n",
            "case. The traders to this state by his hand in discision to the footh, particians, does not at ax a \n",
            "encile, and that member of which will be both the first permits of\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cmLUbTUGFotP"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2Y7-w7wtH7qA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}