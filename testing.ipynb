{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = \"cpu\"\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 14, 256, 256])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###############################################################################\n",
    "# Models\n",
    "###############################################################################\n",
    "num_classes = 14\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "  def __init__(self, input_channels: int, output_channels: int):\n",
    "    \"\"\"Initialize the DownSampleBlock class.\n",
    "      \n",
    "    Parameters:\n",
    "      input_size (int) -- input size to block\n",
    "      input_channels (int) -- #channels into first layer in block\n",
    "      output_channels (int) -- #channels each layer produces\n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "    conv1 = nn.Conv2d(in_channels=input_channels, out_channels=output_channels, kernel_size=3, stride=1, padding=1)\n",
    "    conv2 = nn.Conv2d(in_channels=output_channels, out_channels=output_channels, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    nn.init.normal_(conv1.weight, mean=0.0, std= (2/(9*input_channels))**(1/2)) #sqrt(2/N)\n",
    "    nn.init.normal_(conv2.weight, mean=0.0, std= (2/(9*output_channels))**(1/2)) \n",
    "    \n",
    "    self.conv = nn.Sequential(conv1, nn.BatchNorm2d(output_channels), nn.ReLU(),\n",
    "                              conv2, nn.BatchNorm2d(output_channels), nn.ReLU())\n",
    "    \n",
    "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "    return self.conv(x)\n",
    "\n",
    "class DownSampleBlock(nn.Module):\n",
    "  \"\"\"\n",
    "  DownSampling block in UNET\n",
    "\n",
    "  MaxPool, DoubleCov\n",
    "  MaxPool, Conv, BatchNorm, ReLU, Conv, BatchNorm, ReLU\n",
    "  \"\"\"\n",
    "  def __init__(self, input_channels: int, output_channels: int):\n",
    "    \"\"\"Initialize the DownSampleBlock class.\n",
    "\n",
    "    Parameters:\n",
    "      input_size (int) -- input size to block\n",
    "      input_channels (int) -- #channels into first layer in block\n",
    "      output_channels (int) -- #channels each layer produces\n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "    self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "    self.conv = DoubleConv(input_channels, output_channels)\n",
    "\n",
    "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "    return self.conv(self.max_pool(x))\n",
    "\n",
    "class UpSampleBlock(nn.Module):\n",
    "  \"\"\"\n",
    "  UpSampling block in UNET\n",
    "\n",
    "  Upsample, Conv, Concat, Conv, ReLU, Conv, ReLU\n",
    "  \"\"\"\n",
    "  def __init__(self, input_channels: int, output_channels: int):\n",
    "    \"\"\"Initialize the DownSampleBlock class.\n",
    "    \n",
    "    Parameters:\n",
    "      input_size (int) -- input size to block\n",
    "      input_channels (int) -- #channels into first layer in block\n",
    "      output_channels (int) -- #channels each layer produces\n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "    self.up_sample = nn.ConvTranspose2d(input_channels, output_channels, kernel_size=2, stride=2)\n",
    "    self.conv = DoubleConv(input_channels, output_channels)\n",
    "\n",
    "  def forward(self, x: torch.Tensor, res: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Forward pass for DownSampleBlock.\n",
    "\n",
    "     Parameters:\n",
    "      x (torch.Tensor) -- input tensor to block\n",
    "      res (torch.Tensor) -- residual connection feeding into block\n",
    "    \n",
    "    Return x\n",
    "    x will be used as the input to the next upsizing block (or final layer)\n",
    "    \"\"\"\n",
    "    x = self.up_sample(x)\n",
    "    # size_diff1 = (res.shape[2]-x.shape[2])//2\n",
    "    # size_diff2 = (res.shape[2]-x.shape[2]) - size_diff1\n",
    "    # x = torch.concat((x, res[:, :, size_diff1:-size_diff2, size_diff1:-size_diff2]), dim = 1)\n",
    "    x = torch.cat((x, res), dim = 1)\n",
    "    x = self.conv(x)\n",
    "    return x\n",
    "\n",
    "class UNET(nn.Module):\n",
    "  def __init__(self, channels_in = 3):\n",
    "    \"\"\"Initialize the DownSampleBlock class.\n",
    "    \n",
    "    Parameters:\n",
    "      channels_in (int) -- input images channel size\n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "    self.first_conv = DoubleConv(channels_in, 64)\n",
    "    self.downsample_blocks = nn.ModuleList([DownSampleBlock(c, 2*c) for c in [64, 128, 256, 512]])\n",
    "    self.upsample_blocks = nn.ModuleList([UpSampleBlock(2*c, c) for c in [512, 256, 128, 64]])\n",
    "    self.final_layer = nn.Conv2d(in_channels=64, out_channels=num_classes, kernel_size=1, stride=1, padding=0)\n",
    "    self.loss_fn = nn.CrossEntropyLoss()\n",
    "  \n",
    "  def forward(self, x: torch.Tensor, target=None) -> torch.Tensor:\n",
    "    \"\"\"Forward pass for DownSampleBlock.\n",
    "\n",
    "     Parameters:\n",
    "      x (torch.Tensor) --input tensor to block\n",
    "      targets (torch.Tensor) --target output of model\n",
    "\n",
    "    Returns (logits, loss)\n",
    "\n",
    "    logits (torch.Tensor) --model's raw output\n",
    "    loss (torch.float32) --2d CrossEntropyLoss result\n",
    "    \"\"\"\n",
    "    x = self.first_conv(x)\n",
    "    residuals = []\n",
    "    for downsample in self.downsample_blocks:\n",
    "      residuals.append(x)\n",
    "      x = downsample(x)\n",
    "    for i, upsample in enumerate(self.upsample_blocks):\n",
    "      x = upsample(x, residuals[-(i+1)])\n",
    "    x = self.final_layer(x)\n",
    "\n",
    "    if target is None:\n",
    "      loss = None\n",
    "    else:\n",
    "      diff = 94 # (512 - 324)//2\n",
    "      loss = self.loss_fn(x, target[:, 0, diff:-diff, diff:-diff])\n",
    "    return x, loss\n",
    "  \n",
    "\n",
    "x = torch.randn(3, 3, 256, 256)\n",
    "model = UNET()\n",
    "model(x)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "UNET                                          [3, 14, 256, 256]         --\n",
       "├─DoubleConv: 1-1                             [3, 64, 256, 256]         --\n",
       "│    └─Sequential: 2-1                        [3, 64, 256, 256]         --\n",
       "│    │    └─Conv2d: 3-1                       [3, 64, 256, 256]         1,792\n",
       "│    │    └─BatchNorm2d: 3-2                  [3, 64, 256, 256]         128\n",
       "│    │    └─ReLU: 3-3                         [3, 64, 256, 256]         --\n",
       "│    │    └─Conv2d: 3-4                       [3, 64, 256, 256]         36,928\n",
       "│    │    └─BatchNorm2d: 3-5                  [3, 64, 256, 256]         128\n",
       "│    │    └─ReLU: 3-6                         [3, 64, 256, 256]         --\n",
       "├─ModuleList: 1-2                             --                        --\n",
       "│    └─DownSampleBlock: 2-2                   [3, 128, 128, 128]        --\n",
       "│    │    └─MaxPool2d: 3-7                    [3, 64, 128, 128]         --\n",
       "│    │    └─DoubleConv: 3-8                   [3, 128, 128, 128]        221,952\n",
       "│    └─DownSampleBlock: 2-3                   [3, 256, 64, 64]          --\n",
       "│    │    └─MaxPool2d: 3-9                    [3, 128, 64, 64]          --\n",
       "│    │    └─DoubleConv: 3-10                  [3, 256, 64, 64]          886,272\n",
       "│    └─DownSampleBlock: 2-4                   [3, 512, 32, 32]          --\n",
       "│    │    └─MaxPool2d: 3-11                   [3, 256, 32, 32]          --\n",
       "│    │    └─DoubleConv: 3-12                  [3, 512, 32, 32]          3,542,016\n",
       "│    └─DownSampleBlock: 2-5                   [3, 1024, 16, 16]         --\n",
       "│    │    └─MaxPool2d: 3-13                   [3, 512, 16, 16]          --\n",
       "│    │    └─DoubleConv: 3-14                  [3, 1024, 16, 16]         14,161,920\n",
       "├─ModuleList: 1-3                             --                        --\n",
       "│    └─UpSampleBlock: 2-6                     [3, 512, 32, 32]          --\n",
       "│    │    └─ConvTranspose2d: 3-15             [3, 512, 32, 32]          2,097,664\n",
       "│    │    └─DoubleConv: 3-16                  [3, 512, 32, 32]          7,080,960\n",
       "│    └─UpSampleBlock: 2-7                     [3, 256, 64, 64]          --\n",
       "│    │    └─ConvTranspose2d: 3-17             [3, 256, 64, 64]          524,544\n",
       "│    │    └─DoubleConv: 3-18                  [3, 256, 64, 64]          1,771,008\n",
       "│    └─UpSampleBlock: 2-8                     [3, 128, 128, 128]        --\n",
       "│    │    └─ConvTranspose2d: 3-19             [3, 128, 128, 128]        131,200\n",
       "│    │    └─DoubleConv: 3-20                  [3, 128, 128, 128]        443,136\n",
       "│    └─UpSampleBlock: 2-9                     [3, 64, 256, 256]         --\n",
       "│    │    └─ConvTranspose2d: 3-21             [3, 64, 256, 256]         32,832\n",
       "│    │    └─DoubleConv: 3-22                  [3, 64, 256, 256]         110,976\n",
       "├─Conv2d: 1-4                                 [3, 14, 256, 256]         910\n",
       "===============================================================================================\n",
       "Total params: 31,044,366\n",
       "Trainable params: 31,044,366\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 164.12\n",
       "===============================================================================================\n",
       "Input size (MB): 2.36\n",
       "Forward/backward pass size (MB): 1745.88\n",
       "Params size (MB): 124.18\n",
       "Estimated Total Size (MB): 1872.42\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "summary(model, input_size=(3, 3, 256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
